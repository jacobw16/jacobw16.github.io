{"id":"node_modules/@tensorflow/tfjs-layers/dist/layers/wrappers.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\wrappers.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581030063848},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":29,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../backend/tfjs_backend","loc":{"line":30,"column":16},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"../common","loc":{"line":31,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\common.js"},{"name":"../engine/topology","loc":{"line":32,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"../errors","loc":{"line":33,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../keras_format/common","loc":{"line":34,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\keras_format\\common.js"},{"name":"../utils/generic_utils","loc":{"line":35,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"},{"name":"../utils/types_utils","loc":{"line":36,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"},{"name":"./recurrent","loc":{"line":37,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\recurrent.js"},{"name":"./serialization","loc":{"line":38,"column":30},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\serialization.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Layers that augment the functionality of a base layer.\n */\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"../backend/tfjs_backend\");\nvar common_1 = require(\"../common\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar common_2 = require(\"../keras_format/common\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar types_utils_1 = require(\"../utils/types_utils\");\nvar recurrent_1 = require(\"./recurrent\");\nvar serialization_1 = require(\"./serialization\");\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nvar Wrapper = /** @class */ (function (_super) {\n    __extends(Wrapper, _super);\n    function Wrapper(args) {\n        var _this = \n        // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n        //   `super()`. But we can't do that here due to TypeScript's restriction.\n        //   See: https://github.com/Microsoft/TypeScript/issues/8277\n        //   As a result, we have to add checks in `get trainable()` and\n        //   `set trainable()` below in order to prevent using `this.layer` when\n        //   its value is `undefined`. The super constructor does use the getter\n        //   and the setter of `this.layer`.\n        _super.call(this, args) || this;\n        _this.layer = args.layer;\n        return _this;\n    }\n    Wrapper.prototype.build = function (inputShape) {\n        this.built = true;\n    };\n    Object.defineProperty(Wrapper.prototype, \"trainable\", {\n        // TODO(cais): Implement activityRegularizer getter.\n        get: function () {\n            // Porting Note: the check of `this.layer` here is necessary due to the\n            //   way the `constructor` of this class is written (see Porting Note\n            //   above).\n            if (this.layer != null) {\n                return this.layer.trainable;\n            }\n            else {\n                return false;\n            }\n        },\n        set: function (value) {\n            // Porting Note: the check of `this.layer` here is necessary due to the\n            //   way the `constructor` of this class is written (see Porting Note\n            //   above).\n            if (this.layer != null) {\n                this.layer.trainable = value;\n            }\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"trainableWeights\", {\n        get: function () {\n            return this.layer.trainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"nonTrainableWeights\", {\n        // TODO(cais): Implement setter for trainableWeights.\n        get: function () {\n            return this.layer.nonTrainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"updates\", {\n        // TODO(cais): Implement setter for nonTrainableWeights.\n        get: function () {\n            // tslint:disable-next-line:no-any\n            return this.layer._updates;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"losses\", {\n        // TODO(cais): Implement getUpdatesFor().\n        get: function () {\n            return this.layer.losses;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    // TODO(cais): Implement getLossesFor().\n    Wrapper.prototype.getWeights = function () {\n        return this.layer.getWeights();\n    };\n    Wrapper.prototype.setWeights = function (weights) {\n        this.layer.setWeights(weights);\n    };\n    Wrapper.prototype.getConfig = function () {\n        var config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    Wrapper.prototype.setFastWeightInitDuringBuild = function (value) {\n        _super.prototype.setFastWeightInitDuringBuild.call(this, value);\n        if (this.layer != null) {\n            this.layer.setFastWeightInitDuringBuild(value);\n        }\n    };\n    /** @nocollapse */\n    Wrapper.fromConfig = function (cls, config, customObjects) {\n        if (customObjects === void 0) { customObjects = {}; }\n        var layerConfig = config['layer'];\n        var layer = serialization_1.deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        var newConfig = { layer: layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    };\n    return Wrapper;\n}(topology_1.Layer));\nexports.Wrapper = Wrapper;\nvar TimeDistributed = /** @class */ (function (_super) {\n    __extends(TimeDistributed, _super);\n    function TimeDistributed(args) {\n        var _this = _super.call(this, args) || this;\n        _this.supportsMasking = true;\n        return _this;\n    }\n    TimeDistributed.prototype.build = function (inputShape) {\n        inputShape = types_utils_1.getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new errors_1.ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" +\n                (\"input shape \" + JSON.stringify(inputShape)));\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        _super.prototype.build.call(this, inputShape);\n    };\n    TimeDistributed.prototype.computeOutputShape = function (inputShape) {\n        inputShape = types_utils_1.getExactlyOneShape(inputShape);\n        var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        var childOutputShape = this.layer.computeOutputShape(childInputShape);\n        var timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    };\n    TimeDistributed.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n            inputs = types_utils_1.getExactlyOneTensor(inputs);\n            // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n            // values. Hence the inputs can't have an undetermined first (batch)\n            // dimension, which is why we always use the K.rnn approach here.\n            var step = function (inputs, states) {\n                // TODO(cais): Add useLearningPhase.\n                // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n                //   some cases (e.g., `layer` is a `Sequential` instance), which is\n                //   why `getExactlyOneTensor` is used below.\n                var output = types_utils_1.getExactlyOneTensor(_this.layer.call(inputs, kwargs));\n                return [output, []];\n            };\n            var rnnOutputs = recurrent_1.rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n            var y = rnnOutputs[1];\n            // TODO(cais): Add activity regularization.\n            // TODO(cais): Add useLearningPhase.\n            return y;\n        });\n    };\n    /** @nocollapse */\n    TimeDistributed.className = 'TimeDistributed';\n    return TimeDistributed;\n}(Wrapper));\nexports.TimeDistributed = TimeDistributed;\ntfjs_core_1.serialization.registerClass(TimeDistributed);\nfunction checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(common_2.VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nexports.checkBidirectionalMergeMode = checkBidirectionalMergeMode;\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nvar Bidirectional = /** @class */ (function (_super) {\n    __extends(Bidirectional, _super);\n    function Bidirectional(args) {\n        var _this = _super.call(this, args) || this;\n        // Note: When creating `this.forwardLayer`, the original Layer object\n        //   (`config.layer`) ought to be cloned. This is why we call\n        //   `getConfig()` followed by `deserialize()`. Without this cloning,\n        //   the layer names saved during serialization will incorrectly contain\n        //   the 'forward_' prefix. In Python Keras, this is done using\n        //   `copy.copy` (shallow copy), which does not have a simple equivalent\n        //   in JavaScript. JavaScript's `Object.assign()` does not copy\n        //   methods.\n        var layerConfig = args.layer.getConfig();\n        var forwDict = {};\n        forwDict['className'] = args.layer.getClassName();\n        forwDict['config'] = layerConfig;\n        _this.forwardLayer = serialization_1.deserialize(forwDict);\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        var backDict = {};\n        backDict['className'] = args.layer.getClassName();\n        backDict['config'] = layerConfig;\n        _this.backwardLayer = serialization_1.deserialize(backDict);\n        _this.forwardLayer.name = 'forward_' + _this.forwardLayer.name;\n        _this.backwardLayer.name = 'backward_' + _this.backwardLayer.name;\n        _this.mergeMode = args.mergeMode === undefined ?\n            DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n            args.mergeMode;\n        checkBidirectionalMergeMode(_this.mergeMode);\n        if (args.weights) {\n            throw new errors_1.NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        _this._stateful = args.layer.stateful;\n        _this.returnSequences = args.layer.returnSequences;\n        _this.returnState = args.layer.returnState;\n        _this.supportsMasking = true;\n        _this._trainable = true;\n        _this.inputSpec = args.layer.inputSpec;\n        _this.numConstants = null;\n        return _this;\n    }\n    Object.defineProperty(Bidirectional.prototype, \"trainable\", {\n        get: function () {\n            return this._trainable;\n        },\n        set: function (value) {\n            // Porting Note: the check of `this.layer` here is necessary due to the\n            //   way the `constructor` of this class is written (see Porting Note\n            //   above).\n            this._trainable = value;\n            if (this.forwardLayer != null) {\n                this.forwardLayer.trainable = value;\n            }\n            if (this.backwardLayer != null) {\n                this.backwardLayer.trainable = value;\n            }\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Bidirectional.prototype.getWeights = function () {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    };\n    Bidirectional.prototype.setWeights = function (weights) {\n        var numWeights = weights.length;\n        var numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    };\n    Bidirectional.prototype.computeOutputShape = function (inputShape) {\n        var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        var outputShape;\n        var outputShapes;\n        var stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    };\n    Bidirectional.prototype.apply = function (inputs, kwargs) {\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        var constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        var standardized = recurrent_1.standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if ((initialState == null || initialState.length === 0) &&\n            constants == null) {\n            return _super.prototype.apply.call(this, inputs, kwargs);\n        }\n        var additionalInputs = [];\n        var additionalSpecs = [];\n        if (initialState != null) {\n            var numStates = initialState.length;\n            if (numStates % 2 > 0) {\n                throw new errors_1.ValueError('When passing `initialState` to a Bidrectional RNN, ' +\n                    'the state should be an Array containing the states of ' +\n                    'the underlying RNNs.');\n            }\n            kwargs['initialState'] = initialState;\n            additionalInputs.push.apply(additionalInputs, initialState);\n            var stateSpecs = initialState\n                .map(function (state) { return new topology_1.InputSpec({ shape: state.shape }); });\n            this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n            this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n            additionalSpecs.push.apply(additionalSpecs, stateSpecs);\n        }\n        if (constants != null) {\n            throw new errors_1.NotImplementedError('Support for constants in Bidirectional layers is not ' +\n                'implemented yet.');\n        }\n        var isSymbolicTensor = additionalInputs[0] instanceof topology_1.SymbolicTensor;\n        for (var _i = 0, additionalInputs_1 = additionalInputs; _i < additionalInputs_1.length; _i++) {\n            var tensor = additionalInputs_1[_i];\n            if (tensor instanceof topology_1.SymbolicTensor !== isSymbolicTensor) {\n                throw new errors_1.ValueError('The initial state of a Bidirectional layer cannot be ' +\n                    'specified as a mix of symbolic and non-symbolic tensors');\n            }\n        }\n        if (isSymbolicTensor) {\n            // Compute the full input and specs, including the states.\n            var fullInput = [inputs].concat(additionalInputs);\n            var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call temporarily and replace inputSpec.\n            // Note: with initial states symbolic calls and non-symbolic calls to\n            // this method differ in how the initial states are passed. For\n            // symbolic calls, the initial states are passed in the first arg, as\n            // an Array of SymbolicTensors; for non-symbolic calls, they are\n            // passed in the second arg as a part of the kwargs. Hence the need to\n            // temporarily modify inputSpec here.\n            // TODO(cais): Make refactoring so that this hacky code below is no\n            // longer needed.\n            var originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            var output = _super.prototype.apply.call(this, fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return _super.prototype.apply.call(this, inputs, kwargs);\n        }\n    };\n    Bidirectional.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            var initialState = kwargs['initialState'];\n            var y;\n            var yRev;\n            if (initialState == null) {\n                y = _this.forwardLayer.call(inputs, kwargs);\n                yRev = _this.backwardLayer.call(inputs, kwargs);\n            }\n            else {\n                var forwardState = initialState.slice(0, initialState.length / 2);\n                var backwardState = initialState.slice(initialState.length / 2);\n                y = _this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));\n                yRev = _this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));\n            }\n            var states;\n            if (_this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                }\n                else {\n                }\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (_this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            var output;\n            if (_this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            }\n            else if (_this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            }\n            else if (_this.mergeMode === 'ave') {\n                output = tfc.mul(.5, tfc.add(y, yRev));\n            }\n            else if (_this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            }\n            else if (_this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            // TODO(cais): Properly set learning phase.\n            if (_this.returnState) {\n                if (_this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    };\n    Bidirectional.prototype.resetStates = function (states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    };\n    Bidirectional.prototype.build = function (inputShape) {\n        var _this = this;\n        common_1.nameScope(this.forwardLayer.name, function () {\n            _this.forwardLayer.build(inputShape);\n        });\n        common_1.nameScope(this.backwardLayer.name, function () {\n            _this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    };\n    Bidirectional.prototype.computeMask = function (inputs, mask) {\n        if (Array.isArray(mask)) {\n            mask = mask[0];\n        }\n        var outputMask;\n        if (this.returnSequences) {\n            if (this.mergeMode == null) {\n                outputMask = [mask, mask];\n            }\n            else {\n                outputMask = mask;\n            }\n        }\n        else {\n            if (this.mergeMode == null) {\n                outputMask = [null, null];\n            }\n            else {\n                outputMask = null;\n            }\n        }\n        if (this.returnState) {\n            var states = this.forwardLayer.states;\n            var stateMask = states.map(function (state) { return null; });\n            if (Array.isArray(outputMask)) {\n                return outputMask.concat(stateMask).concat(stateMask);\n            }\n            else {\n                return [outputMask].concat(stateMask).concat(stateMask);\n            }\n        }\n        else {\n            return outputMask;\n        }\n    };\n    Object.defineProperty(Bidirectional.prototype, \"trainableWeights\", {\n        get: function () {\n            return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Bidirectional.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    // TODO(cais): Implement constraints().\n    Bidirectional.prototype.setFastWeightInitDuringBuild = function (value) {\n        _super.prototype.setFastWeightInitDuringBuild.call(this, value);\n        if (this.forwardLayer != null) {\n            this.forwardLayer.setFastWeightInitDuringBuild(value);\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.setFastWeightInitDuringBuild(value);\n        }\n    };\n    Bidirectional.prototype.getConfig = function () {\n        var config = {\n            'mergeMode': this.mergeMode,\n        };\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    Bidirectional.fromConfig = function (cls, config) {\n        var rnnLayer = serialization_1.deserialize(config['layer']);\n        delete config['layer'];\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        if (config['numConstants'] != null) {\n            throw new errors_1.NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" +\n                \"present is not supported yet.\");\n        }\n        // tslint:disable-next-line:no-any\n        var newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    };\n    /** @nocollapse */\n    Bidirectional.className = 'Bidirectional';\n    return Bidirectional;\n}(Wrapper));\nexports.Bidirectional = Bidirectional;\ntfjs_core_1.serialization.registerClass(Bidirectional);\n"},"sourceMaps":{"js":{"version":3,"file":"wrappers.js","sourceRoot":"","sources":["../../src/layers/wrappers.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH;;GAEG;AAEH,2CAA6C;AAC7C,mDAAkE;AAClE,2CAA6C;AAC7C,oCAAoC;AACpC,+CAA+E;AAC/E,oCAA0D;AAC1D,iDAAsG;AAGtG,sDAAwD;AACxD,oDAA6E;AAG7E,yCAAsD;AACtD,iDAA4C;AAS5C;;;;;;GAMG;AACH;IAAsC,2BAAK;IAGzC,iBAAY,IAAsB;QAAlC;QACE,qEAAqE;QACrE,0EAA0E;QAC1E,6DAA6D;QAC7D,gEAAgE;QAChE,wEAAwE;QACxE,wEAAwE;QACxE,oCAAoC;QACpC,kBAAM,IAAI,CAAC,SAEZ;QADC,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC;;IAC1B,CAAC;IAED,uBAAK,GAAL,UAAM,UAAyB;QAC7B,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAID,sBAAI,8BAAS;QAFb,oDAAoD;aAEpD;YACE,uEAAuE;YACvE,qEAAqE;YACrE,YAAY;YACZ,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;gBACtB,OAAO,IAAI,CAAC,KAAK,CAAC,SAAS,CAAC;aAC7B;iBAAM;gBACL,OAAO,KAAK,CAAC;aACd;QACH,CAAC;aAED,UAAc,KAAc;YAC1B,uEAAuE;YACvE,qEAAqE;YACrE,YAAY;YACZ,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;gBACtB,IAAI,CAAC,KAAK,CAAC,SAAS,GAAG,KAAK,CAAC;aAC9B;QACH,CAAC;;;OATA;IAWD,sBAAI,qCAAgB;aAApB;YACE,OAAO,IAAI,CAAC,KAAK,CAAC,gBAAgB,CAAC;QACrC,CAAC;;;OAAA;IAGD,sBAAI,wCAAmB;QAFvB,qDAAqD;aAErD;YACE,OAAO,IAAI,CAAC,KAAK,CAAC,mBAAmB,CAAC;QACxC,CAAC;;;OAAA;IAGD,sBAAI,4BAAO;QAFX,wDAAwD;aAExD;YACE,kCAAkC;YAClC,OAAQ,IAAI,CAAC,KAAa,CAAC,QAAQ,CAAC;QACtC,CAAC;;;OAAA;IAID,sBAAI,2BAAM;QAFV,yCAAyC;aAEzC;YACE,OAAO,IAAI,CAAC,KAAK,CAAC,MAAM,CAAC;QAC3B,CAAC;;;OAAA;IAED,wCAAwC;IAExC,4BAAU,GAAV;QACE,OAAO,IAAI,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC;IACjC,CAAC;IAED,4BAAU,GAAV,UAAW,OAAiB;QAC1B,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;IACjC,CAAC;IAED,2BAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,OAAO,EAAE;gBACP,WAAW,EAAE,IAAI,CAAC,KAAK,CAAC,YAAY,EAAE;gBACtC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,SAAS,EAAE;aACjC;SACF,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,8CAA4B,GAA5B,UAA6B,KAAc;QACzC,iBAAM,4BAA4B,YAAC,KAAK,CAAC,CAAC;QAC1C,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,IAAI,CAAC,KAAK,CAAC,4BAA4B,CAAC,KAAK,CAAC,CAAC;SAChD;IACH,CAAC;IAED,kBAAkB;IACX,kBAAU,GAAjB,UACI,GAA6C,EAC7C,MAAgC,EAChC,aAA8C;QAA9C,8BAAA,EAAA,gBAAgB,EAA8B;QAChD,IAAM,WAAW,GAAG,MAAM,CAAC,OAAO,CAA6B,CAAC;QAChE,IAAM,KAAK,GAAG,2BAAW,CAAC,WAAW,EAAE,aAAa,CAAU,CAAC;QAC/D,OAAO,MAAM,CAAC,OAAO,CAAC,CAAC;QACvB,IAAM,SAAS,GAAG,EAAC,KAAK,OAAA,EAAC,CAAC;QAC1B,MAAM,CAAC,MAAM,CAAC,SAAS,EAAE,MAAM,CAAC,CAAC;QACjC,OAAO,IAAI,GAAG,CAAC,SAAS,CAAC,CAAC;IAC5B,CAAC;IACH,cAAC;AAAD,CAAC,AAvGD,CAAsC,gBAAK,GAuG1C;AAvGqB,0BAAO;AAyG7B;IAAqC,mCAAO;IAG1C,yBAAY,IAAsB;QAAlC,YACE,kBAAM,IAAI,CAAC,SAEZ;QADC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;;IAC9B,CAAC;IAED,+BAAK,GAAL,UAAM,UAAyB;QAC7B,UAAU,GAAG,gCAAkB,CAAC,UAAU,CAAC,CAAC;QAC5C,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;YACzB,MAAM,IAAI,mBAAU,CAChB,mEAAmE;iBACnE,iBAAe,IAAI,CAAC,SAAS,CAAC,UAAU,CAAG,CAAA,CAAC,CAAC;SAClD;QACD,IAAI,CAAC,SAAS,GAAG,CAAC,EAAC,KAAK,EAAE,UAAU,EAAC,CAAC,CAAC;QACvC,IAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QACpE,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,KAAK,EAAE;YACrB,IAAI,CAAC,KAAK,CAAC,KAAK,CAAC,eAAe,CAAC,CAAC;YAClC,IAAI,CAAC,KAAK,CAAC,KAAK,GAAG,IAAI,CAAC;SACzB;QACD,iBAAM,KAAK,YAAC,UAAU,CAAC,CAAC;IAC1B,CAAC;IAED,4CAAkB,GAAlB,UAAmB,UAAyB;QAC1C,UAAU,GAAG,gCAAkB,CAAC,UAAU,CAAC,CAAC;QAC5C,IAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QACpE,IAAM,gBAAgB,GAClB,IAAI,CAAC,KAAK,CAAC,kBAAkB,CAAC,eAAe,CAAU,CAAC;QAC5D,IAAM,SAAS,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC;QAChC,OAAO,CAAC,gBAAgB,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IAC5E,CAAC;IAED,8BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAwBC;QAvBC,OAAO,gBAAI,CAAC;YACV,+DAA+D;YAC/D,MAAM,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;YACrC,oEAAoE;YACpE,oEAAoE;YACpE,iEAAiE;YACjE,IAAM,IAAI,GAAoB,UAAC,MAAc,EAAE,MAAgB;gBAC7D,oCAAoC;gBACpC,oEAAoE;gBACpE,oEAAoE;gBACpE,6CAA6C;gBAC7C,IAAM,MAAM,GAAG,iCAAmB,CAAC,KAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC;gBACpE,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAC;YACtB,CAAC,CAAC;YACF,IAAM,UAAU,GACZ,eAAG,CAAC,IAAI,EAAE,MAAM,EAAE,EAAE,EAAE,KAAK,CAAC,iBAAiB,EAAE,IAAI,CAAC,UAAU,EAC1D,IAAI,CAAC,eAAe,EAAE,KAAK,CAAC,YAAY,EACxC,IAAI,CAAC,wBAAwB,CAAC,CAAC;YACvC,IAAM,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC;YACxB,2CAA2C;YAC3C,oCAAoC;YACpC,OAAO,CAAC,CAAC;QACX,CAAC,CAAC,CAAC;IACL,CAAC;IAxDD,kBAAkB;IACX,yBAAS,GAAG,iBAAiB,CAAC;IA0DvC,sBAAC;CAAA,AA5DD,CAAqC,OAAO,GA4D3C;AA5DY,0CAAe;AA6D5B,yBAAa,CAAC,aAAa,CAAC,eAAe,CAAC,CAAC;AAE7C,SAAgB,2BAA2B,CAAC,KAAc;IACxD,aAAa,CAAC,yBAAyB,CACnC,wCAA+B,EAAE,wBAAwB,EAAE,KAAK,CAAC,CAAC;AACxE,CAAC;AAHD,kEAGC;AAkBD,IAAM,gCAAgC,GAA2B,QAAQ,CAAC;AAE1E;IAAmC,iCAAO;IAWxC,uBAAY,IAA4B;QAAxC,YACE,kBAAM,IAAI,CAAC,SAuCZ;QArCC,qEAAqE;QACrE,6DAA6D;QAC7D,qEAAqE;QACrE,wEAAwE;QACxE,+DAA+D;QAC/D,wEAAwE;QACxE,gEAAgE;QAChE,aAAa;QACb,IAAM,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,SAAS,EAAE,CAAC;QAC3C,IAAM,QAAQ,GAA6B,EAAE,CAAC;QAC9C,QAAQ,CAAC,WAAW,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,EAAE,CAAC;QAClD,QAAQ,CAAC,QAAQ,CAAC,GAAG,WAAW,CAAC;QACjC,KAAI,CAAC,YAAY,GAAG,2BAAW,CAAC,QAAQ,CAAQ,CAAC;QACjD,WAAW,CAAC,aAAa,CAAC;YACtB,WAAW,CAAC,aAAa,CAAC,KAAK,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC;QACvD,IAAM,QAAQ,GAA6B,EAAE,CAAC;QAC9C,QAAQ,CAAC,WAAW,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,EAAE,CAAC;QAClD,QAAQ,CAAC,QAAQ,CAAC,GAAG,WAAW,CAAC;QACjC,KAAI,CAAC,aAAa,GAAG,2BAAW,CAAC,QAAQ,CAAQ,CAAC;QAClD,KAAI,CAAC,YAAY,CAAC,IAAI,GAAG,UAAU,GAAG,KAAI,CAAC,YAAY,CAAC,IAAI,CAAC;QAC7D,KAAI,CAAC,aAAa,CAAC,IAAI,GAAG,WAAW,GAAG,KAAI,CAAC,aAAa,CAAC,IAAI,CAAC;QAEhE,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,SAAS,KAAK,SAAS,CAAC,CAAC;YAC3C,gCAAgC,CAAC,CAAC;YAClC,IAAI,CAAC,SAAS,CAAC;QACnB,2BAA2B,CAAC,KAAI,CAAC,SAAS,CAAC,CAAC;QAC5C,IAAI,IAAI,CAAC,OAAO,EAAE;YAChB,MAAM,IAAI,4BAAmB,CACzB,iEAAiE,CAAC,CAAC;SACxE;QACD,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK,CAAC,QAAQ,CAAC;QACrC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;QAClD,KAAI,CAAC,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,WAAW,CAAC;QAC1C,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,UAAU,GAAG,IAAI,CAAC;QACvB,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK,CAAC,SAAS,CAAC;QACtC,KAAI,CAAC,YAAY,GAAG,IAAI,CAAC;;IAC3B,CAAC;IAED,sBAAI,oCAAS;aAAb;YACE,OAAO,IAAI,CAAC,UAAU,CAAC;QACzB,CAAC;aAED,UAAc,KAAc;YAC1B,uEAAuE;YACvE,qEAAqE;YACrE,YAAY;YACZ,IAAI,CAAC,UAAU,GAAG,KAAK,CAAC;YACxB,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,EAAE;gBAC7B,IAAI,CAAC,YAAY,CAAC,SAAS,GAAG,KAAK,CAAC;aACrC;YACD,IAAI,IAAI,CAAC,aAAa,IAAI,IAAI,EAAE;gBAC9B,IAAI,CAAC,aAAa,CAAC,SAAS,GAAG,KAAK,CAAC;aACtC;QACH,CAAC;;;OAbA;IAeD,kCAAU,GAAV;QACE,OAAO,IAAI,CAAC,YAAY,CAAC,UAAU,EAAE,CAAC,MAAM,CACxC,IAAI,CAAC,aAAa,CAAC,UAAU,EAAE,CAAC,CAAC;IACvC,CAAC;IAED,kCAAU,GAAV,UAAW,OAAiB;QAC1B,IAAM,UAAU,GAAG,OAAO,CAAC,MAAM,CAAC;QAClC,IAAM,cAAc,GAAG,IAAI,CAAC,KAAK,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAClD,IAAI,CAAC,YAAY,CAAC,UAAU,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC,EAAE,cAAc,CAAC,CAAC,CAAC;QAC/D,IAAI,CAAC,aAAa,CAAC,UAAU,CAAC,OAAO,CAAC,KAAK,CAAC,cAAc,CAAC,CAAC,CAAC;IAC/D,CAAC;IAED,0CAAkB,GAAlB,UAAmB,UAAyB;QAC1C,IAAI,WAAW,GACX,IAAI,CAAC,YAAY,CAAC,kBAAkB,CAAC,UAAU,CAAC,CAAC;QACrD,IAAI,CAAC,CAAC,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE;YAClE,WAAW,GAAG,CAAC,WAAoB,CAAC,CAAC;SACtC;QACD,WAAW,GAAG,WAAsB,CAAC;QAErC,IAAI,WAAkB,CAAC;QACvB,IAAI,YAAqB,CAAC;QAC1B,IAAI,UAAmB,CAAC;QACxB,IAAI,IAAI,CAAC,WAAW,EAAE;YACpB,UAAU,GAAG,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YAClC,WAAW,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;SAC9B;aAAM;YACL,WAAW,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;SAC9B;QACD,WAAW,GAAG,WAAW,CAAC;QAC1B,IAAI,IAAI,CAAC,SAAS,KAAK,QAAQ,EAAE;YAC/B,WAAW,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC;YACzC,YAAY,GAAG,CAAC,WAAW,CAAC,CAAC;SAC9B;aAAM,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;YACjC,YAAY,GAAG,CAAC,WAAW,EAAE,WAAW,CAAC,KAAK,EAAE,CAAC,CAAC;SACnD;aAAM;YACL,YAAY,GAAG,CAAC,WAAW,CAAC,CAAC;SAC9B;QAED,IAAI,IAAI,CAAC,WAAW,EAAE;YACpB,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;gBAC1B,OAAO,YAAY,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,EAAE,CAAC,CAAC;aACnE;YACD,OAAO,CAAC,WAAW,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,KAAK,EAAE,CAAC,CAAC;SACpE;QACD,OAAO,aAAa,CAAC,gBAAgB,CAAC,YAAY,CAAC,CAAC;IACtD,CAAC;IAED,6BAAK,GAAL,UACI,MAAuD,EACvD,MAAe;QACjB,IAAI,YAAY,GACZ,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;QACnD,IAAI,SAAS,GACT,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,MAAM,CAAC,WAAW,CAAC,CAAC;QAChD,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,MAAM,GAAG,EAAE,CAAC;SACb;QACD,IAAM,YAAY,GACd,2BAAe,CAAC,MAAM,EAAE,YAAY,EAAE,SAAS,EAAE,IAAI,CAAC,YAAY,CAAC,CAAC;QACxE,MAAM,GAAG,YAAY,CAAC,MAAM,CAAC;QAC7B,YAAY,GAAG,YAAY,CAAC,YAAY,CAAC;QACzC,SAAS,GAAG,YAAY,CAAC,SAAS,CAAC;QAEnC,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;YACzB,YAAY,GAAI,MAAsC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YAChE,MAAM,GAAI,MAAsC,CAAC,CAAC,CAAC,CAAC;SACrD;QAED,IAAI,CAAC,YAAY,IAAI,IAAI,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,CAAC;YACnD,SAAS,IAAI,IAAI,EAAE;YACrB,OAAO,iBAAM,KAAK,YAAC,MAAM,EAAE,MAAM,CAAC,CAAC;SACpC;QACD,IAAM,gBAAgB,GAAiC,EAAE,CAAC;QAC1D,IAAM,eAAe,GAAgB,EAAE,CAAC;QACxC,IAAI,YAAY,IAAI,IAAI,EAAE;YACxB,IAAM,SAAS,GAAG,YAAY,CAAC,MAAM,CAAC;YACtC,IAAI,SAAS,GAAG,CAAC,GAAG,CAAC,EAAE;gBACrB,MAAM,IAAI,mBAAU,CAChB,qDAAqD;oBACrD,wDAAwD;oBACxD,sBAAsB,CAAC,CAAC;aAC7B;YACD,MAAM,CAAC,cAAc,CAAC,GAAG,YAAY,CAAC;YACtC,gBAAgB,CAAC,IAAI,OAArB,gBAAgB,EAAS,YAAY,EAAE;YACvC,IAAM,UAAU,GAAI,YAA6C;iBACzC,GAAG,CAAC,UAAA,KAAK,IAAI,OAAA,IAAI,oBAAS,CAAC,EAAC,KAAK,EAAE,KAAK,CAAC,KAAK,EAAC,CAAC,EAAnC,CAAmC,CAAC,CAAC;YAC1E,IAAI,CAAC,YAAY,CAAC,SAAS,GAAG,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,SAAS,GAAG,CAAC,CAAC,CAAC;YACjE,IAAI,CAAC,aAAa,CAAC,SAAS,GAAG,UAAU,CAAC,KAAK,CAAC,SAAS,GAAG,CAAC,CAAC,CAAC;YAC/D,eAAe,CAAC,IAAI,OAApB,eAAe,EAAS,UAAU,EAAE;SACrC;QACD,IAAI,SAAS,IAAI,IAAI,EAAE;YACrB,MAAM,IAAI,4BAAmB,CACzB,uDAAuD;gBACvD,kBAAkB,CAAC,CAAC;SACzB;QAED,IAAM,gBAAgB,GAAG,gBAAgB,CAAC,CAAC,CAAC,YAAY,yBAAc,CAAC;QACvE,KAAqB,UAAgB,EAAhB,qCAAgB,EAAhB,8BAAgB,EAAhB,IAAgB,EAAE;YAAlC,IAAM,MAAM,yBAAA;YACf,IAAI,MAAM,YAAY,yBAAc,KAAK,gBAAgB,EAAE;gBACzD,MAAM,IAAI,mBAAU,CAChB,uDAAuD;oBACvD,yDAAyD,CAAC,CAAC;aAChE;SACF;QAED,IAAI,gBAAgB,EAAE;YACpB,0DAA0D;YAC1D,IAAM,SAAS,GAAG,CAAC,MAAM,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAC,CAAC;YACpD,IAAM,aAAa,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC;YAC7D,sDAAsD;YACtD,qEAAqE;YACrE,+DAA+D;YAC/D,qEAAqE;YACrE,gEAAgE;YAChE,sEAAsE;YACtE,qCAAqC;YACrC,mEAAmE;YACnE,iBAAiB;YACjB,IAAM,iBAAiB,GAAG,IAAI,CAAC,SAAS,CAAC;YACzC,IAAI,CAAC,SAAS,GAAG,aAAa,CAAC;YAC/B,IAAM,MAAM,GACR,iBAAM,KAAK,YAAC,SAAwC,EAAE,MAAM,CAAC,CAAC;YAClE,IAAI,CAAC,SAAS,GAAG,iBAAiB,CAAC;YACnC,OAAO,MAAM,CAAC;SACf;aAAM;YACL,OAAO,iBAAM,KAAK,YAAC,MAAM,EAAE,MAAM,CAAC,CAAC;SACpC;IACH,CAAC;IAED,4BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAsDC;QArDC,OAAO,gBAAI,CAAC;YACV,IAAM,YAAY,GAAG,MAAM,CAAC,cAAc,CAAC,CAAC;YAE5C,IAAI,CAAkB,CAAC;YACvB,IAAI,IAAqB,CAAC;YAC1B,IAAI,YAAY,IAAI,IAAI,EAAE;gBACxB,CAAC,GAAG,KAAI,CAAC,YAAY,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;gBAC3C,IAAI,GAAG,KAAI,CAAC,aAAa,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;aAChD;iBAAM;gBACL,IAAM,YAAY,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;gBACpE,IAAM,aAAa,GAAG,YAAY,CAAC,KAAK,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;gBAClE,CAAC,GAAG,KAAI,CAAC,YAAY,CAAC,IAAI,CACtB,MAAM,EAAE,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,EAAC,YAAY,EAAE,YAAY,EAAC,CAAC,CAAC,CAAC;gBACjE,IAAI,GAAG,KAAI,CAAC,aAAa,CAAC,IAAI,CAC1B,MAAM,EAAE,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,EAAC,YAAY,EAAE,aAAa,EAAC,CAAC,CAAC,CAAC;aACnE;YAED,IAAI,MAAgB,CAAC;YACrB,IAAI,KAAI,CAAC,WAAW,EAAE;gBACpB,IAAI,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE;oBACpB,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,MAAM,CAAE,IAAiB,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;iBACzD;qBAAM;iBACN;gBACD,CAAC,GAAI,CAAc,CAAC,CAAC,CAAC,CAAC;gBACvB,IAAI,GAAI,IAAiB,CAAC,CAAC,CAAC,CAAC;aAC9B;YAED,IAAI,KAAI,CAAC,eAAe,EAAE;gBACxB,IAAI,GAAG,GAAG,CAAC,OAAO,CAAC,IAAc,EAAE,CAAC,CAAC,CAAC;aACvC;YAED,IAAI,MAAuB,CAAC;YAC5B,IAAI,KAAI,CAAC,SAAS,KAAK,QAAQ,EAAE;gBAC/B,MAAM,GAAG,CAAC,CAAC,WAAW,CAAC,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC,CAAC;aACvD;iBAAM,IAAI,KAAI,CAAC,SAAS,KAAK,KAAK,EAAE;gBACnC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC;aAC/C;iBAAM,IAAI,KAAI,CAAC,SAAS,KAAK,KAAK,EAAE;gBACnC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,GAAG,CAAC,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC,CAAC;aAC5D;iBAAM,IAAI,KAAI,CAAC,SAAS,KAAK,KAAK,EAAE;gBACnC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC;aAC/C;iBAAM,IAAI,KAAI,CAAC,SAAS,IAAI,IAAI,EAAE;gBACjC,MAAM,GAAG,CAAC,CAAW,EAAE,IAAc,CAAC,CAAC;aACxC;YAED,2CAA2C;YAC3C,IAAI,KAAI,CAAC,WAAW,EAAE;gBACpB,IAAI,KAAI,CAAC,SAAS,IAAI,IAAI,EAAE;oBAC1B,OAAQ,MAAmB,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;iBAC5C;gBACD,OAAO,CAAC,MAAgB,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;aAC1C;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAED,mCAAW,GAAX,UAAY,MAAwB;QAClC,IAAI,CAAC,YAAY,CAAC,WAAW,EAAE,CAAC;QAChC,IAAI,CAAC,aAAa,CAAC,WAAW,EAAE,CAAC;IACnC,CAAC;IAED,6BAAK,GAAL,UAAM,UAAyB;QAA/B,iBAQC;QAPC,kBAAS,CAAC,IAAI,CAAC,YAAY,CAAC,IAAI,EAAE;YAChC,KAAI,CAAC,YAAY,CAAC,KAAK,CAAC,UAAU,CAAC,CAAC;QACtC,CAAC,CAAC,CAAC;QACH,kBAAS,CAAC,IAAI,CAAC,aAAa,CAAC,IAAI,EAAE;YACjC,KAAI,CAAC,aAAa,CAAC,KAAK,CAAC,UAAU,CAAC,CAAC;QACvC,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAED,mCAAW,GAAX,UAAY,MAAuB,EAAE,IAAsB;QAEzD,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;YACvB,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;SAChB;QACD,IAAI,UAA2B,CAAC;QAChC,IAAI,IAAI,CAAC,eAAe,EAAE;YACxB,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;gBAC1B,UAAU,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC;aAC3B;iBAAM;gBACL,UAAU,GAAG,IAAI,CAAC;aACnB;SACF;aAAM;YACL,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;gBAC1B,UAAU,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC;aAC3B;iBAAM;gBACL,UAAU,GAAG,IAAI,CAAC;aACnB;SACF;QACD,IAAI,IAAI,CAAC,WAAW,EAAE;YACpB,IAAM,MAAM,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM,CAAC;YACxC,IAAM,SAAS,GAAa,MAAM,CAAC,GAAG,CAAC,UAAA,KAAK,IAAI,OAAA,IAAI,EAAJ,CAAI,CAAC,CAAC;YACtD,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,EAAE;gBAC7B,OAAO,UAAU,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;aACvD;iBAAM;gBACL,OAAO,CAAC,UAAU,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;aACzD;SACF;aAAM;YACL,OAAO,UAAU,CAAC;SACnB;IACH,CAAC;IAED,sBAAI,2CAAgB;aAApB;YACE,OAAO,IAAI,CAAC,YAAY,CAAC,gBAAgB,CAAC,MAAM,CAC5C,IAAI,CAAC,aAAa,CAAC,gBAAgB,CAAC,CAAC;QAC3C,CAAC;;;OAAA;IAED,sBAAI,8CAAmB;aAAvB;YACE,OAAO,IAAI,CAAC,YAAY,CAAC,mBAAmB,CAAC,MAAM,CAC/C,IAAI,CAAC,aAAa,CAAC,mBAAmB,CAAC,CAAC;QAC9C,CAAC;;;OAAA;IAED,uCAAuC;IAEvC,oDAA4B,GAA5B,UAA6B,KAAc;QACzC,iBAAM,4BAA4B,YAAC,KAAK,CAAC,CAAC;QAC1C,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,EAAE;YAC7B,IAAI,CAAC,YAAY,CAAC,4BAA4B,CAAC,KAAK,CAAC,CAAC;SACvD;QACD,IAAI,IAAI,CAAC,aAAa,IAAI,IAAI,EAAE;YAC9B,IAAI,CAAC,aAAa,CAAC,4BAA4B,CAAC,KAAK,CAAC,CAAC;SACxD;IACH,CAAC;IAED,iCAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,WAAW,EAAE,IAAI,CAAC,SAAS;SAC5B,CAAC;QACF,uEAAuE;QACvE,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,kBAAkB;IACX,wBAAU,GAAjB,UACI,GAA6C,EAC7C,MAAgC;QAClC,IAAM,QAAQ,GACV,2BAAW,CAAC,MAAM,CAAC,OAAO,CAA6B,CAAQ,CAAC;QACpE,OAAO,MAAM,CAAC,OAAO,CAAC,CAAC;QACvB,uEAAuE;QACvE,IAAI,MAAM,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;YAClC,MAAM,IAAI,4BAAmB,CACzB,6DAA6D;gBAC7D,+BAA+B,CAAC,CAAC;SACtC;QACD,kCAAkC;QAClC,IAAM,SAAS,GAAyB,MAAM,CAAC;QAC/C,SAAS,CAAC,OAAO,CAAC,GAAG,QAAQ,CAAC;QAC9B,OAAO,IAAI,GAAG,CAAC,SAAS,CAAC,CAAC;IAC5B,CAAC;IA/VD,kBAAkB;IACX,uBAAS,GAAG,eAAe,CAAC;IA+VrC,oBAAC;CAAA,AAjWD,CAAmC,OAAO,GAiWzC;AAjWY,sCAAa;AAkW1B,yBAAa,CAAC,aAAa,CAAC,aAAa,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {InputSpec, Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {BidirectionalMergeMode, Shape, VALID_BIDIRECTIONAL_MERGE_MODES} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {RegularizerFn, RnnStepFunction} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nimport {rnn, RNN, standardizeArgs} from './recurrent';\nimport {deserialize} from './serialization';\n\nexport declare interface WrapperLayerArgs extends LayerArgs {\n  /**\n   * The layer to be wrapped.\n   */\n  layer: Layer;\n}\n\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport abstract class Wrapper extends Layer {\n  readonly layer: Layer;\n\n  constructor(args: WrapperLayerArgs) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    this.built = true;\n  }\n\n  // TODO(cais): Implement activityRegularizer getter.\n\n  get trainable(): boolean {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    return this.layer.trainableWeights;\n  }\n  // TODO(cais): Implement setter for trainableWeights.\n\n  get nonTrainableWeights(): LayerVariable[] {\n    return this.layer.nonTrainableWeights;\n  }\n  // TODO(cais): Implement setter for nonTrainableWeights.\n\n  get updates(): Tensor[] {\n    // tslint:disable-next-line:no-any\n    return (this.layer as any)._updates;\n  }\n\n  // TODO(cais): Implement getUpdatesFor().\n\n  get losses(): RegularizerFn[] {\n    return this.layer.losses;\n  }\n\n  // TODO(cais): Implement getLossesFor().\n\n  getWeights(): Tensor[] {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights: Tensor[]): void {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig(),\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const layerConfig = config['layer'] as serialization.ConfigDict;\n    const layer = deserialize(layerConfig, customObjects) as Layer;\n    delete config['layer'];\n    const newConfig = {layer};\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n}\n\nexport class TimeDistributed extends Wrapper {\n  /** @nocollapse */\n  static className = 'TimeDistributed';\n  constructor(args: WrapperLayerArgs) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    if (inputShape.length < 3) {\n      throw new ValueError(\n          `TimeDistributed layer expects an input shape >= 3D, but received ` +\n          `input shape ${JSON.stringify(inputShape)}`);\n    }\n    this.inputSpec = [{shape: inputShape}];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape =\n        this.layer.computeOutputShape(childInputShape) as Shape;\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs);\n      // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n      const step: RnnStepFunction = (inputs: Tensor, states: Tensor[]) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n      const rnnOutputs =\n          rnn(step, inputs, [], false /* goBackwards */, null /* mask */,\n              null /* constants */, false /* unroll */,\n              true /* needPerStepOutputs */);\n      const y = rnnOutputs[1];\n      // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n      return y;\n    });\n  }\n\n  // TODO(cais): Implement detailed computeMask() logic.\n}\nserialization.registerClass(TimeDistributed);\n\nexport function checkBidirectionalMergeMode(value?: string): void {\n  generic_utils.checkStringTypeUnionValue(\n      VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\n\nexport declare interface BidirectionalLayerArgs extends WrapperLayerArgs {\n  /**\n   * The instance of an `RNN` layer to be wrapped.\n   */\n  layer: RNN;\n\n  /**\n   * Mode by which outputs of the forward and backward RNNs are\n   * combined. If `null` or `undefined`, the output will not be\n   * combined, they will be returned as an `Array`.\n   *\n   * If `undefined` (i.e., not provided), defaults to `'concat'`.\n   */\n  mergeMode?: BidirectionalMergeMode;\n}\n\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE: BidirectionalMergeMode = 'concat';\n\nexport class Bidirectional extends Wrapper {\n  /** @nocollapse */\n  static className = 'Bidirectional';\n  mergeMode: BidirectionalMergeMode;\n  private forwardLayer: RNN;\n  private backwardLayer: RNN;\n  private returnSequences: boolean;\n  private returnState: boolean;\n  private numConstants?: number;\n  private _trainable: boolean;\n\n  constructor(args: BidirectionalLayerArgs) {\n    super(args);\n\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    const layerConfig = args.layer.getConfig();\n    const forwDict: serialization.ConfigDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict) as RNN;\n    layerConfig['goBackwards'] =\n        layerConfig['goBackwards'] === true ? false : true;\n    const backDict: serialization.ConfigDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict) as RNN;\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n\n    this.mergeMode = args.mergeMode === undefined ?\n        DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n        args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError(\n          'weights support is not implemented for Bidirectional layer yet.');\n    }\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable(): boolean {\n    return this._trainable;\n  }\n\n  set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights(): Tensor[] {\n    return this.forwardLayer.getWeights().concat(\n        this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights: Tensor[]): void {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    let layerShapes: Shape|Shape[] =\n        this.forwardLayer.computeOutputShape(inputShape);\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes as Shape];\n    }\n    layerShapes = layerShapes as Shape[];\n\n    let outputShape: Shape;\n    let outputShapes: Shape[];\n    let stateShape: Shape[];\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n    outputShape = outputShape;\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = (inputs as Tensor[] | SymbolicTensor[]).slice(1);\n      inputs = (inputs as Tensor[] | SymbolicTensor[])[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) &&\n        constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n    const additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    const additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      const numStates = initialState.length;\n      if (numStates % 2 > 0) {\n        throw new ValueError(\n            'When passing `initialState` to a Bidrectional RNN, ' +\n            'the state should be an Array containing the states of ' +\n            'the underlying RNNs.');\n      }\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = (initialState as Array<Tensor|SymbolicTensor>)\n                             .map(state => new InputSpec({shape: state.shape}));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n    if (constants != null) {\n      throw new NotImplementedError(\n          'Support for constants in Bidirectional layers is not ' +\n          'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError(\n            'The initial state of a Bidirectional layer cannot be ' +\n            'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output =\n          super.apply(fullInput as Tensor[] | SymbolicTensor[], kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n\n      let y: Tensor|Tensor[];\n      let yRev: Tensor|Tensor[];\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: forwardState}));\n        yRev = this.backwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: backwardState}));\n      }\n\n      let states: Tensor[];\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat((yRev as Tensor[]).slice(1));\n        } else {\n        }\n        y = (y as Tensor[])[0];\n        yRev = (yRev as Tensor[])[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev as Tensor, 1);\n      }\n\n      let output: Tensor|Tensor[];\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y as Tensor, yRev as Tensor]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y as Tensor, yRev as Tensor));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode == null) {\n        output = [y as Tensor, yRev as Tensor];\n      }\n\n      // TODO(cais): Properly set learning phase.\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return (output as Tensor[]).concat(states);\n        }\n        return [output as Tensor].concat(states);\n      }\n      return output;\n    });\n  }\n\n  resetStates(states?: Tensor|Tensor[]): void {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n    let outputMask: Tensor|Tensor[];\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask: Tensor[] = states.map(state => null);\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    return this.forwardLayer.trainableWeights.concat(\n        this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    return this.forwardLayer.nonTrainableWeights.concat(\n        this.backwardLayer.nonTrainableWeights);\n  }\n\n  // TODO(cais): Implement constraints().\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'mergeMode': this.mergeMode,\n    };\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    const rnnLayer =\n        deserialize(config['layer'] as serialization.ConfigDict) as RNN;\n    delete config['layer'];\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(\n          `Deserialization of a Bidirectional layer with numConstants ` +\n          `present is not supported yet.`);\n    }\n    // tslint:disable-next-line:no-any\n    const newConfig: {[key: string]: any} = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n}\nserialization.registerClass(Bidirectional);\n"]}},"error":null,"hash":"3c69c5b9e2f9a6cc973fac98df898985","cacheData":{"env":{}}}