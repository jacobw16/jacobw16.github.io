{"id":"node_modules/@tensorflow/tfjs-layers/dist/layers/advanced_activations.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\advanced_activations.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581030063848},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":28,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../activations","loc":{"line":29,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\activations.js"},{"name":"../backend/tfjs_backend","loc":{"line":30,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"../constraints","loc":{"line":31,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\constraints.js"},{"name":"../engine/topology","loc":{"line":32,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"../errors","loc":{"line":33,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../initializers","loc":{"line":34,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\initializers.js"},{"name":"../regularizers","loc":{"line":35,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\regularizers.js"},{"name":"../utils/types_utils","loc":{"line":36,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n *  Advanced activation layers.\n */\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar activations_1 = require(\"../activations\");\nvar tfjs_backend_1 = require(\"../backend/tfjs_backend\");\nvar constraints_1 = require(\"../constraints\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar initializers_1 = require(\"../initializers\");\nvar regularizers_1 = require(\"../regularizers\");\nvar types_utils_1 = require(\"../utils/types_utils\");\nvar ReLU = /** @class */ (function (_super) {\n    __extends(ReLU, _super);\n    function ReLU(args) {\n        var _this = _super.call(this, args == null ? {} : args) || this;\n        _this.supportsMasking = true;\n        if (args != null) {\n            _this.maxValue = args.maxValue;\n        }\n        return _this;\n    }\n    ReLU.prototype.call = function (inputs, kwargs) {\n        inputs = types_utils_1.getExactlyOneTensor(inputs);\n        var output = tfjs_core_1.relu(inputs);\n        if (this.maxValue != null) {\n            output = tfjs_core_1.clipByValue(output, 0, this.maxValue);\n        }\n        return output;\n    };\n    ReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ReLU.prototype.getConfig = function () {\n        var config = { maxValue: this.maxValue };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    ReLU.className = 'ReLU';\n    return ReLU;\n}(topology_1.Layer));\nexports.ReLU = ReLU;\ntfjs_core_1.serialization.registerClass(ReLU);\nvar LeakyReLU = /** @class */ (function (_super) {\n    __extends(LeakyReLU, _super);\n    function LeakyReLU(args) {\n        var _this = _super.call(this, args == null ? {} : args) || this;\n        _this.DEFAULT_ALPHA = 0.3;\n        if (args == null) {\n            args = {};\n        }\n        _this.alpha = args.alpha == null ? _this.DEFAULT_ALPHA : args.alpha;\n        return _this;\n    }\n    LeakyReLU.prototype.call = function (inputs, kwargs) {\n        var x = types_utils_1.getExactlyOneTensor(inputs);\n        return tfjs_core_1.leakyRelu(x, this.alpha);\n    };\n    LeakyReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    LeakyReLU.prototype.getConfig = function () {\n        var config = { alpha: this.alpha };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    LeakyReLU.className = 'LeakyReLU';\n    return LeakyReLU;\n}(topology_1.Layer));\nexports.LeakyReLU = LeakyReLU;\ntfjs_core_1.serialization.registerClass(LeakyReLU);\nvar PReLU = /** @class */ (function (_super) {\n    __extends(PReLU, _super);\n    function PReLU(args) {\n        var _this = _super.call(this, args == null ? {} : args) || this;\n        _this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n        if (args == null) {\n            args = {};\n        }\n        _this.supportsMasking = true;\n        _this.alphaInitializer =\n            initializers_1.getInitializer(args.alphaInitializer || _this.DEFAULT_ALPHA_INITIALIZER);\n        _this.alphaRegularizer = regularizers_1.getRegularizer(args.alphaRegularizer);\n        _this.alphaConstraint = constraints_1.getConstraint(args.alphaConstraint);\n        if (args.sharedAxes == null) {\n            _this.sharedAxes = null;\n        }\n        else if (Array.isArray(args.sharedAxes)) {\n            _this.sharedAxes = args.sharedAxes;\n        }\n        else if (typeof args.sharedAxes === 'number') {\n            _this.sharedAxes = [args.sharedAxes];\n        }\n        else {\n            throw new errors_1.ValueError(\"Expected sharedAxes to be a number or an array of numbers, \" +\n                (\"but got \" + args.sharedAxes));\n        }\n        return _this;\n    }\n    PReLU.prototype.build = function (inputShape) {\n        inputShape = types_utils_1.getExactlyOneShape(inputShape);\n        var paramShape = inputShape.slice(1);\n        if (this.sharedAxes != null) {\n            for (var _i = 0, _a = this.sharedAxes; _i < _a.length; _i++) {\n                var i = _a[_i];\n                paramShape[i - 1] = 1;\n            }\n        }\n        this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n        // Set input spec.\n        var axes = {};\n        if (this.sharedAxes != null) {\n            for (var i = 1; i < inputShape.length; ++i) {\n                axes[i] = inputShape[i];\n            }\n        }\n        this.inputSpec = [new topology_1.InputSpec({\n                ndim: inputShape.length,\n                axes: axes,\n            })];\n        this.built = true;\n    };\n    PReLU.prototype.call = function (inputs, kwargs) {\n        inputs = types_utils_1.getExactlyOneTensor(inputs);\n        return tfjs_core_1.prelu(inputs, this.alpha.read());\n    };\n    PReLU.prototype.getConfig = function () {\n        var config = {\n            alphaInitializer: initializers_1.serializeInitializer(this.alphaInitializer),\n            alphaRegularizer: regularizers_1.serializeRegularizer(this.alphaRegularizer),\n            alphaConstraint: constraints_1.serializeConstraint(this.alphaConstraint),\n            sharedAxes: this.sharedAxes\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    PReLU.className = 'PReLU';\n    return PReLU;\n}(topology_1.Layer));\nexports.PReLU = PReLU;\ntfjs_core_1.serialization.registerClass(PReLU);\nvar ELU = /** @class */ (function (_super) {\n    __extends(ELU, _super);\n    function ELU(args) {\n        var _this = _super.call(this, args == null ? {} : args) || this;\n        _this.DEFAULT_ALPHA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        if (args.alpha != null && args.alpha !== _this.DEFAULT_ALPHA) {\n            throw new errors_1.NotImplementedError(\"Non-default alpha value (\" + args.alpha + \") is not supported by the \" +\n                \"ELU layer yet.\");\n        }\n        _this.alpha = args.alpha == null ? _this.DEFAULT_ALPHA : args.alpha;\n        return _this;\n    }\n    ELU.prototype.call = function (inputs, kwargs) {\n        var x = types_utils_1.getExactlyOneTensor(inputs);\n        return tfjs_core_1.elu(x);\n    };\n    ELU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ELU.prototype.getConfig = function () {\n        var config = { alpha: this.alpha };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    ELU.className = 'ELU';\n    return ELU;\n}(topology_1.Layer));\nexports.ELU = ELU;\ntfjs_core_1.serialization.registerClass(ELU);\nvar ThresholdedReLU = /** @class */ (function (_super) {\n    __extends(ThresholdedReLU, _super);\n    function ThresholdedReLU(args) {\n        var _this = _super.call(this, args == null ? {} : args) || this;\n        _this.DEFAULT_THETA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        _this.theta = args.theta == null ? _this.DEFAULT_THETA : args.theta;\n        return _this;\n    }\n    ThresholdedReLU.prototype.call = function (inputs, kwargs) {\n        var x = types_utils_1.getExactlyOneTensor(inputs);\n        return x.mul(tfjs_backend_1.cast(x.greater(this.theta), 'float32'));\n    };\n    ThresholdedReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ThresholdedReLU.prototype.getConfig = function () {\n        var config = { theta: this.theta };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    ThresholdedReLU.className = 'ThresholdedReLU';\n    return ThresholdedReLU;\n}(topology_1.Layer));\nexports.ThresholdedReLU = ThresholdedReLU;\ntfjs_core_1.serialization.registerClass(ThresholdedReLU);\nvar Softmax = /** @class */ (function (_super) {\n    __extends(Softmax, _super);\n    function Softmax(args) {\n        var _this = _super.call(this, args == null ? {} : args) || this;\n        _this.DEFAULT_AXIS = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        _this.softmax = new activations_1.Softmax().apply;\n        _this.axis = args.axis == null ? _this.DEFAULT_AXIS : args.axis;\n        return _this;\n    }\n    Softmax.prototype.call = function (inputs, kwargs) {\n        var x = types_utils_1.getExactlyOneTensor(inputs);\n        return this.softmax(x, this.axis);\n    };\n    Softmax.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    Softmax.prototype.getConfig = function () {\n        var config = { axis: this.axis };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    Softmax.className = 'Softmax';\n    return Softmax;\n}(topology_1.Layer));\nexports.Softmax = Softmax;\ntfjs_core_1.serialization.registerClass(Softmax);\n"},"sourceMaps":{"js":{"version":3,"file":"advanced_activations.js","sourceRoot":"","sources":["../../src/layers/advanced_activations.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH;;GAEG;AAEH,mDAAsG;AAEtG,8CAA4D;AAC5D,wDAA6C;AAC7C,8CAA8E;AAC9E,+CAA+D;AAC/D,oCAA0D;AAC1D,gDAAyG;AAEzG,gDAAkF;AAElF,oDAA6E;AAU7E;IAA0B,wBAAK;IAK7B,cAAY,IAAoB;QAAhC,YACE,kBAAM,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAKhC;QAJC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,KAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,CAAC;SAC/B;;IACH,CAAC;IAED,mBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,MAAM,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QACrC,IAAI,MAAM,GAAG,gBAAI,CAAC,MAAM,CAAC,CAAC;QAC1B,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;YACzB,MAAM,GAAG,uBAAW,CAAC,MAAM,EAAE,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC,CAAC;SAChD;QACD,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,iCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,wBAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,QAAQ,EAAE,IAAI,CAAC,QAAQ,EAAC,CAAC;QACnE,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA9BD,kBAAkB;IACX,cAAS,GAAG,MAAM,CAAC;IA8B5B,WAAC;CAAA,AAhCD,CAA0B,gBAAK,GAgC9B;AAhCY,oBAAI;AAiCjB,yBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AASlC;IAA+B,6BAAK;IAOlC,mBAAY,IAAyB;QAArC,YACE,kBAAM,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAKhC;QARQ,mBAAa,GAAG,GAAG,CAAC;QAI3B,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QACD,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;;IACpE,CAAC;IAED,wBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QACtC,OAAO,qBAAS,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;IAClC,CAAC;IAED,sCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,6BAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAC,CAAC;QAC7D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA5BD,kBAAkB;IACX,mBAAS,GAAG,WAAW,CAAC;IA4BjC,gBAAC;CAAA,AA9BD,CAA+B,gBAAK,GA8BnC;AA9BY,8BAAS;AA+BtB,yBAAa,CAAC,aAAa,CAAC,SAAS,CAAC,CAAC;AA6BvC;IAA2B,yBAAK;IAW9B,eAAY,IAAqB;QAAjC,YACE,kBAAM,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAqBhC;QAxBQ,+BAAyB,GAA0B,OAAO,CAAC;QAIlE,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QAED,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,gBAAgB;YACjB,6BAAc,CAAC,IAAI,CAAC,gBAAgB,IAAI,KAAI,CAAC,yBAAyB,CAAC,CAAC;QAC5E,KAAI,CAAC,gBAAgB,GAAG,6BAAc,CAAC,IAAI,CAAC,gBAAgB,CAAC,CAAC;QAC9D,KAAI,CAAC,eAAe,GAAG,2BAAa,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;QAC3D,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,KAAI,CAAC,UAAU,GAAG,IAAI,CAAC;SACxB;aAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,UAAU,CAAC,EAAE;YACzC,KAAI,CAAC,UAAU,GAAG,IAAI,CAAC,UAAU,CAAC;SACnC;aAAM,IAAI,OAAO,IAAI,CAAC,UAAU,KAAK,QAAQ,EAAE;YAC9C,KAAI,CAAC,UAAU,GAAG,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;SACrC;aAAM;YACL,MAAM,IAAI,mBAAU,CAChB,6DAA6D;iBAC7D,aAAW,IAAI,CAAC,UAAY,CAAA,CAAC,CAAC;SACnC;;IACH,CAAC;IAED,qBAAK,GAAL,UAAM,UAAyB;QAC7B,UAAU,GAAG,gCAAkB,CAAC,UAAU,CAAC,CAAC;QAC5C,IAAM,UAAU,GAAU,UAAU,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QAC9C,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,KAAgB,UAAe,EAAf,KAAA,IAAI,CAAC,UAAU,EAAf,cAAe,EAAf,IAAe,EAAE;gBAA5B,IAAM,CAAC,SAAA;gBACV,UAAU,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC;aACvB;SACF;QACD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,SAAS,CACvB,OAAO,EAAE,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC,gBAAgB,EACrD,IAAI,CAAC,gBAAgB,EAAE,IAAI,EAAE,IAAI,CAAC,eAAe,CAAC,CAAC;QACvD,kBAAkB;QAClB,IAAM,IAAI,GAA6B,EAAE,CAAC;QAC1C,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBAC1C,IAAI,CAAC,CAAC,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC;aACzB;SACF;QACD,IAAI,CAAC,SAAS,GAAG,CAAC,IAAI,oBAAS,CAAC;gBAC9B,IAAI,EAAE,UAAU,CAAC,MAAM;gBACvB,IAAI,MAAA;aACL,CAAC,CAAC,CAAC;QACJ,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAED,oBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,MAAM,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QACrC,OAAO,iBAAK,CAAC,MAAM,EAAE,IAAI,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;IAC1C,CAAC;IAED,yBAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,gBAAgB,EAAE,mCAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;YAC7D,gBAAgB,EAAE,mCAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;YAC7D,eAAe,EAAE,iCAAmB,CAAC,IAAI,CAAC,eAAe,CAAC;YAC1D,UAAU,EAAE,IAAI,CAAC,UAAU;SAC5B,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA1ED,kBAAkB;IACX,eAAS,GAAG,OAAO,CAAC;IA0E7B,YAAC;CAAA,AA5ED,CAA2B,gBAAK,GA4E/B;AA5EY,sBAAK;AA6ElB,yBAAa,CAAC,aAAa,CAAC,KAAK,CAAC,CAAC;AASnC;IAAyB,uBAAK;IAO5B,aAAY,IAAmB;QAA/B,YACE,kBAAM,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAYhC;QAfQ,mBAAa,GAAG,GAAG,CAAC;QAI3B,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QAED,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,IAAI,IAAI,CAAC,KAAK,KAAK,KAAI,CAAC,aAAa,EAAE;YAC3D,MAAM,IAAI,4BAAmB,CACzB,8BAA4B,IAAI,CAAC,KAAK,+BAA4B;gBAClE,gBAAgB,CAAC,CAAC;SACvB;QAED,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;;IACpE,CAAC;IAED,kBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QACtC,OAAO,eAAG,CAAC,CAAC,CAAC,CAAC;IAChB,CAAC;IAED,gCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,uBAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAC,CAAC;QAC7D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAnCD,kBAAkB;IACX,aAAS,GAAG,KAAK,CAAC;IAmC3B,UAAC;CAAA,AArCD,CAAyB,gBAAK,GAqC7B;AArCY,kBAAG;AAsChB,yBAAa,CAAC,aAAa,CAAC,GAAG,CAAC,CAAC;AASjC;IAAqC,mCAAK;IAOxC,yBAAY,IAA+B;QAA3C,YACE,kBAAM,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAMhC;QATQ,mBAAa,GAAG,GAAG,CAAC;QAI3B,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QAED,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;;IACpE,CAAC;IAED,8BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QACtC,OAAO,CAAC,CAAC,GAAG,CAAC,mBAAI,CAAC,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,EAAE,SAAS,CAAC,CAAC,CAAC;IACvD,CAAC;IAED,4CAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,mCAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAC,CAAC;QAC7D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA7BD,kBAAkB;IACX,yBAAS,GAAG,iBAAiB,CAAC;IA6BvC,sBAAC;CAAA,AA/BD,CAAqC,gBAAK,GA+BzC;AA/BY,0CAAe;AAgC5B,yBAAa,CAAC,aAAa,CAAC,eAAe,CAAC,CAAC;AAU7C;IAA6B,2BAAK;IAOhC,iBAAY,IAAuB;QAAnC,YACE,kBAAM,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAMhC;QATQ,kBAAY,GAAG,GAAG,CAAC;QAI1B,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QACD,KAAI,CAAC,OAAO,GAAG,IAAI,qBAAiB,EAAE,CAAC,KAAK,CAAC;QAC7C,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,YAAY,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;;IAChE,CAAC;IAED,sBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QACtC,OAAO,IAAI,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;IACpC,CAAC;IAED,oCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,2BAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,IAAI,EAAE,IAAI,CAAC,IAAI,EAAC,CAAC;QAC3D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA7BD,kBAAkB;IACX,iBAAS,GAAG,SAAS,CAAC;IA6B/B,cAAC;CAAA,AA/BD,CAA6B,gBAAK,GA+BjC;AA/BY,0BAAO;AAgCpB,yBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\n\nimport {clipByValue, elu, leakyRelu, prelu, relu, serialization, Tensor} from '@tensorflow/tfjs-core';\n\nimport {Softmax as softmaxActivation} from '../activations';\nimport {cast} from '../backend/tfjs_backend';\nimport {Constraint, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nexport declare interface ReLULayerArgs extends LayerArgs {\n  /**\n   * Float, the maximum output value.\n   */\n  maxValue?: number;\n}\n\nexport class ReLU extends Layer {\n  /** @nocollapse */\n  static className = 'ReLU';\n  maxValue: number;\n\n  constructor(args?: ReLULayerArgs) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n    return output;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {maxValue: this.maxValue};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ReLU);\n\nexport declare interface LeakyReLULayerArgs extends LayerArgs {\n  /**\n   * Float `>= 0`. Negative slope coefficient. Defaults to `0.3`.\n   */\n  alpha?: number;\n}\n\nexport class LeakyReLU extends Layer {\n  /** @nocollapse */\n  static className = 'LeakyReLU';\n  readonly alpha: number;\n\n  readonly DEFAULT_ALPHA = 0.3;\n\n  constructor(args?: LeakyReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {alpha: this.alpha};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LeakyReLU);\n\nexport declare interface PReLULayerArgs extends LayerArgs {\n  /**\n   * Initializer for the learnable alpha.\n   */\n  alphaInitializer?: Initializer|InitializerIdentifier;\n\n  /**\n   * Regularizer for the learnable alpha.\n   */\n  alphaRegularizer?: Regularizer;\n\n  /**\n   * Constraint for the learnable alpha.\n   */\n  alphaConstraint?: Constraint;\n\n  /**\n   * The axes along which to share learnable parameters for the activation\n   * function. For example, if the incoming feature maps are from a 2D\n   * convolution with output shape `[numExamples, height, width, channels]`,\n   * and you wish to share parameters across space (height and width) so that\n   * each filter channels has only one set of parameters, set\n   * `shared_axes: [1, 2]`.\n   */\n  sharedAxes?: number|number[];\n}\n\nexport class PReLU extends Layer {\n  /** @nocollapse */\n  static className = 'PReLU';\n  private readonly alphaInitializer: Initializer;\n  private readonly alphaRegularizer: Regularizer;\n  private readonly alphaConstraint: Constraint;\n  private readonly sharedAxes: number[];\n  private alpha: LayerVariable;\n\n  readonly DEFAULT_ALPHA_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  constructor(args?: PReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    this.supportsMasking = true;\n    this.alphaInitializer =\n        getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(\n          `Expected sharedAxes to be a number or an array of numbers, ` +\n          `but got ${args.sharedAxes}`);\n    }\n  }\n\n  build(inputShape: Shape|Shape[]) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape: Shape = inputShape.slice(1);\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n    this.alpha = this.addWeight(\n        'alpha', paramShape, 'float32', this.alphaInitializer,\n        this.alphaRegularizer, true, this.alphaConstraint);\n    // Set input spec.\n    const axes: {[axis: number]: number} = {};\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes,\n    })];\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(PReLU);\n\nexport declare interface ELULayerArgs extends LayerArgs {\n  /**\n   * Float `>= 0`. Negative slope coefficient. Defaults to `1.0`.\n   */\n  alpha?: number;\n}\n\nexport class ELU extends Layer {\n  /** @nocollapse */\n  static className = 'ELU';\n  readonly alpha: number;\n\n  readonly DEFAULT_ALPHA = 1.0;\n\n  constructor(args?: ELULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(\n          `Non-default alpha value (${args.alpha}) is not supported by the ` +\n          `ELU layer yet.`);\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {alpha: this.alpha};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ELU);\n\nexport declare interface ThresholdedReLULayerArgs extends LayerArgs {\n  /**\n   * Float >= 0. Threshold location of activation.\n   */\n  theta?: number;\n}\n\nexport class ThresholdedReLU extends Layer {\n  /** @nocollapse */\n  static className = 'ThresholdedReLU';\n  readonly theta: number;\n\n  readonly DEFAULT_THETA = 1.0;\n\n  constructor(args?: ThresholdedReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return x.mul(cast(x.greater(this.theta), 'float32'));\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {theta: this.theta};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ThresholdedReLU);\n\nexport declare interface SoftmaxLayerArgs extends LayerArgs {\n  /**\n   * Integer, axis along which the softmax normalization is applied.\n   * Defaults to `-1` (i.e., the last axis).\n   */\n  axis?: number;\n}\n\nexport class Softmax extends Layer {\n  /** @nocollapse */\n  static className = 'Softmax';\n  readonly axis: number;\n  readonly softmax: (t: Tensor, a?: number) => Tensor;\n  readonly DEFAULT_AXIS = 1.0;\n\n  constructor(args?: SoftmaxLayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {axis: this.axis};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Softmax);\n"]}},"error":null,"hash":"cd47f0323af90813229b8e930a2c6e78","cacheData":{"env":{}}}