{"id":"node_modules/@tensorflow/tfjs-layers/dist/engine/container.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\engine\\container.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581896610560},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":26,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../backend/state","loc":{"line":27,"column":22},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\state.js"},{"name":"../errors","loc":{"line":28,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../layers/serialization","loc":{"line":29,"column":30},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\serialization.js"},{"name":"../utils/generic_utils","loc":{"line":30,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"},{"name":"../utils/serialization_utils","loc":{"line":31,"column":36},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\serialization_utils.js"},{"name":"../utils/types_utils","loc":{"line":32,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"},{"name":"../variables","loc":{"line":33,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\variables.js"},{"name":"../version","loc":{"line":34,"column":24},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\version.js"},{"name":"./executor","loc":{"line":35,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\executor.js"},{"name":"./input_layer","loc":{"line":36,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\input_layer.js"},{"name":"./topology","loc":{"line":37,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\container.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/* Original source: keras/engine/topology.py */\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar state_1 = require(\"../backend/state\");\nvar errors_1 = require(\"../errors\");\nvar serialization_1 = require(\"../layers/serialization\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar serialization_utils_1 = require(\"../utils/serialization_utils\");\nvar types_utils = require(\"../utils/types_utils\");\nvar variables_1 = require(\"../variables\");\nvar version_1 = require(\"../version\");\nvar executor_1 = require(\"./executor\");\nvar input_layer_1 = require(\"./input_layer\");\nvar topology_1 = require(\"./topology\");\n/**\n * A Container is a directed acyclic graph of layers.\n *\n * It is the topological form of a \"model\". A LayersModel\n * is simply a Container with added training routines.\n *\n */\nvar Container = /** @class */ (function (_super) {\n    __extends(Container, _super);\n    function Container(args) {\n        var _this = \n        // No args passed to super's constructor.\n        _super.call(this, {}) || this;\n        _this.containerNodes = new Set();\n        _this.name = args.name;\n        if (_this.name == null) {\n            var prefix = _this.getClassName().toLowerCase();\n            _this.name = state_1.getUid(prefix);\n        }\n        _this.supportsMasking = false;\n        _this.trainable_ = true;\n        // TODO(michaelterry): Initialize perInputLosses/Updates here.\n        // Container-specific properties.\n        if (Array.isArray(args.inputs)) {\n            _this.inputs = args.inputs.slice();\n        }\n        else {\n            _this.inputs = [args.inputs];\n        }\n        if (Array.isArray(args.outputs)) {\n            _this.outputs = args.outputs.slice();\n        }\n        else {\n            _this.outputs = [args.outputs];\n        }\n        // Check for redundancy in inputs.\n        if (generic_utils.unique(_this.inputs).length !== _this.inputs.length) {\n            throw new errors_1.ValueError('The list of inputs passed to the model is ' +\n                'redundant. All inputs should only appear once. Found: ' +\n                (\"\" + _this.inputs.map(function (x) { return x.name; })));\n        }\n        // Check for redundancy in outputs.\n        if (generic_utils.unique(_this.outputs).length !== _this.outputs.length) {\n            console.warn('The list of outputs passed to the model is redundant. ' +\n                'All outputs should only appear once. Found: ' +\n                (\"\" + _this.outputs.map(function (x) { return x.name; })));\n        }\n        /*\n          List of initial layers (1 to 1 mapping with this.inputs, hence the same\n          layer might appear twice)\n        */\n        _this.inputLayers = [];\n        _this.inputLayersNodeIndices = [];\n        _this.inputLayersTensorIndices = [];\n        /*\n          List of layers (1 to 1 mapping with this.outputs, hence the same layer\n          might appear twice)\n        */\n        _this.outputLayers = [];\n        _this.outputLayersNodeIndices = [];\n        _this.outputLayersTensorIndices = [];\n        /*\n          All layers in order of horizontal graph traversal. Entries are unique.\n          Includes input and output layers.\n        */\n        _this.layers = [];\n        /*\n          References to container layers that were constructed internally. We need\n          these to properly dispose of tensors from nested containers.\n        */\n        _this.internalContainerRefs = [];\n        // TODO(michaelterry): Determine if caching still needed with eager\n        // backend.\n        /*\n          This is for performance optimization when calling the Container on new\n          inputs. Every time the Container is called on a set on input tensors,\n          we compute the output tensors, output masks and output shapes in one pass,\n          then cache them here. When one of these outputs is queried later,\n          we retrieve it from there instead of recomputing it.\n        */\n        // this.outputTensorCache = {};\n        // this.outputShapeCache = {};\n        // Build this.outputLayers:\n        for (var _i = 0, _a = _this.outputs; _i < _a.length; _i++) {\n            var x = _a[_i];\n            var layer = x.sourceLayer;\n            var nodeIndex = x.nodeIndex;\n            var tensorIndex = x.tensorIndex;\n            _this.outputLayers.push(layer);\n            _this.outputLayersNodeIndices.push(nodeIndex);\n            _this.outputLayersTensorIndices.push(tensorIndex);\n        }\n        // TODO(michaelterry): Add output mask cache code.\n        // Build this.inputLayers:\n        for (var _b = 0, _c = _this.inputs; _b < _c.length; _b++) {\n            var x = _c[_b];\n            var layer = x.sourceLayer;\n            var nodeIndex = x.nodeIndex;\n            var tensorIndex = x.tensorIndex;\n            /*\n              It's supposed to be an input layer, so only one node\n              and one tensor output.\n            */\n            generic_utils.assert(nodeIndex === 0, 'input layer has >1 nodes');\n            generic_utils.assert(tensorIndex === 0, 'input layer has >1 tensors');\n            _this.inputLayers.push(layer);\n            _this.inputLayersNodeIndices.push(nodeIndex);\n            _this.inputLayersTensorIndices.push(tensorIndex);\n        }\n        // Build this.inputNames and this.outputNames.\n        _this.inputNames = [];\n        _this.outputNames = [];\n        _this.feedInputShapes = [];\n        _this.feedInputNames = [];\n        _this.feedOutputNames = [];\n        for (var i = 0; i < _this.inputLayers.length; i++) {\n            var layer = _this.inputLayers[i];\n            // Check that layer is an InputLayer.\n            if (!(layer instanceof input_layer_1.InputLayer)) {\n                throw new TypeError('Input layers to a LayersModel must be InputLayer objects. ' +\n                    (\"Received inputs: \" + args.inputs + \". \") +\n                    (\"Input \" + i + \" (0-based) originates \") +\n                    (\"from layer type \" + layer.getClassName() + \".\"));\n            }\n            _this.inputNames.push(layer.name);\n            _this.feedInputShapes.push(layer.batchInputShape);\n            _this.feedInputNames.push(layer.name);\n        }\n        for (var _d = 0, _e = _this.outputLayers; _d < _e.length; _d++) {\n            var layer = _e[_d];\n            _this.outputNames.push(layer.name);\n        }\n        _this.internalInputShapes = _this.inputs.map(function (x) { return x.shape; });\n        _this.internalOutputShapes = _this.outputs.map(function (x) { return x.shape; });\n        /*\n          Container_nodes: set of nodes included in the graph (not all nodes\n          included in the layers are relevant to the current graph).\n        */\n        // ids of all nodes relevant to the Container:\n        var nodesDepths = {};\n        // To recover nodes from their ID.\n        var nodeIDToNode = {};\n        var layersDepths = {};\n        // To layers from their ID.\n        var layerIDToLayer = {};\n        var layerIndices = {};\n        var nodesInDecreasingDepth = [];\n        /**\n         * Builds a map of the graph of layers.\n         *\n         * This recursively updates the map `layerIndices`,\n         * the list `nodesInDecreasingDepth` and the set `containerNodes`.\n         *\n         * @param tensor Some tensor in a graph.\n         * @param finishedNodes Set of nodes whose subgraphs have been traversed\n         *         completely. Useful to prevent duplicated work.\n         * @param nodesInProgress Set of nodes that are currently active on the\n         *         recursion stack. Useful to detect cycles.\n         * @param layer Layer from which `tensor` comes from. If not provided,\n         *   will be obtained from tensor.sourceLayer.\n         * @param nodeIndex Node index from which `tensor` comes from.\n         * @param tensorIndex TensorIndex from which `tensor` comes from.\n         *\n         * @exception RuntimeError if a cycle is detected.\n         */\n        var buildMapOfGraph = function (tensor, finishedNodes, nodesInProgress, layer, nodeIndex, tensorIndex) {\n            if (layer == null || nodeIndex == null || tensorIndex == null) {\n                layer = tensor.sourceLayer;\n                nodeIndex = tensor.nodeIndex;\n                tensorIndex = tensor.tensorIndex;\n            }\n            var node = layer.inboundNodes[nodeIndex];\n            // Prevent cycles.\n            if (nodesInProgress.indexOf(node) !== -1) {\n                throw new errors_1.RuntimeError(\"The tensor \" + tensor.name + \" at layer \\\"\" + layer.name + \"\\\" \" +\n                    'is part of a cycle.');\n            }\n            // Don't repeat work for shared subgraphs\n            if (finishedNodes.indexOf(node) !== -1) {\n                return;\n            }\n            // Update containerNodes.\n            _this.containerNodes.add(Container.nodeKey(layer, nodeIndex));\n            // Store the traversal order for layer sorting.\n            if (!(layer.id in layerIndices)) {\n                layerIndices[layer.id] = Object.keys(layerIndices).length;\n            }\n            if (nodesInProgress.indexOf(node) === -1) {\n                nodesInProgress.push(node);\n            }\n            // Propagate to all previous tensors connected to this node.\n            var numInboundLayers = node.inboundLayers.length;\n            for (var i = 0; i < numInboundLayers; i++) {\n                var x = node.inputTensors[i];\n                var layer_1 = node.inboundLayers[i];\n                var nodeIndex_1 = node.nodeIndices[i];\n                var tensorIndex_1 = node.tensorIndices[i];\n                buildMapOfGraph(x, finishedNodes, nodesInProgress, layer_1, nodeIndex_1, tensorIndex_1);\n            }\n            finishedNodes.push(node);\n            while (nodesInProgress.indexOf(node) >= 0) {\n                nodesInProgress.splice(nodesInProgress.indexOf(node), 1);\n            }\n            nodesInDecreasingDepth.push(node);\n        };\n        var finishedNodes = [];\n        var nodesInProgress = [];\n        for (var _f = 0, _g = _this.outputs; _f < _g.length; _f++) {\n            var x = _g[_f];\n            buildMapOfGraph(x, finishedNodes, nodesInProgress);\n        }\n        var reversedNodesInDecreasingDepth = nodesInDecreasingDepth.slice().reverse();\n        for (var _h = 0, reversedNodesInDecreasingDepth_1 = reversedNodesInDecreasingDepth; _h < reversedNodesInDecreasingDepth_1.length; _h++) {\n            var node = reversedNodesInDecreasingDepth_1[_h];\n            nodeIDToNode[node.id] = node;\n            // If the depth is not set, the node has no outbound nodes (depth 0).\n            if (!(node.id in nodesDepths)) {\n                nodesDepths[node.id] = 0;\n            }\n            var depth = nodesDepths[node.id];\n            // Update the depth of the corresponding layer\n            var previousDepth = (layersDepths[node.outboundLayer.id] == null ?\n                0 :\n                layersDepths[node.outboundLayer.id]);\n            /*\n              If we've seen this layer before at a higher depth, we should use that\n              depth instead of the node depth.  This is necessary for shared layers\n              that have inputs at different depth levels in the graph.\n            */\n            depth = Math.max(depth, previousDepth);\n            layersDepths[node.outboundLayer.id] = depth;\n            layerIDToLayer[node.outboundLayer.id] = node.outboundLayer;\n            nodesDepths[node.id] = depth;\n            // Update the depth of inbound nodes.\n            for (var i = 0; i < node.inboundLayers.length; i++) {\n                var inboundLayer = node.inboundLayers[i];\n                var nodeIndex = node.nodeIndices[i];\n                var inboundNode = inboundLayer.inboundNodes[nodeIndex];\n                var previousDepth_1 = (nodesDepths[inboundNode.id] == null ? 0 :\n                    nodesDepths[inboundNode.id]);\n                nodesDepths[inboundNode.id] = Math.max(depth + 1, previousDepth_1);\n                nodeIDToNode[inboundNode.id] = inboundNode;\n            }\n        }\n        // Build a dict {depth: list of nodes with this depth}\n        var nodesByDepth = {};\n        for (var nodeID in nodesDepths) {\n            var depth = nodesDepths[nodeID];\n            if (!(depth in nodesByDepth)) {\n                nodesByDepth[depth] = [];\n            }\n            nodesByDepth[depth].push(nodeIDToNode[nodeID]);\n        }\n        // Build a dict {depth: list of layers with this depth}\n        var layersByDepth = {};\n        for (var layerID in layersDepths) {\n            var depth = layersDepths[layerID];\n            if (!(depth in layersByDepth)) {\n                layersByDepth[depth] = [];\n            }\n            layersByDepth[depth].push(layerIDToLayer[layerID]);\n        }\n        // Get sorted list of layer depths.\n        var depthKeys = Object.keys(layersByDepth)\n            .map(function (x) { return parseInt(x, 10); })\n            .sort(generic_utils.reverseNumberCompare);\n        // Set this.layers and this.layersByDepth.\n        _this.layers = [];\n        for (var _j = 0, depthKeys_1 = depthKeys; _j < depthKeys_1.length; _j++) {\n            var depth = depthKeys_1[_j];\n            var layersForDepth = layersByDepth[depth];\n            // Container.layers needs to have a deterministic order:\n            // here we order them by traversal order.\n            layersForDepth.sort(function (a, b) {\n                var aIndex = layerIndices[a.id];\n                var bIndex = layerIndices[b.id];\n                if (aIndex < bIndex) {\n                    return -1;\n                }\n                if (aIndex > bIndex) {\n                    return 1;\n                }\n                return 0;\n            });\n            for (var _k = 0, layersForDepth_1 = layersForDepth; _k < layersForDepth_1.length; _k++) {\n                var layer = layersForDepth_1[_k];\n                if (layer instanceof Container) {\n                    _this.internalContainerRefs.push(layer);\n                }\n                _this.layers.push(layer);\n            }\n        }\n        _this.layersByDepth = layersByDepth;\n        // Get sorted list of node depths;\n        depthKeys = Object.keys(nodesByDepth)\n            .map(function (x) { return parseInt(x, 10); })\n            .sort(generic_utils.reverseNumberCompare);\n        // Check that all tensors required are computable.\n        // computable_tensors: all tensors in the graph\n        // that can be computed from the inputs provided.\n        var computableTensors = _this.inputs.slice();\n        // To provide a better error msg.\n        var layersWithCompleteInput = [];\n        for (var _l = 0, depthKeys_2 = depthKeys; _l < depthKeys_2.length; _l++) {\n            var depth = depthKeys_2[_l];\n            for (var _m = 0, _o = nodesByDepth[depth]; _m < _o.length; _m++) {\n                var node = _o[_m];\n                var layer = node.outboundLayer;\n                if (layer != null) {\n                    for (var _p = 0, _q = node.inputTensors; _p < _q.length; _p++) {\n                        var x = _q[_p];\n                        if (computableTensors.indexOf(x) === -1) {\n                            throw new errors_1.RuntimeError(\"Graph disconnected: cannot obtain value for tensor \" + x +\n                                (\" at layer \\\"\" + layer.name + \"\\\". \") +\n                                'The following previous layers were accessed without ' +\n                                (\"issue: \" + layersWithCompleteInput));\n                        }\n                    }\n                    for (var _r = 0, _s = node.outputTensors; _r < _s.length; _r++) {\n                        var x = _s[_r];\n                        computableTensors.push(x);\n                    }\n                    layersWithCompleteInput.push(layer.name);\n                }\n            }\n        }\n        // Set this.containerNodes and this.nodesByDepth.\n        _this.nodesByDepth = nodesByDepth;\n        // Ensure name unicity, which will be crucial for serialization\n        // (since serialized nodes refer to layers by their name).\n        var allNames = _this.layers.map(function (x) { return x.name; });\n        var _loop_1 = function (name_1) {\n            var numOccurrences = allNames.filter(function (x) { return x === name_1; }).length;\n            if (numOccurrences !== 1) {\n                throw new errors_1.RuntimeError(\"The name \\\"\" + name_1 + \"\\\" is used \" + numOccurrences + \" times \" +\n                    'in the model. All layer names should be unique. Layer names: ' +\n                    JSON.stringify(allNames));\n            }\n        };\n        for (var _t = 0, allNames_1 = allNames; _t < allNames_1.length; _t++) {\n            var name_1 = allNames_1[_t];\n            _loop_1(name_1);\n        }\n        // Layer parameters.\n        // The new container starts with a single inbound node\n        // for its inputs, and no outbound nodes.\n        // Will be appended to by future calls to apply().\n        _this.outboundNodes = [];\n        // Will be appended to below, and by future calls to apply().\n        _this.inboundNodes = [];\n        // Create the node linking internal inputs to internal outputs.\n        // (This call has side effects.)\n        // tslint:disable-next-line:no-unused-expression\n        new topology_1.Node({\n            outboundLayer: _this,\n            inboundLayers: [],\n            nodeIndices: [],\n            tensorIndices: [],\n            inputTensors: _this.inputs,\n            outputTensors: _this.outputs,\n            inputMasks: _this.inputs.map(function (x) { return null; }),\n            outputMasks: _this.outputs.map(function (x) { return null; }),\n            inputShapes: _this.inputs.map(function (x) { return x.shape; }),\n            outputShapes: _this.outputs.map(function (x) { return x.shape; })\n        });\n        _this.built = true;\n        _this._refCount = 1; // The ref count of a container always start at 1.\n        return _this;\n    }\n    Container.prototype.assertNotDisposed = function () {\n        if (this._refCount === 0) {\n            throw new Error(\"Container '\" + this.name + \"' is already disposed.\");\n        }\n    };\n    /**\n     * Attempt to dispose a LayersModel's weights.\n     *\n     * This method decrease the reference count of the LayersModel object by 1.\n     *\n     * A LayersModel is reference-counted. Its reference count is incremented by 1\n     * when it is first constructed and when it is used as a Layer of another\n     * LayersModel.\n     *\n     * If the reference count of a LayersModel becomes 0, the `dispose` method of\n     * all its constituent `Layer`s will be called.\n     *\n     * Note: If the reference count is greater than 0 after the decrement, the\n     * `dispose` method of its constituent `Layer`s will *not* be called.\n     *\n     * After a LayersModel is disposed, it cannot be used in calls such as\n     * 'predict`, `evaluate` or `fit` anymore.\n     *\n     * @returns A DisposeResult Object with the following fields:\n     *   - refCountAfterDispose: The reference count of the LayersModel after this\n     *     `dispose()` call.\n     *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n     *     during this `dispose()` call.\n     * @throws {Error} If the layer is not built yet, or if the LayersModel has\n     *   already been disposed.\n     */\n    Container.prototype.dispose = function () {\n        this.assertNotDisposed();\n        var result = { refCountAfterDispose: null, numDisposedVariables: 0 };\n        if (--this._refCount === 0) {\n            for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n                var layer = _a[_i];\n                result.numDisposedVariables += layer.dispose().numDisposedVariables;\n            }\n            // Call dispose on each internally created container layer again to ensure\n            // their refCounts hit zero and their tensors are subsequently deleted.\n            for (var _b = 0, _c = this.internalContainerRefs; _b < _c.length; _b++) {\n                var container = _c[_b];\n                result.numDisposedVariables += container.dispose().numDisposedVariables;\n            }\n        }\n        result.refCountAfterDispose = this._refCount;\n        return result;\n    };\n    Object.defineProperty(Container.prototype, \"trainable\", {\n        get: function () {\n            return this.trainable_;\n        },\n        set: function (trainable) {\n            this.layers.forEach(function (layer) {\n                // tslint:disable-next-line:no-any\n                layer._trainableWeights\n                    .forEach(function (w) { return w.trainable = trainable; });\n            });\n            this.trainable_ = trainable;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Container.prototype, \"trainableWeights\", {\n        get: function () {\n            // Porting Note: This check below is to prevent errors where the\n            //   _trainableWeights inherited from the parent class (Layer) gets\n            //   inadvertently used.\n            if (this._trainableWeights.length > 0) {\n                throw new errors_1.ValueError('Container instance unexpectedly contains _trainableWeights.' +\n                    'The trainable weights of a Container are a union of the ' +\n                    'trainable weights of its consituent Layers. Its own ' +\n                    '_trainableWeights must remain an empty Array.');\n            }\n            if (!this.trainable) {\n                return [];\n            }\n            var weights = [];\n            for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n                var layer = _a[_i];\n                weights = weights.concat(layer.trainableWeights);\n            }\n            return weights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Container.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            var weights = [];\n            for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n                var layer = _a[_i];\n                weights.push.apply(weights, layer.nonTrainableWeights);\n            }\n            if (!this.trainable) {\n                var trainableWeights = [];\n                for (var _b = 0, _c = this.layers; _b < _c.length; _b++) {\n                    var layer = _c[_b];\n                    trainableWeights.push.apply(trainableWeights, layer.trainableWeights);\n                }\n                return trainableWeights.concat(weights);\n            }\n            return weights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Container.prototype, \"weights\", {\n        get: function () {\n            return this.trainableWeights.concat(this.nonTrainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    /**\n     * Loads all layer weights from a JSON object.\n     *\n     * Porting Note: HDF5 weight files cannot be directly loaded in JavaScript /\n     *   TypeScript. The utility script at `scripts/pykeras.py` offers means\n     *   to convert them into JSON strings compatible with this method.\n     * Porting Note: TensorFlow.js Layers supports only loading by name currently.\n     *\n     * @param weights A JSON mapping weight names to weight values as nested\n     *   arrays of numbers, or a `NamedTensorMap`, i.e., a JSON mapping weight\n     *   names to `tf.Tensor` objects.\n     * @param strict Require that the provided weights exactly match those\n     *   required by the container.  Default: `true`.  Passing `false` means that\n     *   extra weights and missing weights will be silently ignored.\n     */\n    Container.prototype.loadWeights = function (weights, strict) {\n        if (strict === void 0) { strict = true; }\n        var nameToWeight = {};\n        var totalWeightsCount = 0;\n        for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n            var layer = _a[_i];\n            for (var _b = 0, _c = layer.weights; _b < _c.length; _b++) {\n                var weight = _c[_b];\n                if (nameToWeight[weight.originalName] != null) {\n                    throw new errors_1.ValueError(\"Duplicate weight name: \" + weight.originalName);\n                }\n                nameToWeight[weight.originalName] = weight;\n                totalWeightsCount++;\n            }\n        }\n        var weightValueTuples = [];\n        for (var name_2 in weights) {\n            if (nameToWeight[name_2] != null) {\n                weightValueTuples.push([nameToWeight[name_2], weights[name_2]]);\n            }\n            else if (strict) {\n                throw new errors_1.ValueError(\"Provided weight data has no target variable: \" + name_2);\n            }\n            delete nameToWeight[name_2];\n        }\n        if (strict) {\n            // Check that all weights are set.\n            var unsetNames = [];\n            for (var name_3 in nameToWeight) {\n                unsetNames.push(name_3);\n            }\n            if (unsetNames.length > 0) {\n                throw new errors_1.ValueError(unsetNames.length + \" of \" + totalWeightsCount + \" weights are not set: \" +\n                    (\"\" + unsetNames));\n            }\n        }\n        variables_1.batchSetValue(weightValueTuples);\n    };\n    /**\n     * Util shared between different serialization methods.\n     * @returns LayersModel config with Keras version information added.\n     */\n    Container.prototype.updatedConfig = function () {\n        var theConfig = this.getConfig();\n        var modelConfig = {};\n        modelConfig['className'] = this.getClassName();\n        modelConfig['config'] = theConfig;\n        modelConfig['kerasVersion'] = \"tfjs-layers \" + version_1.version;\n        // TODO(nielsene): Replace something like K.backend() once\n        // possible.\n        modelConfig['backend'] = 'TensorFlow.js';\n        return modelConfig;\n    };\n    /**\n     * Returns a JSON string containing the network configuration.\n     *\n     * To load a network from a JSON save file, use\n     * models.modelFromJSON(jsonString);\n     * @param extraJsonArgs Unused in tfjs-layers, maintained for PyKeras\n     * @param returnString Whether the return value should be stringified\n     *    (default: `true`).\n     * @returns a JSON string if `returnString` (default), or a JSON object if\n     *   `!returnString`.\n     */\n    // tslint:disable-next-line:no-any\n    Container.prototype.toJSON = function (unused, returnString) {\n        if (returnString === void 0) { returnString = true; }\n        var modelConfig = serialization_utils_1.convertTsToPythonic(this.updatedConfig());\n        return returnString ? JSON.stringify(modelConfig) : modelConfig;\n    };\n    /**\n     * Call the model on new inputs.\n     *\n     * In this case `call` just reapplies all ops in the graph to the new inputs\n     * (e.g. build a new computational graph from the provided inputs).\n     *\n     * @param inputs A tensor or list of tensors.\n     * @param mask A mask or list of masks. A mask can be either a tensor or null\n     *   (no mask).\n     *\n     * @return A tensor if there is a single output, or a list of tensors if there\n     *   are more than one outputs.\n     */\n    Container.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            inputs = generic_utils.toList(inputs);\n            var feedDict = new executor_1.FeedDict();\n            for (var i = 0; i < _this.inputs.length; ++i) {\n                feedDict.add(_this.inputs[i], inputs[i]);\n            }\n            return executor_1.execute(_this.outputs, feedDict, kwargs);\n        });\n    };\n    /**\n     * Computes an output mask tensor.\n     *\n     * @param inputs Tensor or list of tensors.\n     * @param mask Tensor or list of tensors.\n     *\n     * @return null or a tensor (or list of tensors, one per output tensor of the\n     * layer).\n     */\n    Container.prototype.computeMask = function (inputs, mask) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            inputs = generic_utils.toList(inputs);\n            var masks;\n            if (mask == null) {\n                masks = generic_utils.pyListRepeat(null, inputs.length);\n            }\n            else {\n                masks = generic_utils.toList(mask);\n            }\n            // TODO(michaelterry): Add support for mask caching.\n            return _this.runInternalGraph(inputs, masks)[1];\n        });\n    };\n    /**\n     * Computes the output shape of the layer.\n     *\n     * Assumes that the layer will be built to match that input shape provided.\n     *\n     * @param inputShape A shape (tuple of integers) or a list of shape tuples\n     *   (one per output tensor of the layer). Shape tuples can include null for\n     *   free dimensions, instead of an integer.\n     */\n    Container.prototype.computeOutputShape = function (inputShape) {\n        var inputShapes = types_utils.normalizeShapeList(inputShape);\n        if (inputShapes.length !== this.inputLayers.length) {\n            throw new errors_1.ValueError(\"Invalid inputShape argument \" + inputShape + \": \" +\n                (\"model has \" + this.inputLayers.length + \" tensor inputs.\"));\n        }\n        // TODO(michaelterry): Add caching\n        var layersToOutputShapes = {};\n        for (var i = 0; i < inputShapes.length; i++) {\n            var layer = this.inputLayers[i];\n            var inputShape_1 = inputShapes[i];\n            // It's an input layer: computeOutputShape is identity,\n            // and there is only one node and one tensor output.\n            var shapeKey = layer.name + '_0_0';\n            layersToOutputShapes[shapeKey] = inputShape_1;\n        }\n        var depthKeys = Object.keys(this.nodesByDepth)\n            .map(function (x) { return parseInt(x, 10); })\n            .sort(generic_utils.reverseNumberCompare);\n        // Iterate over nodes, by depth level.\n        if (depthKeys.length > 1) {\n            for (var _i = 0, depthKeys_3 = depthKeys; _i < depthKeys_3.length; _i++) {\n                var depth = depthKeys_3[_i];\n                var nodes = this.nodesByDepth[depth];\n                for (var _a = 0, nodes_1 = nodes; _a < nodes_1.length; _a++) {\n                    var node = nodes_1[_a];\n                    // This is always a single layer, never a list.\n                    var layer = node.outboundLayer;\n                    if (this.inputLayers.map(function (x) { return x.id; }).indexOf(layer.id) !== -1) {\n                        // We've already covered the input layers a few lines above.\n                        continue;\n                    }\n                    // Potentially redundant list, same size of node.inputTensors.\n                    var inputShapes_1 = [];\n                    for (var j = 0; j < node.inboundLayers.length; j++) {\n                        var inboundLayer = node.inboundLayers[j];\n                        var nodeIndex_2 = node.nodeIndices[j];\n                        var tensorIndex = node.tensorIndices[j];\n                        var shapeKey = inboundLayer.name + \"_\" + nodeIndex_2 + \"_\" + tensorIndex;\n                        var inputShape_2 = layersToOutputShapes[shapeKey];\n                        inputShapes_1.push(inputShape_2);\n                    }\n                    var outputShape = layer.computeOutputShape(generic_utils.singletonOrArray(inputShapes_1));\n                    var outputShapes_1 = types_utils.normalizeShapeList(outputShape);\n                    var nodeIndex = layer.inboundNodes.indexOf(node);\n                    for (var j = 0; j < outputShapes_1.length; j++) {\n                        var shapeKey = layer.name + \"_\" + nodeIndex + \"_\" + j;\n                        layersToOutputShapes[shapeKey] = outputShapes_1[j];\n                    }\n                }\n            }\n        }\n        // Read final output shapes from layersToOutputShapes.\n        var outputShapes = [];\n        var outputShapeKeys = [];\n        for (var i = 0; i < this.outputLayers.length; i++) {\n            var layer = this.outputLayers[i];\n            var nodeIndex = this.outputLayersNodeIndices[i];\n            var tensorIndex = this.outputLayersTensorIndices[i];\n            var shapeKey = layer.name + \"_\" + nodeIndex + \"_\" + tensorIndex;\n            outputShapeKeys.push(shapeKey);\n        }\n        for (var i = 0; i < outputShapeKeys.length; i++) {\n            var key = outputShapeKeys[i];\n            generic_utils.assert(key in layersToOutputShapes);\n            outputShapes.push(layersToOutputShapes[key]);\n        }\n        // TODO(michaelterry): Update cache\n        return generic_utils.singletonOrArray(outputShapes);\n    };\n    /**\n     * Computes output tensors for new inputs.\n     *\n     * Note:\n     *   - Expects `inputs` to be a list (potentially with 1 element).\n     *\n     * @param inputs List of tensors\n     * @param masks List of masks (tensors or null).\n     * @return Three lists: outputTensors, outputMasks, outputShapes\n     */\n    Container.prototype.runInternalGraph = function (inputs, masks) {\n        if (masks == null) {\n            masks = generic_utils.pyListRepeat(null, inputs.length);\n        }\n        // Dictionary mapping reference tensors to tuples\n        // (computed tensor, compute mask)\n        // we assume a 1:1 mapping from tensor to mask\n        // TODO: raise exception when a `.computeMask()` call\n        // does not return a list the same size as `call`\n        var tensorMap = {};\n        for (var i = 0; i < this.inputs.length; ++i) {\n            var x = this.inputs[i];\n            var y = inputs[i];\n            var mask = masks[i];\n            tensorMap[x.id] = [y, mask];\n        }\n        var depthKeys = Object.keys(this.nodesByDepth)\n            .map(function (x) { return parseInt(x, 10); })\n            .sort(generic_utils.reverseNumberCompare);\n        for (var _i = 0, depthKeys_4 = depthKeys; _i < depthKeys_4.length; _i++) {\n            var depth = depthKeys_4[_i];\n            var nodes = this.nodesByDepth[depth];\n            for (var _a = 0, nodes_2 = nodes; _a < nodes_2.length; _a++) {\n                var node = nodes_2[_a];\n                // This is always a single layer, never a list.\n                var layer = node.outboundLayer;\n                var referenceInputTensors = node.inputTensors;\n                var referenceOutputTensors = node.outputTensors;\n                // If all previous input tensors are available in tensorMap,\n                // then call node.inboundLayer on them.\n                // List of tuples [input, mask]:\n                var computedData = new Array();\n                for (var _b = 0, referenceInputTensors_1 = referenceInputTensors; _b < referenceInputTensors_1.length; _b++) {\n                    var x = referenceInputTensors_1[_b];\n                    if (x.id in tensorMap) {\n                        computedData.push(tensorMap[x.id]);\n                    }\n                }\n                if (computedData.length === referenceInputTensors.length) {\n                    // TODO(michaelterry): Add K.name_scope here, if we need it.\n                    var kwargs = {};\n                    var computedTensors = void 0;\n                    var computedMasks = void 0;\n                    var outputTensors_1 = void 0;\n                    var outputMasks_1 = void 0;\n                    // call layer\n                    if (node.callArgs != null) {\n                        kwargs = node.callArgs;\n                    }\n                    if (computedData.length === 1) {\n                        var _c = computedData[0], computedTensor = _c[0], computedMask = _c[1];\n                        if (kwargs['mask'] == null) {\n                            kwargs['mask'] = computedMask;\n                        }\n                        outputTensors_1 =\n                            generic_utils.toList(layer.call(computedTensor, kwargs));\n                        outputMasks_1 = generic_utils.toList(layer.computeMask(computedTensor, computedMask));\n                        computedTensors = [computedTensor];\n                        computedMasks = [computedMask];\n                    }\n                    else {\n                        computedTensors = computedData.map(function (x) { return x[0]; });\n                        computedMasks = computedData.map(function (x) { return x[1]; });\n                        if (kwargs['mask'] == null) {\n                            kwargs['mask'] = computedMasks;\n                        }\n                        outputTensors_1 =\n                            generic_utils.toList(layer.call(computedTensors, kwargs));\n                        outputMasks_1 = generic_utils.toList(layer.computeMask(computedTensors, computedMasks));\n                    }\n                    if (layer.activityRegularizer) {\n                        throw new errors_1.NotImplementedError('LayersModel invocation with concrete Tensor value(s) in the ' +\n                            'presence of activity regularizer(s) is not supported yet.');\n                    }\n                    // TODO(michaelterry): Add model updates and losses\n                    // Update tensor map.\n                    for (var i = 0; i < referenceOutputTensors.length; ++i) {\n                        var x = referenceOutputTensors[i];\n                        var y = outputTensors_1[i];\n                        var mask = outputMasks_1[i];\n                        tensorMap[x.id] = [y, mask];\n                    }\n                }\n            }\n        }\n        var outputTensors = [];\n        var outputMasks = [];\n        var outputShapes = [];\n        for (var _d = 0, _e = this.outputs; _d < _e.length; _d++) {\n            var x = _e[_d];\n            generic_utils.assert(x.id in tensorMap, \"Could not compute output \" + x.name + \" : \" + x.id);\n            var _f = tensorMap[x.id], tensor = _f[0], mask = _f[1];\n            outputShapes.push(tensor.shape);\n            outputTensors.push(tensor);\n            outputMasks.push(mask);\n        }\n        // TODO(michaelterry): Add support for caches.\n        return [outputTensors, outputMasks, outputShapes];\n    };\n    /**\n     * Builds a map of internal node keys to node ordering.\n     * Used in serializaion a node orderings may change as unused nodes are\n     * dropped. Porting Note:  This helper method was pulled out of getConfig to\n     * improve readability.\n     * @param layers An array of Layers in the model.\n     * @returns Map of Node Keys to index order within the layer.\n     */\n    Container.prototype.buildNodeConversionMap = function (layers) {\n        var nodeConversionMap = {};\n        var keptNodes;\n        for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n            var layer = _a[_i];\n            keptNodes = layer instanceof Container ? 1 : 0;\n            for (var originalNodeIndex = 0; originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {\n                var nodeKey = Container.nodeKey(layer, originalNodeIndex);\n                if (this.containerNodes.has(nodeKey)) {\n                    // i.e. we mark it to be saved\n                    nodeConversionMap[nodeKey] = keptNodes;\n                    keptNodes += 1;\n                }\n            }\n        }\n        return nodeConversionMap;\n    };\n    /**\n     * Retrieves a layer based on either its name (unique) or index.\n     *\n     * Indices are based on order of horizontal graph traversal (bottom-up).\n     *\n     * If both `name` and `index` are specified, `index` takes precedence.\n     *\n     * @param name Name of layer.\n     * @param index Index of layer.\n     * @returns A Layer instance.\n     * @throws ValueError: In case of invalid layer name or index.\n     */\n    /**\n     * @doc {\n     *    heading: 'Layers',\n     *    subheading: 'Classes',\n     *    namespace: 'layers',\n     *    subclasses: ['LayersModel']\n     * }\n     */\n    Container.prototype.getLayer = function (name, index) {\n        if (index != null) {\n            if (this.layers.length <= index) {\n                throw new errors_1.ValueError(\"Was asked to retrieve layer at index \" + index + \", but model only \" +\n                    (\"has \" + this.layers.length + \" layer(s).\"));\n            }\n            else {\n                return this.layers[index];\n            }\n        }\n        else {\n            if (name == null) {\n                throw new errors_1.ValueError('Provide either a layer name or layer index');\n            }\n        }\n        for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n            var layer = _a[_i];\n            if (layer.name === name) {\n                return layer;\n            }\n        }\n        throw new errors_1.ValueError(\"No such layer: \" + name);\n    };\n    /**\n     * Retrieves the Container's current loss values.\n     *\n     * Used for regularizers during training.\n     */\n    Container.prototype.calculateLosses = function () {\n        var _this = this;\n        // Porting Node: This is an augmentation to Container.loss in PyKeras.\n        //   In PyKeras, Container.loss returns symbolic tensors. Here a concrete\n        //   Tensor (specifically Scalar) values are returned. This is due to the\n        //   imperative backend.\n        return tfjs_core_1.tidy(function () {\n            var losses = [];\n            for (var _i = 0, _a = _this.layers; _i < _a.length; _i++) {\n                var layer = _a[_i];\n                for (var nodeIndex = 0; nodeIndex < layer.inboundNodes.length; ++nodeIndex) {\n                    var nodeKey = Container.nodeKey(layer, nodeIndex);\n                    if (_this.containerNodes.has(nodeKey)) {\n                        losses.push.apply(losses, layer.calculateLosses());\n                    }\n                }\n            }\n            // TODO(cais): Add any unconditional model-level losses?\n            return losses;\n        });\n    };\n    Container.prototype.getConfig = function () {\n        var config = { name: this.name };\n        // Build a map from layer unique name (self._node_key)\n        // to the index of the nodes that are saved in the config.\n        // Only nodes in container_nodes are saved.\n        var nodeConversionMap = this.buildNodeConversionMap(this.layers);\n        // Serialize and save the layers in layerConfigs\n        var layerConfigs = [];\n        for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n            var layer = _a[_i];\n            var layerClassName = layer.getClassName();\n            var layerConfig = layer.getConfig();\n            var filteredInboundNodes = [];\n            for (var originalNodeIndex = 0; originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {\n                var node = layer.inboundNodes[originalNodeIndex];\n                var nodeKey = Container.nodeKey(layer, originalNodeIndex);\n                var kwargs = {};\n                if (this.containerNodes.has(nodeKey)) {\n                    // The node is relevant to the model:\n                    // add to filteredInboundNodes.\n                    if (node.callArgs) {\n                        try {\n                            JSON.stringify(node.callArgs);\n                            kwargs = node.callArgs;\n                        }\n                        catch (err) {\n                            console.warn(\"Layer \" + layer.name + \" was passed \" +\n                                \"non-serializable keyword arguments: \" +\n                                (node.callArgs + \". They will not be included \") +\n                                \"in the serialized model (and thus will be \" +\n                                \"missing at deserialization time).\");\n                            kwargs = {};\n                        }\n                    }\n                    if (node.inboundLayers.length > 0) {\n                        var nodeData = [];\n                        for (var i = 0; i < node.inboundLayers.length; i++) {\n                            var inboundLayer = node.inboundLayers[i];\n                            var nodeIndex = node.nodeIndices[i];\n                            var tensorIndex = node.tensorIndices[i];\n                            var nodeKey_1 = Container.nodeKey(inboundLayer, nodeIndex);\n                            var newNodeIndex = nodeConversionMap[nodeKey_1];\n                            if (newNodeIndex == null) {\n                                newNodeIndex = 0;\n                            }\n                            nodeData.push([inboundLayer.name, newNodeIndex, tensorIndex, kwargs]);\n                        }\n                        filteredInboundNodes.push(nodeData);\n                    }\n                }\n            }\n            var dict = {};\n            dict['name'] = layer.name;\n            dict['className'] = layerClassName;\n            dict['config'] = layerConfig;\n            dict['inboundNodes'] = filteredInboundNodes;\n            layerConfigs.push(dict);\n        }\n        config['layers'] = layerConfigs;\n        // Gather info about inputs and outputs\n        var modelInputs = [];\n        for (var i = 0; i < this.inputLayers.length; i++) {\n            var layer = this.inputLayers[i];\n            var nodeIndex = this.inputLayersNodeIndices[i];\n            var nodeKey = Container.nodeKey(layer, nodeIndex);\n            if (!this.containerNodes.has(nodeKey)) {\n                continue;\n            }\n            var newNodeIndex = nodeConversionMap[nodeKey];\n            if (newNodeIndex === null || newNodeIndex === undefined) {\n                newNodeIndex = 0;\n            }\n            var tensorIndex = this.inputLayersTensorIndices[i];\n            modelInputs.push([layer.name, newNodeIndex, tensorIndex]);\n        }\n        config['inputLayers'] = modelInputs;\n        var modelOutputs = [];\n        for (var i = 0; i < this.outputLayers.length; i++) {\n            var layer = this.outputLayers[i];\n            var nodeIndex = this.outputLayersNodeIndices[i];\n            var nodeKey = Container.nodeKey(layer, nodeIndex);\n            if (!this.containerNodes.has(nodeKey)) {\n                continue;\n            }\n            var newNodeIndex = nodeConversionMap[nodeKey];\n            if (newNodeIndex === null || newNodeIndex === undefined) {\n                newNodeIndex = 0;\n            }\n            var tensorIndex = this.outputLayersTensorIndices[i];\n            modelOutputs.push([layer.name, newNodeIndex, tensorIndex]);\n        }\n        config['outputLayers'] = modelOutputs;\n        return config;\n    };\n    /**\n     * Instantiates a LayersModel from its config (output of `get_config()`).\n     * @param cls the class to create\n     * @param config LayersModel config dictionary.\n     * @param customObjects An optional dictionary of custom objects.\n     * @param fastWeightInit Optional flag to use fast weight initialization\n     *   during deserialization. This is applicable to cases in which\n     *   the initialization will be immediately overwritten by loaded weight\n     *   values. Default: `false`.\n     * @returns A LayersModel instance.\n     * @throws ValueError: In case of improperly formatted config dict.\n     */\n    /** @nocollapse */\n    Container.fromConfig = function (cls, config, customObjects, fastWeightInit) {\n        if (customObjects === void 0) { customObjects = {}; }\n        if (fastWeightInit === void 0) { fastWeightInit = false; }\n        // Layer instances created during\n        // the graph reconstruction process\n        var createdLayers = {};\n        // Dictionary mapping layer instances to\n        // node data that specifies a layer call.\n        // It acts as a queue that maintains any unprocessed\n        // layer call until it becomes possible to process it\n        // (i.e. until the input tensors to the call all exist).\n        var unprocessedNodes = {};\n        function addUnprocessedNode(layer, nodeData) {\n            if (!(layer.name in unprocessedNodes)) {\n                unprocessedNodes[layer.name] = [nodeData];\n            }\n            else {\n                unprocessedNodes[layer.name].push(nodeData);\n            }\n        }\n        function processNode(layer, nodeData) {\n            var inputTensors = [];\n            var kwargs;\n            for (var _i = 0, nodeData_1 = nodeData; _i < nodeData_1.length; _i++) {\n                var inputData = nodeData_1[_i];\n                var inboundLayerName = inputData[0];\n                var inboundNodeIndex = inputData[1];\n                var inboundTensorIndex = inputData[2];\n                kwargs = inputData[3] == null ?\n                    {} :\n                    inputData[3];\n                if (!(inboundLayerName in createdLayers)) {\n                    addUnprocessedNode(layer, nodeData);\n                    return;\n                }\n                var inboundLayer = createdLayers[inboundLayerName];\n                if (inboundLayer.inboundNodes.length <= inboundNodeIndex) {\n                    addUnprocessedNode(layer, nodeData);\n                    return;\n                }\n                var inboundNode = inboundLayer.inboundNodes[inboundNodeIndex];\n                inputTensors.push(inboundNode.outputTensors[inboundTensorIndex]);\n            }\n            // Call layer on its inputs, thus creating the node\n            // and building the layer if needed.\n            // Note: This has Eager vs Graph Implications.\n            if (inputTensors.length > 0) {\n                layer.apply(generic_utils.singletonOrArray(inputTensors), kwargs); // was ** kwargs\n            }\n        }\n        /**\n         * Deserialize a layer, then call it on appropriate inputs.\n         * @param layerData: layer config dict.\n         * @throws ValueError: In case of improperly formatted `layer_data`\n         * dict.\n         */\n        function processLayer(layerData) {\n            var layerName = layerData['name'];\n            // Instantiate layer.\n            var layer = serialization_1.deserialize(layerData, config['customObjects'] != null ?\n                config['customObjects'] :\n                {});\n            layer.setFastWeightInitDuringBuild(fastWeightInit);\n            createdLayers[layerName] = layer;\n            // Gather layer inputs.\n            var inboundNodesData = layerData['inboundNodes'];\n            inboundNodesData.forEach(function (nodeData) {\n                if (!(nodeData instanceof Array)) {\n                    throw new errors_1.ValueError(\"Corrupted configuration, expected array for nodeData: \" + nodeData);\n                }\n                // We don't process nodes (i.e. make layer calls)\n                // on the fly because the inbound node may not yet exist,\n                // in case of layer shared at different topological depths\n                // (e.g.a model such as A(B(A(B(x)))))\n                addUnprocessedNode(layer, nodeData);\n            });\n        }\n        // First, we create all layers and enqueue nodes to be processed.\n        var name = config['name'];\n        var layersFromConfig = config['layers'];\n        for (var _i = 0, layersFromConfig_1 = layersFromConfig; _i < layersFromConfig_1.length; _i++) {\n            var layerData = layersFromConfig_1[_i];\n            processLayer(layerData);\n        }\n        // Then we process nodes in order of layer depth.\n        // Nodes that cannot yet be processed(if the inbound node\n        // does not yet exist) are re - enqueued, and the process\n        // is repeated until all nodes are processed.\n        while (!generic_utils.isObjectEmpty(unprocessedNodes)) {\n            for (var _a = 0, layersFromConfig_2 = layersFromConfig; _a < layersFromConfig_2.length; _a++) {\n                var layerData = layersFromConfig_2[_a];\n                var layer = createdLayers[layerData['name']];\n                if (layer.name in unprocessedNodes) {\n                    var currentUnprocessedNodesForLayer = unprocessedNodes[layer.name];\n                    delete unprocessedNodes[layer.name];\n                    for (var _b = 0, currentUnprocessedNodesForLayer_1 = currentUnprocessedNodesForLayer; _b < currentUnprocessedNodesForLayer_1.length; _b++) {\n                        var nodeData = currentUnprocessedNodesForLayer_1[_b];\n                        processNode(layer, nodeData);\n                    }\n                }\n            }\n        }\n        var inputTensors = [];\n        var outputTensors = [];\n        var inputLayersFromConfig = config['inputLayers'];\n        for (var _c = 0, inputLayersFromConfig_1 = inputLayersFromConfig; _c < inputLayersFromConfig_1.length; _c++) {\n            var layerData = inputLayersFromConfig_1[_c];\n            var layerName = layerData[0];\n            var nodeIndex = layerData[1];\n            var tensorIndex = layerData[2];\n            generic_utils.assert(layerName in createdLayers);\n            var layer = createdLayers[layerName];\n            var layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;\n            inputTensors.push(layerOutputTensors[tensorIndex]);\n        }\n        var outputLayersFromConfig = config['outputLayers'];\n        for (var _d = 0, outputLayersFromConfig_1 = outputLayersFromConfig; _d < outputLayersFromConfig_1.length; _d++) {\n            var layerData = outputLayersFromConfig_1[_d];\n            var layerName = layerData[0];\n            var nodeIndex = layerData[1];\n            var tensorIndex = layerData[2];\n            generic_utils.assert(layerName in createdLayers);\n            var layer = createdLayers[layerName];\n            var layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;\n            outputTensors.push(layerOutputTensors[tensorIndex]);\n        }\n        return new cls({ inputs: inputTensors, outputs: outputTensors, name: name });\n    };\n    Object.defineProperty(Container.prototype, \"stateful\", {\n        /**\n         * Determine whether the container is stateful.\n         *\n         * Porting Note: this is the equivalent of the stateful @property of\n         *   the Container class in PyKeras.\n         */\n        get: function () {\n            // Porting Note: This check is to prevent inadvertent setting of the\n            //   _stateful property of the Container instance.\n            if (this._stateful) {\n                throw new errors_1.ValueError('Container instance unexpectedly has _stateful = true. The ' +\n                    'statefulness of a Container is determined by the Layers it ' +\n                    'contains. Its _stateful property must remain the default false.');\n            }\n            for (var _i = 0, _a = this.layers; _i < _a.length; _i++) {\n                var layer = _a[_i];\n                if (layer.stateful) {\n                    return true;\n                }\n            }\n            return false;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    /**\n     * Reset the state of all stateful constituent layers (if any).\n     *\n     * Examples of stateful layers include RNN layers whose `stateful` property\n     * is set as `true`.\n     */\n    Container.prototype.resetStates = function () {\n        var _this = this;\n        tfjs_core_1.tidy(function () {\n            _this.layers.forEach(function (layer) {\n                // tslint:disable:no-any\n                if (layer.stateful) {\n                    layer.resetStates();\n                }\n                // tslint:enable:no-any\n            });\n        });\n    };\n    return Container;\n}(topology_1.Layer));\nexports.Container = Container;\n"},"sourceMaps":{"js":{"version":3,"file":"container.js","sourceRoot":"","sources":["../../src/engine/container.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH,+CAA+C;AAE/C,mDAA0F;AAE1F,0CAAwC;AACxC,oCAAwE;AAIxE,yDAAwE;AAExE,sDAAwD;AACxD,oEAAiE;AACjE,kDAAoD;AACpD,0CAA0D;AAC1D,sCAAoD;AAEpD,uCAA6C;AAC7C,6CAAyC;AACzC,uCAAsE;AAStE;;;;;;GAMG;AACH;IAAwC,6BAAK;IAoC3C,mBAAY,IAAmB;QAA/B;QACE,yCAAyC;QACzC,kBAAM,EAAE,CAAC,SAoYV;QAxZD,oBAAc,GAAG,IAAI,GAAG,EAAU,CAAC;QAqBjC,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;QACtB,IAAI,KAAI,CAAC,IAAI,IAAI,IAAI,EAAE;YACrB,IAAM,MAAM,GAAG,KAAI,CAAC,YAAY,EAAE,CAAC,WAAW,EAAE,CAAC;YACjD,KAAI,CAAC,IAAI,GAAG,cAAM,CAAC,MAAM,CAAC,CAAC;SAC5B;QAED,KAAI,CAAC,eAAe,GAAG,KAAK,CAAC;QAC7B,KAAI,CAAC,UAAU,GAAG,IAAI,CAAC;QAEvB,8DAA8D;QAE9D,iCAAiC;QACjC,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,EAAE;YAC9B,KAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,KAAK,EAAE,CAAC;SACnC;aAAM;YACL,KAAI,CAAC,MAAM,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;SAC7B;QACD,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC,EAAE;YAC/B,KAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC,KAAK,EAAE,CAAC;SACrC;aAAM;YACL,KAAI,CAAC,OAAO,GAAG,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;SAC/B;QAED,kCAAkC;QAClC,IAAI,aAAa,CAAC,MAAM,CAAC,KAAI,CAAC,MAAM,CAAC,CAAC,MAAM,KAAK,KAAI,CAAC,MAAM,CAAC,MAAM,EAAE;YACnE,MAAM,IAAI,mBAAU,CAChB,4CAA4C;gBAC5C,wDAAwD;iBACxD,KAAG,KAAI,CAAC,MAAM,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,IAAI,EAAN,CAAM,CAAG,CAAA,CAAC,CAAC;SACxC;QAED,mCAAmC;QACnC,IAAI,aAAa,CAAC,MAAM,CAAC,KAAI,CAAC,OAAO,CAAC,CAAC,MAAM,KAAK,KAAI,CAAC,OAAO,CAAC,MAAM,EAAE;YACrE,OAAO,CAAC,IAAI,CACR,wDAAwD;gBACxD,8CAA8C;iBAC9C,KAAG,KAAI,CAAC,OAAO,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,IAAI,EAAN,CAAM,CAAG,CAAA,CAAC,CAAC;SACzC;QAED;;;UAGE;QACF,KAAI,CAAC,WAAW,GAAG,EAAE,CAAC;QACtB,KAAI,CAAC,sBAAsB,GAAG,EAAE,CAAC;QACjC,KAAI,CAAC,wBAAwB,GAAG,EAAE,CAAC;QACnC;;;UAGE;QACF,KAAI,CAAC,YAAY,GAAG,EAAE,CAAC;QACvB,KAAI,CAAC,uBAAuB,GAAG,EAAE,CAAC;QAClC,KAAI,CAAC,yBAAyB,GAAG,EAAE,CAAC;QACpC;;;UAGE;QACF,KAAI,CAAC,MAAM,GAAG,EAAE,CAAC;QAEjB;;;UAGE;QACF,KAAI,CAAC,qBAAqB,GAAG,EAAE,CAAC;QAEhC,mEAAmE;QACnE,WAAW;QACX;;;;;;UAME;QACF,+BAA+B;QAC/B,8BAA8B;QAE9B,2BAA2B;QAC3B,KAAgB,UAAY,EAAZ,KAAA,KAAI,CAAC,OAAO,EAAZ,cAAY,EAAZ,IAAY,EAAE;YAAzB,IAAM,CAAC,SAAA;YACV,IAAM,KAAK,GAAG,CAAC,CAAC,WAAW,CAAC;YAC5B,IAAM,SAAS,GAAG,CAAC,CAAC,SAAS,CAAC;YAC9B,IAAM,WAAW,GAAG,CAAC,CAAC,WAAW,CAAC;YAClC,KAAI,CAAC,YAAY,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAC9B,KAAI,CAAC,uBAAuB,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;YAC7C,KAAI,CAAC,yBAAyB,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;SAClD;QAED,kDAAkD;QAElD,0BAA0B;QAC1B,KAAgB,UAAW,EAAX,KAAA,KAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;YAAxB,IAAM,CAAC,SAAA;YACV,IAAM,KAAK,GAAG,CAAC,CAAC,WAAW,CAAC;YAC5B,IAAM,SAAS,GAAG,CAAC,CAAC,SAAS,CAAC;YAC9B,IAAM,WAAW,GAAG,CAAC,CAAC,WAAW,CAAC;YAClC;;;cAGE;YACF,aAAa,CAAC,MAAM,CAAC,SAAS,KAAK,CAAC,EAAE,0BAA0B,CAAC,CAAC;YAClE,aAAa,CAAC,MAAM,CAAC,WAAW,KAAK,CAAC,EAAE,4BAA4B,CAAC,CAAC;YACtE,KAAI,CAAC,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAC7B,KAAI,CAAC,sBAAsB,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;YAC5C,KAAI,CAAC,wBAAwB,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;SACjD;QAED,8CAA8C;QAC9C,KAAI,CAAC,UAAU,GAAG,EAAE,CAAC;QACrB,KAAI,CAAC,WAAW,GAAG,EAAE,CAAC;QACtB,KAAI,CAAC,eAAe,GAAG,EAAE,CAAC;QAC1B,KAAI,CAAC,cAAc,GAAG,EAAE,CAAC;QACzB,KAAI,CAAC,eAAe,GAAG,EAAE,CAAC;QAC1B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAI,CAAC,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YAChD,IAAM,KAAK,GAAG,KAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,qCAAqC;YACrC,IAAI,CAAC,CAAC,KAAK,YAAY,wBAAU,CAAC,EAAE;gBAClC,MAAM,IAAI,SAAS,CACf,4DAA4D;qBAC5D,sBAAoB,IAAI,CAAC,MAAM,OAAI,CAAA;qBACnC,WAAS,CAAC,2BAAwB,CAAA;qBAClC,qBAAmB,KAAK,CAAC,YAAY,EAAE,MAAG,CAAA,CAAC,CAAC;aACjD;YACD,KAAI,CAAC,UAAU,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;YACjC,KAAI,CAAC,eAAe,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC,CAAC;YAEjD,KAAI,CAAC,cAAc,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;SACtC;QACD,KAAoB,UAAiB,EAAjB,KAAA,KAAI,CAAC,YAAY,EAAjB,cAAiB,EAAjB,IAAiB,EAAE;YAAlC,IAAM,KAAK,SAAA;YACd,KAAI,CAAC,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;SACnC;QAED,KAAI,CAAC,mBAAmB,GAAG,KAAI,CAAC,MAAM,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,KAAK,EAAP,CAAO,CAAC,CAAC;QACzD,KAAI,CAAC,oBAAoB,GAAG,KAAI,CAAC,OAAO,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,KAAK,EAAP,CAAO,CAAC,CAAC;QAE3D;;;UAGE;QACF,8CAA8C;QAC9C,IAAM,WAAW,GAA+B,EAAE,CAAC;QACnD,kCAAkC;QAClC,IAAM,YAAY,GAA6B,EAAE,CAAC;QAClD,IAAM,YAAY,GAAgC,EAAE,CAAC;QACrD,2BAA2B;QAC3B,IAAM,cAAc,GAA+B,EAAE,CAAC;QACtD,IAAM,YAAY,GAAgC,EAAE,CAAC;QACrD,IAAM,sBAAsB,GAAW,EAAE,CAAC;QAE1C;;;;;;;;;;;;;;;;;WAiBG;QACH,IAAM,eAAe,GACjB,UAAC,MAAsB,EAAE,aAAqB,EAAE,eAAuB,EACtE,KAAa,EAAE,SAAkB,EAAE,WAAoB;YACtD,IAAI,KAAK,IAAI,IAAI,IAAI,SAAS,IAAI,IAAI,IAAI,WAAW,IAAI,IAAI,EAAE;gBAC7D,KAAK,GAAG,MAAM,CAAC,WAAW,CAAC;gBAC3B,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;gBAC7B,WAAW,GAAG,MAAM,CAAC,WAAW,CAAC;aAClC;YACD,IAAM,IAAI,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC;YAE3C,kBAAkB;YAClB,IAAI,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACxC,MAAM,IAAI,qBAAY,CAClB,gBAAc,MAAM,CAAC,IAAI,oBAAc,KAAK,CAAC,IAAI,QAAI;oBACrD,qBAAqB,CAAC,CAAC;aAC5B;YAED,yCAAyC;YACzC,IAAI,aAAa,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACtC,OAAO;aACR;YAED,yBAAyB;YACzB,KAAI,CAAC,cAAc,CAAC,GAAG,CAAC,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC,CAAC;YAE7D,+CAA+C;YAC/C,IAAI,CAAC,CAAC,KAAK,CAAC,EAAE,IAAI,YAAY,CAAC,EAAE;gBAC/B,YAAY,CAAC,KAAK,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC;aAC3D;YAED,IAAI,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACxC,eAAe,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aAC5B;YAED,4DAA4D;YAC5D,IAAM,gBAAgB,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,CAAC;YACnD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,gBAAgB,EAAE,CAAC,EAAE,EAAE;gBACzC,IAAM,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;gBAC/B,IAAM,OAAK,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBACpC,IAAM,WAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;gBACtC,IAAM,aAAW,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBAC1C,eAAe,CACX,CAAC,EAAE,aAAa,EAAE,eAAe,EAAE,OAAK,EAAE,WAAS,EACnD,aAAW,CAAC,CAAC;aAClB;YACD,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YACzB,OAAO,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;gBACzC,eAAe,CAAC,MAAM,CAAC,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC;aAC1D;YACD,sBAAsB,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QACpC,CAAC,CAAC;QAEN,IAAM,aAAa,GAAW,EAAE,CAAC;QACjC,IAAM,eAAe,GAAW,EAAE,CAAC;QACnC,KAAgB,UAAY,EAAZ,KAAA,KAAI,CAAC,OAAO,EAAZ,cAAY,EAAZ,IAAY,EAAE;YAAzB,IAAM,CAAC,SAAA;YACV,eAAe,CAAC,CAAC,EAAE,aAAa,EAAE,eAAe,CAAC,CAAC;SACpD;QAED,IAAM,8BAA8B,GAChC,sBAAsB,CAAC,KAAK,EAAE,CAAC,OAAO,EAAE,CAAC;QAC7C,KAAmB,UAA8B,EAA9B,iEAA8B,EAA9B,4CAA8B,EAA9B,IAA8B,EAAE;YAA9C,IAAM,IAAI,uCAAA;YACb,YAAY,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;YAC7B,qEAAqE;YACrE,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,IAAI,WAAW,CAAC,EAAE;gBAC7B,WAAW,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC;aAC1B;YACD,IAAI,KAAK,GAAG,WAAW,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YAEjC,8CAA8C;YAC9C,IAAM,aAAa,GACf,CAAC,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,IAAI,IAAI,CAAC,CAAC;gBACzC,CAAC,CAAC,CAAC;gBACH,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,CAAC,CAAC;YAE9C;;;;cAIE;YACF,KAAK,GAAG,IAAI,CAAC,GAAG,CAAC,KAAK,EAAE,aAAa,CAAC,CAAC;YACvC,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC;YAC5C,cAAc,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC;YAC3D,WAAW,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC;YAE7B,qCAAqC;YACrC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;gBAClD,IAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBAC3C,IAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;gBACtC,IAAM,WAAW,GAAG,YAAY,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC;gBACzD,IAAM,eAAa,GACf,CAAC,WAAW,CAAC,WAAW,CAAC,EAAE,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;oBACH,WAAW,CAAC,WAAW,CAAC,EAAE,CAAC,CAAC,CAAC;gBACxE,WAAW,CAAC,WAAW,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,KAAK,GAAG,CAAC,EAAE,eAAa,CAAC,CAAC;gBACjE,YAAY,CAAC,WAAW,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC;aAC5C;SACF;QAED,sDAAsD;QACtD,IAAM,YAAY,GAA8B,EAAE,CAAC;QACnD,KAAK,IAAM,MAAM,IAAI,WAAW,EAAE;YAChC,IAAM,KAAK,GAAG,WAAW,CAAC,MAAM,CAAC,CAAC;YAClC,IAAI,CAAC,CAAC,KAAK,IAAI,YAAY,CAAC,EAAE;gBAC5B,YAAY,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC;aAC1B;YACD,YAAY,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC,MAAM,CAAC,CAAC,CAAC;SAChD;QAED,uDAAuD;QACvD,IAAM,aAAa,GAA+B,EAAE,CAAC;QACrD,KAAK,IAAM,OAAO,IAAI,YAAY,EAAE;YAClC,IAAM,KAAK,GAAG,YAAY,CAAC,OAAO,CAAC,CAAC;YACpC,IAAI,CAAC,CAAC,KAAK,IAAI,aAAa,CAAC,EAAE;gBAC7B,aAAa,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC;aAC3B;YACD,aAAa,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,CAAC;SACpD;QAED,mCAAmC;QACnC,IAAI,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,aAAa,CAAC;aACrB,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,EAAf,CAAe,CAAC;aACzB,IAAI,CAAC,aAAa,CAAC,oBAAoB,CAAC,CAAC;QAE9D,0CAA0C;QAC1C,KAAI,CAAC,MAAM,GAAG,EAAE,CAAC;QACjB,KAAoB,UAAS,EAAT,uBAAS,EAAT,uBAAS,EAAT,IAAS,EAAE;YAA1B,IAAM,KAAK,kBAAA;YACd,IAAM,cAAc,GAAG,aAAa,CAAC,KAAK,CAAC,CAAC;YAC5C,wDAAwD;YACxD,yCAAyC;YACzC,cAAc,CAAC,IAAI,CAAC,UAAC,CAAC,EAAE,CAAC;gBACvB,IAAM,MAAM,GAAG,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBAClC,IAAM,MAAM,GAAG,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBAClC,IAAI,MAAM,GAAG,MAAM,EAAE;oBACnB,OAAO,CAAC,CAAC,CAAC;iBACX;gBACD,IAAI,MAAM,GAAG,MAAM,EAAE;oBACnB,OAAO,CAAC,CAAC;iBACV;gBACD,OAAO,CAAC,CAAC;YACX,CAAC,CAAC,CAAC;YACH,KAAoB,UAAc,EAAd,iCAAc,EAAd,4BAAc,EAAd,IAAc,EAAE;gBAA/B,IAAM,KAAK,uBAAA;gBACd,IAAI,KAAK,YAAY,SAAS,EAAE;oBAC9B,KAAI,CAAC,qBAAqB,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;iBACxC;gBACD,KAAI,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;aACzB;SACF;QACD,KAAI,CAAC,aAAa,GAAG,aAAa,CAAC;QAEnC,kCAAkC;QAClC,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,YAAY,CAAC;aACpB,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,EAAf,CAAe,CAAC;aACzB,IAAI,CAAC,aAAa,CAAC,oBAAoB,CAAC,CAAC;QAE1D,kDAAkD;QAClD,+CAA+C;QAC/C,iDAAiD;QACjD,IAAM,iBAAiB,GAAG,KAAI,CAAC,MAAM,CAAC,KAAK,EAAE,CAAC;QAE9C,iCAAiC;QACjC,IAAM,uBAAuB,GAAa,EAAE,CAAC;QAC7C,KAAoB,UAAS,EAAT,uBAAS,EAAT,uBAAS,EAAT,IAAS,EAAE;YAA1B,IAAM,KAAK,kBAAA;YACd,KAAmB,UAAmB,EAAnB,KAAA,YAAY,CAAC,KAAK,CAAC,EAAnB,cAAmB,EAAnB,IAAmB,EAAE;gBAAnC,IAAM,IAAI,SAAA;gBACb,IAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC;gBACjC,IAAI,KAAK,IAAI,IAAI,EAAE;oBACjB,KAAgB,UAAiB,EAAjB,KAAA,IAAI,CAAC,YAAY,EAAjB,cAAiB,EAAjB,IAAiB,EAAE;wBAA9B,IAAM,CAAC,SAAA;wBACV,IAAI,iBAAiB,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;4BACvC,MAAM,IAAI,qBAAY,CAClB,wDAAsD,CAAG;iCACzD,iBAAc,KAAK,CAAC,IAAI,SAAK,CAAA;gCAC7B,sDAAsD;iCACtD,YAAU,uBAAyB,CAAA,CAAC,CAAC;yBAC1C;qBACF;oBACD,KAAgB,UAAkB,EAAlB,KAAA,IAAI,CAAC,aAAa,EAAlB,cAAkB,EAAlB,IAAkB,EAAE;wBAA/B,IAAM,CAAC,SAAA;wBACV,iBAAiB,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;qBAC3B;oBACD,uBAAuB,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;iBAC1C;aACF;SACF;QAED,iDAAiD;QACjD,KAAI,CAAC,YAAY,GAAG,YAAY,CAAC;QAEjC,+DAA+D;QAC/D,0DAA0D;QAC1D,IAAM,QAAQ,GAAG,KAAI,CAAC,MAAM,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,IAAI,EAAN,CAAM,CAAC,CAAC;gCACnC,MAAI;YACb,IAAM,cAAc,GAAG,QAAQ,CAAC,MAAM,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,KAAK,MAAI,EAAV,CAAU,CAAC,CAAC,MAAM,CAAC;YAC/D,IAAI,cAAc,KAAK,CAAC,EAAE;gBACxB,MAAM,IAAI,qBAAY,CAClB,gBAAa,MAAI,mBAAa,cAAc,YAAS;oBACrD,+DAA+D;oBAC/D,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,CAAC;aAC/B;;QAPH,KAAmB,UAAQ,EAAR,qBAAQ,EAAR,sBAAQ,EAAR,IAAQ;YAAtB,IAAM,MAAI,iBAAA;oBAAJ,MAAI;SAQd;QAED,oBAAoB;QACpB,sDAAsD;QACtD,yCAAyC;QACzC,kDAAkD;QAClD,KAAI,CAAC,aAAa,GAAG,EAAE,CAAC;QACxB,6DAA6D;QAC7D,KAAI,CAAC,YAAY,GAAG,EAAE,CAAC;QAEvB,+DAA+D;QAC/D,gCAAgC;QAChC,gDAAgD;QAChD,IAAI,eAAI,CAAC;YACP,aAAa,EAAE,KAAI;YACnB,aAAa,EAAE,EAAE;YACjB,WAAW,EAAE,EAAE;YACf,aAAa,EAAE,EAAE;YACjB,YAAY,EAAE,KAAI,CAAC,MAAM;YACzB,aAAa,EAAE,KAAI,CAAC,OAAO;YAC3B,UAAU,EAAE,KAAI,CAAC,MAAM,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,IAAI,EAAJ,CAAI,CAAC;YACtC,WAAW,EAAE,KAAI,CAAC,OAAO,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,IAAI,EAAJ,CAAI,CAAC;YACxC,WAAW,EAAE,KAAI,CAAC,MAAM,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,KAAK,EAAP,CAAO,CAAC;YAC1C,YAAY,EAAE,KAAI,CAAC,OAAO,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,KAAK,EAAP,CAAO,CAAC;SAC7C,CAAC,CAAC;QACH,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC;QAClB,KAAI,CAAC,SAAS,GAAG,CAAC,CAAC,CAAE,kDAAkD;;IACzE,CAAC;IAES,qCAAiB,GAA3B;QACE,IAAI,IAAI,CAAC,SAAS,KAAK,CAAC,EAAE;YACxB,MAAM,IAAI,KAAK,CAAC,gBAAc,IAAI,CAAC,IAAI,2BAAwB,CAAC,CAAC;SAClE;IACH,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;OAyBG;IACH,2BAAO,GAAP;QACE,IAAI,CAAC,iBAAiB,EAAE,CAAC;QACzB,IAAM,MAAM,GACQ,EAAC,oBAAoB,EAAE,IAAI,EAAE,oBAAoB,EAAE,CAAC,EAAC,CAAC;QAC1E,IAAI,EAAE,IAAI,CAAC,SAAS,KAAK,CAAC,EAAE;YAC1B,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;gBAA5B,IAAM,KAAK,SAAA;gBACd,MAAM,CAAC,oBAAoB,IAAI,KAAK,CAAC,OAAO,EAAE,CAAC,oBAAoB,CAAC;aACrE;YAED,0EAA0E;YAC1E,uEAAuE;YACvE,KAAwB,UAA0B,EAA1B,KAAA,IAAI,CAAC,qBAAqB,EAA1B,cAA0B,EAA1B,IAA0B,EAAE;gBAA/C,IAAM,SAAS,SAAA;gBAClB,MAAM,CAAC,oBAAoB,IAAI,SAAS,CAAC,OAAO,EAAE,CAAC,oBAAoB,CAAC;aACzE;SACF;QACD,MAAM,CAAC,oBAAoB,GAAG,IAAI,CAAC,SAAS,CAAC;QAC7C,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,sBAAI,gCAAS;aAAb;YACE,OAAO,IAAI,CAAC,UAAU,CAAC;QACzB,CAAC;aAED,UAAc,SAAkB;YAC9B,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,UAAA,KAAK;gBACvB,kCAAkC;gBAChC,KAAa,CAAC,iBAAqC;qBAChD,OAAO,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,SAAS,GAAG,SAAS,EAAvB,CAAuB,CAAC,CAAC;YAC7C,CAAC,CAAC,CAAC;YACH,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC;QAC9B,CAAC;;;OATA;IAWD,sBAAI,uCAAgB;aAApB;YACE,gEAAgE;YAChE,mEAAmE;YACnE,wBAAwB;YACxB,IAAI,IAAI,CAAC,iBAAiB,CAAC,MAAM,GAAG,CAAC,EAAE;gBACrC,MAAM,IAAI,mBAAU,CAChB,6DAA6D;oBAC7D,0DAA0D;oBAC1D,sDAAsD;oBACtD,+CAA+C,CAAC,CAAC;aACtD;YAED,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;gBACnB,OAAO,EAAE,CAAC;aACX;YACD,IAAI,OAAO,GAAoB,EAAE,CAAC;YAClC,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;gBAA5B,IAAM,KAAK,SAAA;gBACd,OAAO,GAAG,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,gBAAgB,CAAC,CAAC;aAClD;YACD,OAAO,OAAO,CAAC;QACjB,CAAC;;;OAAA;IAED,sBAAI,0CAAmB;aAAvB;YACE,IAAM,OAAO,GAAoB,EAAE,CAAC;YACpC,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;gBAA5B,IAAM,KAAK,SAAA;gBACd,OAAO,CAAC,IAAI,OAAZ,OAAO,EAAS,KAAK,CAAC,mBAAmB,EAAE;aAC5C;YACD,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;gBACnB,IAAM,gBAAgB,GAAoB,EAAE,CAAC;gBAC7C,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;oBAA5B,IAAM,KAAK,SAAA;oBACd,gBAAgB,CAAC,IAAI,OAArB,gBAAgB,EAAS,KAAK,CAAC,gBAAgB,EAAE;iBAClD;gBACD,OAAO,gBAAgB,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;aACzC;YACD,OAAO,OAAO,CAAC;QACjB,CAAC;;;OAAA;IAED,sBAAI,8BAAO;aAAX;YACE,OAAO,IAAI,CAAC,gBAAgB,CAAC,MAAM,CAAC,IAAI,CAAC,mBAAmB,CAAC,CAAC;QAChE,CAAC;;;OAAA;IAED;;;;;;;;;;;;;;OAcG;IACH,+BAAW,GAAX,UAAY,OAAuB,EAAE,MAAa;QAAb,uBAAA,EAAA,aAAa;QAChD,IAAM,YAAY,GAAoC,EAAE,CAAC;QACzD,IAAI,iBAAiB,GAAG,CAAC,CAAC;QAC1B,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;YAA5B,IAAM,KAAK,SAAA;YACd,KAAqB,UAAa,EAAb,KAAA,KAAK,CAAC,OAAO,EAAb,cAAa,EAAb,IAAa,EAAE;gBAA/B,IAAM,MAAM,SAAA;gBACf,IAAI,YAAY,CAAC,MAAM,CAAC,YAAY,CAAC,IAAI,IAAI,EAAE;oBAC7C,MAAM,IAAI,mBAAU,CAAC,4BAA0B,MAAM,CAAC,YAAc,CAAC,CAAC;iBACvE;gBACD,YAAY,CAAC,MAAM,CAAC,YAAY,CAAC,GAAG,MAAM,CAAC;gBAC3C,iBAAiB,EAAE,CAAC;aACrB;SACF;QAED,IAAM,iBAAiB,GAAmC,EAAE,CAAC;QAC7D,KAAK,IAAM,MAAI,IAAI,OAAO,EAAE;YAC1B,IAAI,YAAY,CAAC,MAAI,CAAC,IAAI,IAAI,EAAE;gBAC9B,iBAAiB,CAAC,IAAI,CAAC,CAAC,YAAY,CAAC,MAAI,CAAC,EAAE,OAAO,CAAC,MAAI,CAAC,CAAC,CAAC,CAAC;aAC7D;iBAAM,IAAI,MAAM,EAAE;gBACjB,MAAM,IAAI,mBAAU,CAChB,kDAAgD,MAAM,CAAC,CAAC;aAC7D;YACD,OAAO,YAAY,CAAC,MAAI,CAAC,CAAC;SAC3B;QAED,IAAI,MAAM,EAAE;YACV,kCAAkC;YAClC,IAAM,UAAU,GAAa,EAAE,CAAC;YAChC,KAAK,IAAM,MAAI,IAAI,YAAY,EAAE;gBAC/B,UAAU,CAAC,IAAI,CAAC,MAAI,CAAC,CAAC;aACvB;YACD,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;gBACzB,MAAM,IAAI,mBAAU,CACb,UAAU,CAAC,MAAM,YAChB,iBAAiB,2BAAwB;qBAC7C,KAAG,UAAY,CAAA,CAAC,CAAC;aACtB;SACF;QAED,yBAAa,CAAC,iBAAiB,CAAC,CAAC;IACnC,CAAC;IAED;;;OAGG;IACO,iCAAa,GAAvB;QACE,IAAM,SAAS,GAAG,IAAI,CAAC,SAAS,EAAE,CAAC;QACnC,IAAM,WAAW,GAA6B,EAAE,CAAC;QACjD,WAAW,CAAC,WAAW,CAAC,GAAG,IAAI,CAAC,YAAY,EAAE,CAAC;QAC/C,WAAW,CAAC,QAAQ,CAAC,GAAG,SAAS,CAAC;QAClC,WAAW,CAAC,cAAc,CAAC,GAAG,iBAAe,iBAAe,CAAC;QAC7D,0DAA0D;QAC1D,YAAY;QACZ,WAAW,CAAC,SAAS,CAAC,GAAG,eAAe,CAAC;QACzC,OAAO,WAAW,CAAC;IACrB,CAAC;IAED;;;;;;;;;;OAUG;IACH,kCAAkC;IAClC,0BAAM,GAAN,UAAO,MAAY,EAAE,YAAmB;QAAnB,6BAAA,EAAA,mBAAmB;QACtC,IAAM,WAAW,GAAG,yCAAmB,CAAC,IAAI,CAAC,aAAa,EAAE,CAAe,CAAC;QAC5E,OAAO,YAAY,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC;IAClE,CAAC;IAED;;;;;;;;;;;;OAYG;IACH,wBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBASC;QARC,OAAO,gBAAI,CAAC;YACV,MAAM,GAAG,aAAa,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YACtC,IAAM,QAAQ,GAAG,IAAI,mBAAQ,EAAE,CAAC;YAChC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBAC3C,QAAQ,CAAC,GAAG,CAAC,KAAI,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACzC;YACD,OAAO,kBAAO,CAAC,KAAI,CAAC,OAAO,EAAE,QAAQ,EAAE,MAAM,CAAsB,CAAC;QACtE,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;;;OAQG;IACH,+BAAW,GAAX,UAAY,MAAuB,EAAE,IAAsB;QAA3D,iBAaC;QAXC,OAAO,gBAAI,CAAC;YACV,MAAM,GAAG,aAAa,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YACtC,IAAI,KAAe,CAAC;YACpB,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,KAAK,GAAG,aAAa,CAAC,YAAY,CAAC,IAAI,EAAE,MAAM,CAAC,MAAM,CAAC,CAAC;aACzD;iBAAM;gBACL,KAAK,GAAG,aAAa,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC;aACpC;YACD,oDAAoD;YACpD,OAAO,KAAI,CAAC,gBAAgB,CAAC,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QACjD,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;;;OAQG;IACH,sCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,IAAM,WAAW,GAAG,WAAW,CAAC,kBAAkB,CAAC,UAAU,CAAC,CAAC;QAC/D,IAAI,WAAW,CAAC,MAAM,KAAK,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE;YAClD,MAAM,IAAI,mBAAU,CAChB,iCAA+B,UAAU,OAAI;iBAC7C,eAAa,IAAI,CAAC,WAAW,CAAC,MAAM,oBAAiB,CAAA,CAAC,CAAC;SAC5D;QAED,kCAAkC;QAClC,IAAM,oBAAoB,GAAgC,EAAE,CAAC;QAC7D,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YAC3C,IAAM,KAAK,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,IAAM,YAAU,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,uDAAuD;YACvD,oDAAoD;YACpD,IAAM,QAAQ,GAAG,KAAK,CAAC,IAAI,GAAG,MAAM,CAAC;YACrC,oBAAoB,CAAC,QAAQ,CAAC,GAAG,YAAU,CAAC;SAC7C;QAED,IAAM,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,YAAY,CAAC;aACzB,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,EAAf,CAAe,CAAC;aACzB,IAAI,CAAC,aAAa,CAAC,oBAAoB,CAAC,CAAC;QAChE,sCAAsC;QACtC,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE;YACxB,KAAoB,UAAS,EAAT,uBAAS,EAAT,uBAAS,EAAT,IAAS,EAAE;gBAA1B,IAAM,KAAK,kBAAA;gBACd,IAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC;gBACvC,KAAmB,UAAK,EAAL,eAAK,EAAL,mBAAK,EAAL,IAAK,EAAE;oBAArB,IAAM,IAAI,cAAA;oBACb,+CAA+C;oBAC/C,IAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC;oBACjC,IAAI,IAAI,CAAC,WAAW,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,EAAE,EAAJ,CAAI,CAAC,CAAC,OAAO,CAAC,KAAK,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,EAAE;wBAC5D,4DAA4D;wBAC5D,SAAS;qBACV;oBACD,8DAA8D;oBAC9D,IAAM,aAAW,GAAY,EAAE,CAAC;oBAChC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;wBAClD,IAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;wBAC3C,IAAM,WAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;wBACtC,IAAM,WAAW,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;wBAC1C,IAAM,QAAQ,GAAM,YAAY,CAAC,IAAI,SAAI,WAAS,SAAI,WAAa,CAAC;wBACpE,IAAM,YAAU,GAAG,oBAAoB,CAAC,QAAQ,CAAC,CAAC;wBAClD,aAAW,CAAC,IAAI,CAAC,YAAU,CAAC,CAAC;qBAC9B;oBAED,IAAM,WAAW,GAAG,KAAK,CAAC,kBAAkB,CACxC,aAAa,CAAC,gBAAgB,CAAC,aAAW,CAAC,CAAC,CAAC;oBAEjD,IAAM,cAAY,GAAG,WAAW,CAAC,kBAAkB,CAAC,WAAW,CAAC,CAAC;oBACjE,IAAM,SAAS,GAAG,KAAK,CAAC,YAAY,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC;oBACnD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,cAAY,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;wBAC5C,IAAM,QAAQ,GAAM,KAAK,CAAC,IAAI,SAAI,SAAS,SAAI,CAAG,CAAC;wBACnD,oBAAoB,CAAC,QAAQ,CAAC,GAAG,cAAY,CAAC,CAAC,CAAC,CAAC;qBAClD;iBACF;aACF;SACF;QAED,sDAAsD;QACtD,IAAM,YAAY,GAAY,EAAE,CAAC;QACjC,IAAM,eAAe,GAAa,EAAE,CAAC;QACrC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YACjD,IAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;YACnC,IAAM,SAAS,GAAG,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,CAAC;YAClD,IAAM,WAAW,GAAG,IAAI,CAAC,yBAAyB,CAAC,CAAC,CAAC,CAAC;YACtD,IAAM,QAAQ,GAAM,KAAK,CAAC,IAAI,SAAI,SAAS,SAAI,WAAa,CAAC;YAC7D,eAAe,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;SAChC;QAED,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,eAAe,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YAC/C,IAAM,GAAG,GAAG,eAAe,CAAC,CAAC,CAAC,CAAC;YAC/B,aAAa,CAAC,MAAM,CAAC,GAAG,IAAI,oBAAoB,CAAC,CAAC;YAClD,YAAY,CAAC,IAAI,CAAC,oBAAoB,CAAC,GAAG,CAAC,CAAC,CAAC;SAC9C;QAED,mCAAmC;QACnC,OAAO,aAAa,CAAC,gBAAgB,CAAC,YAAY,CAAC,CAAC;IACtD,CAAC;IAED;;;;;;;;;OASG;IACO,oCAAgB,GAA1B,UAA2B,MAAgB,EAAE,KAAgB;QAE3D,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,KAAK,GAAG,aAAa,CAAC,YAAY,CAAC,IAAI,EAAE,MAAM,CAAC,MAAM,CAAC,CAAC;SACzD;QAED,iDAAiD;QACjD,kCAAkC;QAClC,8CAA8C;QAC9C,qDAAqD;QACrD,iDAAiD;QACjD,IAAM,SAAS,GAA2C,EAAE,CAAC;QAC7D,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;YAC3C,IAAM,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;YACzB,IAAM,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACpB,IAAM,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC;YACtB,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;SAC7B;QAED,IAAM,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,YAAY,CAAC;aACzB,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,EAAf,CAAe,CAAC;aACzB,IAAI,CAAC,aAAa,CAAC,oBAAoB,CAAC,CAAC;QAChE,KAAoB,UAAS,EAAT,uBAAS,EAAT,uBAAS,EAAT,IAAS,EAAE;YAA1B,IAAM,KAAK,kBAAA;YACd,IAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC;YACvC,KAAmB,UAAK,EAAL,eAAK,EAAL,mBAAK,EAAL,IAAK,EAAE;gBAArB,IAAM,IAAI,cAAA;gBACb,+CAA+C;gBAC/C,IAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC;gBACjC,IAAM,qBAAqB,GAAG,IAAI,CAAC,YAAY,CAAC;gBAChD,IAAM,sBAAsB,GAAG,IAAI,CAAC,aAAa,CAAC;gBAElD,4DAA4D;gBAC5D,uCAAuC;gBACvC,gCAAgC;gBAChC,IAAM,YAAY,GAAG,IAAI,KAAK,EAAoB,CAAC;gBACnD,KAAgB,UAAqB,EAArB,+CAAqB,EAArB,mCAAqB,EAArB,IAAqB,EAAE;oBAAlC,IAAM,CAAC,8BAAA;oBACV,IAAI,CAAC,CAAC,EAAE,IAAI,SAAS,EAAE;wBACrB,YAAY,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;qBACpC;iBACF;gBACD,IAAI,YAAY,CAAC,MAAM,KAAK,qBAAqB,CAAC,MAAM,EAAE;oBACxD,4DAA4D;oBAC5D,IAAI,MAAM,GAAW,EAAE,CAAC;oBACxB,IAAI,eAAe,SAAU,CAAC;oBAC9B,IAAI,aAAa,SAAU,CAAC;oBAC5B,IAAI,eAAa,SAAU,CAAC;oBAC5B,IAAI,aAAW,SAAU,CAAC;oBAC1B,aAAa;oBACb,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;wBACzB,MAAM,GAAG,IAAI,CAAC,QAAQ,CAAC;qBACxB;oBACD,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;wBACvB,IAAA,oBAAgD,EAA/C,sBAAc,EAAE,oBAA+B,CAAC;wBACvD,IAAI,MAAM,CAAC,MAAM,CAAC,IAAI,IAAI,EAAE;4BAC1B,MAAM,CAAC,MAAM,CAAC,GAAG,YAAY,CAAC;yBAC/B;wBACD,eAAa;4BACT,aAAa,CAAC,MAAM,CAAC,KAAK,CAAC,IAAI,CAAC,cAAc,EAAE,MAAM,CAAC,CAAC,CAAC;wBAC7D,aAAW,GAAG,aAAa,CAAC,MAAM,CAC9B,KAAK,CAAC,WAAW,CAAC,cAAc,EAAE,YAAY,CAAC,CAAC,CAAC;wBACrD,eAAe,GAAG,CAAC,cAAc,CAAC,CAAC;wBACnC,aAAa,GAAG,CAAC,YAAY,CAAC,CAAC;qBAChC;yBAAM;wBACL,eAAe,GAAG,YAAY,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,CAAC,CAAC,EAAJ,CAAI,CAAC,CAAC;wBAC9C,aAAa,GAAG,YAAY,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,CAAC,CAAC,CAAC,EAAJ,CAAI,CAAC,CAAC;wBAC5C,IAAI,MAAM,CAAC,MAAM,CAAC,IAAI,IAAI,EAAE;4BAC1B,MAAM,CAAC,MAAM,CAAC,GAAG,aAAa,CAAC;yBAChC;wBACD,eAAa;4BACT,aAAa,CAAC,MAAM,CAAC,KAAK,CAAC,IAAI,CAAC,eAAe,EAAE,MAAM,CAAC,CAAC,CAAC;wBAC9D,aAAW,GAAG,aAAa,CAAC,MAAM,CAC9B,KAAK,CAAC,WAAW,CAAC,eAAe,EAAE,aAAa,CAAC,CAAC,CAAC;qBACxD;oBAED,IAAI,KAAK,CAAC,mBAAmB,EAAE;wBAC7B,MAAM,IAAI,4BAAmB,CACzB,8DAA8D;4BAC9D,2DAA2D,CAAC,CAAC;qBAClE;oBACD,mDAAmD;oBAEnD,qBAAqB;oBACrB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,sBAAsB,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;wBACtD,IAAM,CAAC,GAAG,sBAAsB,CAAC,CAAC,CAAC,CAAC;wBACpC,IAAM,CAAC,GAAG,eAAa,CAAC,CAAC,CAAC,CAAC;wBAC3B,IAAM,IAAI,GAAG,aAAW,CAAC,CAAC,CAAC,CAAC;wBAC5B,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;qBAC7B;iBACF;aACF;SACF;QAED,IAAM,aAAa,GAAa,EAAE,CAAC;QACnC,IAAM,WAAW,GAAa,EAAE,CAAC;QACjC,IAAM,YAAY,GAAY,EAAE,CAAC;QACjC,KAAgB,UAAY,EAAZ,KAAA,IAAI,CAAC,OAAO,EAAZ,cAAY,EAAZ,IAAY,EAAE;YAAzB,IAAM,CAAC,SAAA;YACV,aAAa,CAAC,MAAM,CAChB,CAAC,CAAC,EAAE,IAAI,SAAS,EAAE,8BAA4B,CAAC,CAAC,IAAI,WAAM,CAAC,CAAC,EAAI,CAAC,CAAC;YACjE,IAAA,oBAAgC,EAA/B,cAAM,EAAE,YAAuB,CAAC;YACvC,YAAY,CAAC,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;YAChC,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;YAC3B,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACxB;QAED,8CAA8C;QAC9C,OAAO,CAAC,aAAa,EAAE,WAAW,EAAE,YAAY,CAAC,CAAC;IACpD,CAAC;IAED;;;;;;;OAOG;IACK,0CAAsB,GAA9B,UAA+B,MAAe;QAC5C,IAAM,iBAAiB,GAAgC,EAAE,CAAC;QAC1D,IAAI,SAAiB,CAAC;QACtB,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;YAA5B,IAAM,KAAK,SAAA;YACd,SAAS,GAAG,KAAK,YAAY,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YAC/C,KAAK,IAAI,iBAAiB,GAAG,CAAC,EACzB,iBAAiB,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EAAE,iBAAiB,EAAE,EAAE;gBACvE,IAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,iBAAiB,CAAC,CAAC;gBAC5D,IAAI,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;oBACpC,8BAA8B;oBAC9B,iBAAiB,CAAC,OAAO,CAAC,GAAG,SAAS,CAAC;oBACvC,SAAS,IAAI,CAAC,CAAC;iBAChB;aACF;SACF;QACD,OAAO,iBAAiB,CAAC;IAC3B,CAAC;IAED;;;;;;;;;;;OAWG;IACH;;;;;;;OAOG;IACH,4BAAQ,GAAR,UAAS,IAAa,EAAE,KAAc;QACpC,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM,IAAI,KAAK,EAAE;gBAC/B,MAAM,IAAI,mBAAU,CAChB,0CAAwC,KAAK,sBAAmB;qBAChE,SAAO,IAAI,CAAC,MAAM,CAAC,MAAM,eAAY,CAAA,CAAC,CAAC;aAC5C;iBAAM;gBACL,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;aAC3B;SACF;aAAM;YACL,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,MAAM,IAAI,mBAAU,CAAC,4CAA4C,CAAC,CAAC;aACpE;SACF;QAED,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;YAA5B,IAAM,KAAK,SAAA;YACd,IAAI,KAAK,CAAC,IAAI,KAAK,IAAI,EAAE;gBACvB,OAAO,KAAK,CAAC;aACd;SACF;QACD,MAAM,IAAI,mBAAU,CAAC,oBAAkB,IAAM,CAAC,CAAC;IACjD,CAAC;IAED;;;;OAIG;IACH,mCAAe,GAAf;QAAA,iBAmBC;QAlBC,sEAAsE;QACtE,yEAAyE;QACzE,yEAAyE;QACzE,wBAAwB;QACxB,OAAO,gBAAI,CAAC;YACV,IAAM,MAAM,GAAa,EAAE,CAAC;YAC5B,KAAoB,UAAW,EAAX,KAAA,KAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;gBAA5B,IAAM,KAAK,SAAA;gBACd,KAAK,IAAI,SAAS,GAAG,CAAC,EAAE,SAAS,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EACxD,EAAE,SAAS,EAAE;oBAChB,IAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;oBACpD,IAAI,KAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;wBACpC,MAAM,CAAC,IAAI,OAAX,MAAM,EAAS,KAAK,CAAC,eAAe,EAAE,EAAE;qBACzC;iBACF;aACF;YACD,wDAAwD;YACxD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAED,6BAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,IAAI,EAAE,IAAI,CAAC,IAAI,EAAC,CAAC;QAE3D,sDAAsD;QACtD,0DAA0D;QAC1D,2CAA2C;QAC3C,IAAM,iBAAiB,GACnB,IAAI,CAAC,sBAAsB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;QAE7C,gDAAgD;QAChD,IAAM,YAAY,GAAG,EAAE,CAAC;QACxB,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;YAA5B,IAAM,KAAK,SAAA;YACd,IAAM,cAAc,GAAG,KAAK,CAAC,YAAY,EAAE,CAAC;YAC5C,IAAM,WAAW,GAAG,KAAK,CAAC,SAAS,EAAE,CAAC;YACtC,IAAM,oBAAoB,GAAG,EAAE,CAAC;YAChC,KAAK,IAAI,iBAAiB,GAAG,CAAC,EACzB,iBAAiB,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EAAE,iBAAiB,EAAE,EAAE;gBACvE,IAAM,IAAI,GAAG,KAAK,CAAC,YAAY,CAAC,iBAAiB,CAAC,CAAC;gBACnD,IAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,iBAAiB,CAAC,CAAC;gBAC5D,IAAI,MAAM,GAAG,EAAE,CAAC;gBAChB,IAAI,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;oBACpC,qCAAqC;oBACrC,+BAA+B;oBAC/B,IAAI,IAAI,CAAC,QAAQ,EAAE;wBACjB,IAAI;4BACF,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;4BAC9B,MAAM,GAAG,IAAI,CAAC,QAAQ,CAAC;yBACxB;wBAAC,OAAO,GAAG,EAAE;4BACZ,OAAO,CAAC,IAAI,CACR,WAAS,KAAK,CAAC,IAAI,iBAAc;gCACjC,sCAAsC;iCACnC,IAAI,CAAC,QAAQ,iCAA8B,CAAA;gCAC9C,4CAA4C;gCAC5C,mCAAmC,CAAC,CAAC;4BACzC,MAAM,GAAG,EAAE,CAAC;yBACb;qBACF;oBACD,IAAI,IAAI,CAAC,aAAa,CAAC,MAAM,GAAG,CAAC,EAAE;wBACjC,IAAM,QAAQ,GAAG,EAAE,CAAC;wBACpB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;4BAClD,IAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;4BAC3C,IAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;4BACtC,IAAM,WAAW,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;4BAC1C,IAAM,SAAO,GAAG,SAAS,CAAC,OAAO,CAAC,YAAY,EAAE,SAAS,CAAC,CAAC;4BAC3D,IAAI,YAAY,GAAG,iBAAiB,CAAC,SAAO,CAAC,CAAC;4BAC9C,IAAI,YAAY,IAAI,IAAI,EAAE;gCACxB,YAAY,GAAG,CAAC,CAAC;6BAClB;4BACD,QAAQ,CAAC,IAAI,CACT,CAAC,YAAY,CAAC,IAAI,EAAE,YAAY,EAAE,WAAW,EAAE,MAAM,CAAC,CAAC,CAAC;yBAC7D;wBACD,oBAAoB,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;qBACrC;iBACF;aACF;YACD,IAAM,IAAI,GAA6B,EAAE,CAAC;YAC1C,IAAI,CAAC,MAAM,CAAC,GAAG,KAAK,CAAC,IAAI,CAAC;YAC1B,IAAI,CAAC,WAAW,CAAC,GAAG,cAAc,CAAC;YACnC,IAAI,CAAC,QAAQ,CAAC,GAAG,WAAW,CAAC;YAC7B,IAAI,CAAC,cAAc,CAAC,GAAG,oBAAoB,CAAC;YAC5C,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACzB;QACD,MAAM,CAAC,QAAQ,CAAC,GAAG,YAAY,CAAC;QAChC,uCAAuC;QACvC,IAAM,WAAW,GAAG,EAAE,CAAC;QACvB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YAChD,IAAM,KAAK,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,IAAM,SAAS,GAAG,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,CAAC;YAEjD,IAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;YACpD,IAAI,CAAC,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;gBACrC,SAAS;aACV;YACD,IAAI,YAAY,GAAG,iBAAiB,CAAC,OAAO,CAAC,CAAC;YAC9C,IAAI,YAAY,KAAK,IAAI,IAAI,YAAY,KAAK,SAAS,EAAE;gBACvD,YAAY,GAAG,CAAC,CAAC;aAClB;YACD,IAAM,WAAW,GAAG,IAAI,CAAC,wBAAwB,CAAC,CAAC,CAAC,CAAC;YACrD,WAAW,CAAC,IAAI,CAAC,CAAC,KAAK,CAAC,IAAI,EAAE,YAAY,EAAE,WAAW,CAAC,CAAC,CAAC;SAC3D;QACD,MAAM,CAAC,aAAa,CAAC,GAAG,WAAW,CAAC;QAEpC,IAAM,YAAY,GAAG,EAAE,CAAC;QACxB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YACjD,IAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;YACnC,IAAM,SAAS,GAAG,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,CAAC;YAElD,IAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;YACpD,IAAI,CAAC,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;gBACrC,SAAS;aACV;YACD,IAAI,YAAY,GAAG,iBAAiB,CAAC,OAAO,CAAC,CAAC;YAC9C,IAAI,YAAY,KAAK,IAAI,IAAI,YAAY,KAAK,SAAS,EAAE;gBACvD,YAAY,GAAG,CAAC,CAAC;aAClB;YACD,IAAM,WAAW,GAAG,IAAI,CAAC,yBAAyB,CAAC,CAAC,CAAC,CAAC;YACtD,YAAY,CAAC,IAAI,CAAC,CAAC,KAAK,CAAC,IAAI,EAAE,YAAY,EAAE,WAAW,CAAC,CAAC,CAAC;SAC5D;QACD,MAAM,CAAC,cAAc,CAAC,GAAG,YAAY,CAAC;QACtC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;;;;;;;;;;OAWG;IACH,kBAAkB;IACX,oBAAU,GAAjB,UACI,GAA6C,EAC7C,MAAgC,EAChC,aAA8C,EAC9C,cAAsB;QADtB,8BAAA,EAAA,gBAAgB,EAA8B;QAC9C,+BAAA,EAAA,sBAAsB;QACxB,iCAAiC;QACjC,mCAAmC;QACnC,IAAM,aAAa,GAAiC,EAAE,CAAC;QAEvD,wCAAwC;QACxC,yCAAyC;QACzC,oDAAoD;QACpD,qDAAqD;QACrD,wDAAwD;QACxD,IAAM,gBAAgB,GAAkD,EAAE,CAAC;QAC3E,SAAS,kBAAkB,CACvB,KAAY,EAAE,QAAkC;YAClD,IAAI,CAAC,CAAC,KAAK,CAAC,IAAI,IAAI,gBAAgB,CAAC,EAAE;gBACrC,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC;aAC3C;iBAAM;gBACL,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;aAC7C;QACH,CAAC;QAED,SAAS,WAAW,CAAC,KAAY,EAAE,QAAkC;YACnE,IAAM,YAAY,GAAqB,EAAE,CAAC;YAC1C,IAAI,MAAM,CAAC;YACX,KAAwB,UAAQ,EAAR,qBAAQ,EAAR,sBAAQ,EAAR,IAAQ,EAAE;gBAA7B,IAAM,SAAS,iBAAA;gBAClB,IAAM,gBAAgB,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gBACtC,IAAM,gBAAgB,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gBACtC,IAAM,kBAAkB,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gBAExC,MAAM,GAAG,SAAS,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC;oBAC3B,EAAE,CAAC,CAAC;oBACJ,SAAS,CAAC,CAAC,CAA6B,CAAC;gBAC7C,IAAI,CAAC,CAAC,gBAAgB,IAAI,aAAa,CAAC,EAAE;oBACxC,kBAAkB,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;oBACpC,OAAO;iBACR;gBACD,IAAM,YAAY,GAAG,aAAa,CAAC,gBAAgB,CAAC,CAAC;gBACrD,IAAI,YAAY,CAAC,YAAY,CAAC,MAAM,IAAI,gBAAgB,EAAE;oBACxD,kBAAkB,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;oBACpC,OAAO;iBACR;gBACD,IAAM,WAAW,GAAG,YAAY,CAAC,YAAY,CAAC,gBAAgB,CAAC,CAAC;gBAChE,YAAY,CAAC,IAAI,CAAC,WAAW,CAAC,aAAa,CAAC,kBAAkB,CAAC,CAAC,CAAC;aAClE;YACD,mDAAmD;YACnD,oCAAoC;YACpC,8CAA8C;YAC9C,IAAI,YAAY,CAAC,MAAM,GAAG,CAAC,EAAE;gBAC3B,KAAK,CAAC,KAAK,CACP,aAAa,CAAC,gBAAgB,CAAC,YAAY,CAAC,EAC5C,MAAM,CAAC,CAAC,CAAE,gBAAgB;aAC/B;QACH,CAAC;QAED;;;;;WAKG;QACH,SAAS,YAAY,CAAC,SAAwC;YAC5D,IAAM,SAAS,GAAG,SAAS,CAAC,MAAM,CAAW,CAAC;YAC9C,qBAAqB;YACrB,IAAM,KAAK,GACP,2BAAgB,CACZ,SAAS,EACT,MAAM,CAAC,eAAe,CAAC,IAAI,IAAI,CAAC,CAAC;gBAC7B,MAAM,CAAC,eAAe,CAA6B,CAAC,CAAC;gBACrD,EAAE,CAAU,CAAC;YACzB,KAAK,CAAC,4BAA4B,CAAC,cAAc,CAAC,CAAC;YACnD,aAAa,CAAC,SAAS,CAAC,GAAG,KAAK,CAAC;YACjC,uBAAuB;YACvB,IAAM,gBAAgB,GAClB,SAAS,CAAC,cAAc,CAA+B,CAAC;YAC5D,gBAAgB,CAAC,OAAO,CAAC,UAAA,QAAQ;gBAC/B,IAAI,CAAC,CAAC,QAAQ,YAAY,KAAK,CAAC,EAAE;oBAChC,MAAM,IAAI,mBAAU,CAChB,2DACI,QAAU,CAAC,CAAC;iBACrB;gBACD,iDAAiD;gBACjD,yDAAyD;gBACzD,0DAA0D;gBAC1D,sCAAsC;gBACtC,kBAAkB,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;YACtC,CAAC,CAAC,CAAC;QACL,CAAC;QAED,iEAAiE;QACjE,IAAM,IAAI,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;QAC5B,IAAM,gBAAgB,GAAG,MAAM,CAAC,QAAQ,CAA+B,CAAC;QACxE,KAAwB,UAAgB,EAAhB,qCAAgB,EAAhB,8BAAgB,EAAhB,IAAgB,EAAE;YAArC,IAAM,SAAS,yBAAA;YAClB,YAAY,CAAC,SAAS,CAAC,CAAC;SACzB;QAED,iDAAiD;QACjD,yDAAyD;QACzD,yDAAyD;QACzD,6CAA6C;QAC7C,OAAO,CAAC,aAAa,CAAC,aAAa,CAAC,gBAAgB,CAAC,EAAE;YACrD,KAAwB,UAAgB,EAAhB,qCAAgB,EAAhB,8BAAgB,EAAhB,IAAgB,EAAE;gBAArC,IAAM,SAAS,yBAAA;gBAClB,IAAM,KAAK,GAAG,aAAa,CAAC,SAAS,CAAC,MAAM,CAAW,CAAC,CAAC;gBACzD,IAAI,KAAK,CAAC,IAAI,IAAI,gBAAgB,EAAE;oBAClC,IAAM,+BAA+B,GAAG,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;oBACrE,OAAO,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;oBACpC,KAAuB,UAA+B,EAA/B,mEAA+B,EAA/B,6CAA+B,EAA/B,IAA+B,EAAE;wBAAnD,IAAM,QAAQ,wCAAA;wBACjB,WAAW,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;qBAC9B;iBACF;aACF;SACF;QAED,IAAM,YAAY,GAAqB,EAAE,CAAC;QAC1C,IAAM,aAAa,GAAqB,EAAE,CAAC;QAC3C,IAAM,qBAAqB,GACvB,MAAM,CAAC,aAAa,CAA+B,CAAC;QACxD,KAAwB,UAAqB,EAArB,+CAAqB,EAArB,mCAAqB,EAArB,IAAqB,EAAE;YAA1C,IAAM,SAAS,8BAAA;YAClB,IAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,IAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,IAAM,WAAW,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YAC3C,aAAa,CAAC,MAAM,CAAC,SAAS,IAAI,aAAa,CAAC,CAAC;YACjD,IAAM,KAAK,GAAG,aAAa,CAAC,SAAS,CAAC,CAAC;YACvC,IAAM,kBAAkB,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC,aAAa,CAAC;YACvE,YAAY,CAAC,IAAI,CAAC,kBAAkB,CAAC,WAAW,CAAC,CAAC,CAAC;SACpD;QACD,IAAM,sBAAsB,GACxB,MAAM,CAAC,cAAc,CAA+B,CAAC;QACzD,KAAwB,UAAsB,EAAtB,iDAAsB,EAAtB,oCAAsB,EAAtB,IAAsB,EAAE;YAA3C,IAAM,SAAS,+BAAA;YAClB,IAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,IAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,IAAM,WAAW,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YAC3C,aAAa,CAAC,MAAM,CAAC,SAAS,IAAI,aAAa,CAAC,CAAC;YACjD,IAAM,KAAK,GAAG,aAAa,CAAC,SAAS,CAAC,CAAC;YACvC,IAAM,kBAAkB,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC,aAAa,CAAC;YACvE,aAAa,CAAC,IAAI,CAAC,kBAAkB,CAAC,WAAW,CAAC,CAAC,CAAC;SACrD;QACD,OAAO,IAAI,GAAG,CAAC,EAAC,MAAM,EAAE,YAAY,EAAE,OAAO,EAAE,aAAa,EAAE,IAAI,MAAA,EAAC,CAAC,CAAC;IACvE,CAAC;IAQD,sBAAI,+BAAQ;QANZ;;;;;WAKG;aACH;YACE,oEAAoE;YACpE,kDAAkD;YAClD,IAAI,IAAI,CAAC,SAAS,EAAE;gBAClB,MAAM,IAAI,mBAAU,CAChB,4DAA4D;oBAC5D,6DAA6D;oBAC7D,iEAAiE,CAAC,CAAC;aACxE;YACD,KAAoB,UAAW,EAAX,KAAA,IAAI,CAAC,MAAM,EAAX,cAAW,EAAX,IAAW,EAAE;gBAA5B,IAAM,KAAK,SAAA;gBACd,IAAI,KAAK,CAAC,QAAQ,EAAE;oBAClB,OAAO,IAAI,CAAC;iBACb;aACF;YACD,OAAO,KAAK,CAAC;QACf,CAAC;;;OAAA;IAED;;;;;OAKG;IACH,+BAAW,GAAX;QAAA,iBAUC;QATC,gBAAI,CAAC;YACH,KAAI,CAAC,MAAM,CAAC,OAAO,CAAC,UAAA,KAAK;gBACvB,wBAAwB;gBACxB,IAAI,KAAK,CAAC,QAAQ,EAAE;oBAClB,KAAK,CAAC,WAAW,EAAE,CAAC;iBACrB;gBACD,uBAAuB;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;IACL,CAAC;IACH,gBAAC;AAAD,CAAC,AAlvCD,CAAwC,gBAAK,GAkvC5C;AAlvCqB,8BAAS","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/engine/topology.py */\n\nimport {NamedTensorMap, Scalar, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\n\nimport {getUid} from '../backend/state';\nimport {NotImplementedError, RuntimeError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {TensorKeyWithArgsArray} from '../keras_format/node_config';\nimport {PyJsonDict} from '../keras_format/types';\nimport {deserialize as deserializeLayer} from '../layers/serialization';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {convertTsToPythonic} from '../utils/serialization_utils';\nimport * as types_utils from '../utils/types_utils';\nimport {batchSetValue, LayerVariable} from '../variables';\nimport {version as layersVersion} from '../version';\n\nimport {execute, FeedDict} from './executor';\nimport {InputLayer} from './input_layer';\nimport {DisposeResult, Layer, Node, SymbolicTensor} from './topology';\n\n/** Constructor config for Container. */\nexport interface ContainerArgs {\n  inputs: SymbolicTensor|SymbolicTensor[];\n  outputs: SymbolicTensor|SymbolicTensor[];\n  name?: string;\n}\n\n/**\n * A Container is a directed acyclic graph of layers.\n *\n * It is the topological form of a \"model\". A LayersModel\n * is simply a Container with added training routines.\n *\n */\nexport abstract class Container extends Layer {\n  inputs: SymbolicTensor[];\n  outputs: SymbolicTensor[];\n\n  inputLayers: Layer[];\n  inputLayersNodeIndices: number[];\n  inputLayersTensorIndices: number[];\n\n  outputLayers: Layer[];\n  outputLayersNodeIndices: number[];\n  outputLayersTensorIndices: number[];\n\n  layers: Layer[];\n  layersByDepth: {[depth: string]: Layer[]};\n  nodesByDepth: {[depth: string]: Node[]};\n\n  internalContainerRefs: Container[];\n\n  containerNodes = new Set<string>();\n\n  // TODO(michaelterry): Add cache support\n  // private outputMaskCache: any;\n  // private outputTensorCache: any;\n  // private outputShapeCache: any;\n\n  inputNames: string[];\n  outputNames: string[];\n  feedInputShapes: Shape[];\n\n  protected internalInputShapes: Shape[];\n  protected internalOutputShapes: Shape[];\n  // TODO(cais): Maybe 'feed' should not in the names of these variables,\n  //   due to the fact that our backend is not symbolic.\n  protected feedInputNames: string[];\n  protected feedOutputNames: string[];\n\n  constructor(args: ContainerArgs) {\n    // No args passed to super's constructor.\n    super({});\n    this.name = args.name;\n    if (this.name == null) {\n      const prefix = this.getClassName().toLowerCase();\n      this.name = getUid(prefix);\n    }\n\n    this.supportsMasking = false;\n    this.trainable_ = true;\n\n    // TODO(michaelterry): Initialize perInputLosses/Updates here.\n\n    // Container-specific properties.\n    if (Array.isArray(args.inputs)) {\n      this.inputs = args.inputs.slice();\n    } else {\n      this.inputs = [args.inputs];\n    }\n    if (Array.isArray(args.outputs)) {\n      this.outputs = args.outputs.slice();\n    } else {\n      this.outputs = [args.outputs];\n    }\n\n    // Check for redundancy in inputs.\n    if (generic_utils.unique(this.inputs).length !== this.inputs.length) {\n      throw new ValueError(\n          'The list of inputs passed to the model is ' +\n          'redundant. All inputs should only appear once. Found: ' +\n          `${this.inputs.map(x => x.name)}`);\n    }\n\n    // Check for redundancy in outputs.\n    if (generic_utils.unique(this.outputs).length !== this.outputs.length) {\n      console.warn(\n          'The list of outputs passed to the model is redundant. ' +\n          'All outputs should only appear once. Found: ' +\n          `${this.outputs.map(x => x.name)}`);\n    }\n\n    /*\n      List of initial layers (1 to 1 mapping with this.inputs, hence the same\n      layer might appear twice)\n    */\n    this.inputLayers = [];\n    this.inputLayersNodeIndices = [];\n    this.inputLayersTensorIndices = [];\n    /*\n      List of layers (1 to 1 mapping with this.outputs, hence the same layer\n      might appear twice)\n    */\n    this.outputLayers = [];\n    this.outputLayersNodeIndices = [];\n    this.outputLayersTensorIndices = [];\n    /*\n      All layers in order of horizontal graph traversal. Entries are unique.\n      Includes input and output layers.\n    */\n    this.layers = [];\n\n    /*\n      References to container layers that were constructed internally. We need\n      these to properly dispose of tensors from nested containers.\n    */\n    this.internalContainerRefs = [];\n\n    // TODO(michaelterry): Determine if caching still needed with eager\n    // backend.\n    /*\n      This is for performance optimization when calling the Container on new\n      inputs. Every time the Container is called on a set on input tensors,\n      we compute the output tensors, output masks and output shapes in one pass,\n      then cache them here. When one of these outputs is queried later,\n      we retrieve it from there instead of recomputing it.\n    */\n    // this.outputTensorCache = {};\n    // this.outputShapeCache = {};\n\n    // Build this.outputLayers:\n    for (const x of this.outputs) {\n      const layer = x.sourceLayer;\n      const nodeIndex = x.nodeIndex;\n      const tensorIndex = x.tensorIndex;\n      this.outputLayers.push(layer);\n      this.outputLayersNodeIndices.push(nodeIndex);\n      this.outputLayersTensorIndices.push(tensorIndex);\n    }\n\n    // TODO(michaelterry): Add output mask cache code.\n\n    // Build this.inputLayers:\n    for (const x of this.inputs) {\n      const layer = x.sourceLayer;\n      const nodeIndex = x.nodeIndex;\n      const tensorIndex = x.tensorIndex;\n      /*\n        It's supposed to be an input layer, so only one node\n        and one tensor output.\n      */\n      generic_utils.assert(nodeIndex === 0, 'input layer has >1 nodes');\n      generic_utils.assert(tensorIndex === 0, 'input layer has >1 tensors');\n      this.inputLayers.push(layer);\n      this.inputLayersNodeIndices.push(nodeIndex);\n      this.inputLayersTensorIndices.push(tensorIndex);\n    }\n\n    // Build this.inputNames and this.outputNames.\n    this.inputNames = [];\n    this.outputNames = [];\n    this.feedInputShapes = [];\n    this.feedInputNames = [];\n    this.feedOutputNames = [];\n    for (let i = 0; i < this.inputLayers.length; i++) {\n      const layer = this.inputLayers[i];\n      // Check that layer is an InputLayer.\n      if (!(layer instanceof InputLayer)) {\n        throw new TypeError(\n            'Input layers to a LayersModel must be InputLayer objects. ' +\n            `Received inputs: ${args.inputs}. ` +\n            `Input ${i} (0-based) originates ` +\n            `from layer type ${layer.getClassName()}.`);\n      }\n      this.inputNames.push(layer.name);\n      this.feedInputShapes.push(layer.batchInputShape);\n\n      this.feedInputNames.push(layer.name);\n    }\n    for (const layer of this.outputLayers) {\n      this.outputNames.push(layer.name);\n    }\n\n    this.internalInputShapes = this.inputs.map(x => x.shape);\n    this.internalOutputShapes = this.outputs.map(x => x.shape);\n\n    /*\n      Container_nodes: set of nodes included in the graph (not all nodes\n      included in the layers are relevant to the current graph).\n    */\n    // ids of all nodes relevant to the Container:\n    const nodesDepths: {[nodeID: string]: number} = {};\n    // To recover nodes from their ID.\n    const nodeIDToNode: {[nodeID: string]: Node} = {};\n    const layersDepths: {[layerID: string]: number} = {};\n    // To layers from their ID.\n    const layerIDToLayer: {[layerID: string]: Layer} = {};\n    const layerIndices: {[layerID: string]: number} = {};\n    const nodesInDecreasingDepth: Node[] = [];\n\n    /**\n     * Builds a map of the graph of layers.\n     *\n     * This recursively updates the map `layerIndices`,\n     * the list `nodesInDecreasingDepth` and the set `containerNodes`.\n     *\n     * @param tensor Some tensor in a graph.\n     * @param finishedNodes Set of nodes whose subgraphs have been traversed\n     *         completely. Useful to prevent duplicated work.\n     * @param nodesInProgress Set of nodes that are currently active on the\n     *         recursion stack. Useful to detect cycles.\n     * @param layer Layer from which `tensor` comes from. If not provided,\n     *   will be obtained from tensor.sourceLayer.\n     * @param nodeIndex Node index from which `tensor` comes from.\n     * @param tensorIndex TensorIndex from which `tensor` comes from.\n     *\n     * @exception RuntimeError if a cycle is detected.\n     */\n    const buildMapOfGraph =\n        (tensor: SymbolicTensor, finishedNodes: Node[], nodesInProgress: Node[],\n         layer?: Layer, nodeIndex?: number, tensorIndex?: number) => {\n          if (layer == null || nodeIndex == null || tensorIndex == null) {\n            layer = tensor.sourceLayer;\n            nodeIndex = tensor.nodeIndex;\n            tensorIndex = tensor.tensorIndex;\n          }\n          const node = layer.inboundNodes[nodeIndex];\n\n          // Prevent cycles.\n          if (nodesInProgress.indexOf(node) !== -1) {\n            throw new RuntimeError(\n                `The tensor ${tensor.name} at layer \"${layer.name}\" ` +\n                'is part of a cycle.');\n          }\n\n          // Don't repeat work for shared subgraphs\n          if (finishedNodes.indexOf(node) !== -1) {\n            return;\n          }\n\n          // Update containerNodes.\n          this.containerNodes.add(Container.nodeKey(layer, nodeIndex));\n\n          // Store the traversal order for layer sorting.\n          if (!(layer.id in layerIndices)) {\n            layerIndices[layer.id] = Object.keys(layerIndices).length;\n          }\n\n          if (nodesInProgress.indexOf(node) === -1) {\n            nodesInProgress.push(node);\n          }\n\n          // Propagate to all previous tensors connected to this node.\n          const numInboundLayers = node.inboundLayers.length;\n          for (let i = 0; i < numInboundLayers; i++) {\n            const x = node.inputTensors[i];\n            const layer = node.inboundLayers[i];\n            const nodeIndex = node.nodeIndices[i];\n            const tensorIndex = node.tensorIndices[i];\n            buildMapOfGraph(\n                x, finishedNodes, nodesInProgress, layer, nodeIndex,\n                tensorIndex);\n          }\n          finishedNodes.push(node);\n          while (nodesInProgress.indexOf(node) >= 0) {\n            nodesInProgress.splice(nodesInProgress.indexOf(node), 1);\n          }\n          nodesInDecreasingDepth.push(node);\n        };\n\n    const finishedNodes: Node[] = [];\n    const nodesInProgress: Node[] = [];\n    for (const x of this.outputs) {\n      buildMapOfGraph(x, finishedNodes, nodesInProgress);\n    }\n\n    const reversedNodesInDecreasingDepth =\n        nodesInDecreasingDepth.slice().reverse();\n    for (const node of reversedNodesInDecreasingDepth) {\n      nodeIDToNode[node.id] = node;\n      // If the depth is not set, the node has no outbound nodes (depth 0).\n      if (!(node.id in nodesDepths)) {\n        nodesDepths[node.id] = 0;\n      }\n      let depth = nodesDepths[node.id];\n\n      // Update the depth of the corresponding layer\n      const previousDepth =\n          (layersDepths[node.outboundLayer.id] == null ?\n               0 :\n               layersDepths[node.outboundLayer.id]);\n\n      /*\n        If we've seen this layer before at a higher depth, we should use that\n        depth instead of the node depth.  This is necessary for shared layers\n        that have inputs at different depth levels in the graph.\n      */\n      depth = Math.max(depth, previousDepth);\n      layersDepths[node.outboundLayer.id] = depth;\n      layerIDToLayer[node.outboundLayer.id] = node.outboundLayer;\n      nodesDepths[node.id] = depth;\n\n      // Update the depth of inbound nodes.\n      for (let i = 0; i < node.inboundLayers.length; i++) {\n        const inboundLayer = node.inboundLayers[i];\n        const nodeIndex = node.nodeIndices[i];\n        const inboundNode = inboundLayer.inboundNodes[nodeIndex];\n        const previousDepth =\n            (nodesDepths[inboundNode.id] == null ? 0 :\n                                                   nodesDepths[inboundNode.id]);\n        nodesDepths[inboundNode.id] = Math.max(depth + 1, previousDepth);\n        nodeIDToNode[inboundNode.id] = inboundNode;\n      }\n    }\n\n    // Build a dict {depth: list of nodes with this depth}\n    const nodesByDepth: {[depth: string]: Node[]} = {};\n    for (const nodeID in nodesDepths) {\n      const depth = nodesDepths[nodeID];\n      if (!(depth in nodesByDepth)) {\n        nodesByDepth[depth] = [];\n      }\n      nodesByDepth[depth].push(nodeIDToNode[nodeID]);\n    }\n\n    // Build a dict {depth: list of layers with this depth}\n    const layersByDepth: {[depth: string]: Layer[]} = {};\n    for (const layerID in layersDepths) {\n      const depth = layersDepths[layerID];\n      if (!(depth in layersByDepth)) {\n        layersByDepth[depth] = [];\n      }\n      layersByDepth[depth].push(layerIDToLayer[layerID]);\n    }\n\n    // Get sorted list of layer depths.\n    let depthKeys = Object.keys(layersByDepth)\n                        .map(x => parseInt(x, 10))\n                        .sort(generic_utils.reverseNumberCompare);\n\n    // Set this.layers and this.layersByDepth.\n    this.layers = [];\n    for (const depth of depthKeys) {\n      const layersForDepth = layersByDepth[depth];\n      // Container.layers needs to have a deterministic order:\n      // here we order them by traversal order.\n      layersForDepth.sort((a, b) => {\n        const aIndex = layerIndices[a.id];\n        const bIndex = layerIndices[b.id];\n        if (aIndex < bIndex) {\n          return -1;\n        }\n        if (aIndex > bIndex) {\n          return 1;\n        }\n        return 0;\n      });\n      for (const layer of layersForDepth) {\n        if (layer instanceof Container) {\n          this.internalContainerRefs.push(layer);\n        }\n        this.layers.push(layer);\n      }\n    }\n    this.layersByDepth = layersByDepth;\n\n    // Get sorted list of node depths;\n    depthKeys = Object.keys(nodesByDepth)\n                    .map(x => parseInt(x, 10))\n                    .sort(generic_utils.reverseNumberCompare);\n\n    // Check that all tensors required are computable.\n    // computable_tensors: all tensors in the graph\n    // that can be computed from the inputs provided.\n    const computableTensors = this.inputs.slice();\n\n    // To provide a better error msg.\n    const layersWithCompleteInput: string[] = [];\n    for (const depth of depthKeys) {\n      for (const node of nodesByDepth[depth]) {\n        const layer = node.outboundLayer;\n        if (layer != null) {\n          for (const x of node.inputTensors) {\n            if (computableTensors.indexOf(x) === -1) {\n              throw new RuntimeError(\n                  `Graph disconnected: cannot obtain value for tensor ${x}` +\n                  ` at layer \"${layer.name}\". ` +\n                  'The following previous layers were accessed without ' +\n                  `issue: ${layersWithCompleteInput}`);\n            }\n          }\n          for (const x of node.outputTensors) {\n            computableTensors.push(x);\n          }\n          layersWithCompleteInput.push(layer.name);\n        }\n      }\n    }\n\n    // Set this.containerNodes and this.nodesByDepth.\n    this.nodesByDepth = nodesByDepth;\n\n    // Ensure name unicity, which will be crucial for serialization\n    // (since serialized nodes refer to layers by their name).\n    const allNames = this.layers.map(x => x.name);\n    for (const name of allNames) {\n      const numOccurrences = allNames.filter(x => x === name).length;\n      if (numOccurrences !== 1) {\n        throw new RuntimeError(\n            `The name \"${name}\" is used ${numOccurrences} times ` +\n            'in the model. All layer names should be unique. Layer names: ' +\n            JSON.stringify(allNames));\n      }\n    }\n\n    // Layer parameters.\n    // The new container starts with a single inbound node\n    // for its inputs, and no outbound nodes.\n    // Will be appended to by future calls to apply().\n    this.outboundNodes = [];\n    // Will be appended to below, and by future calls to apply().\n    this.inboundNodes = [];\n\n    // Create the node linking internal inputs to internal outputs.\n    // (This call has side effects.)\n    // tslint:disable-next-line:no-unused-expression\n    new Node({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: this.inputs,\n      outputTensors: this.outputs,\n      inputMasks: this.inputs.map(x => null),\n      outputMasks: this.outputs.map(x => null),\n      inputShapes: this.inputs.map(x => x.shape),\n      outputShapes: this.outputs.map(x => x.shape)\n    });\n    this.built = true;\n    this._refCount = 1;  // The ref count of a container always start at 1.\n  }\n\n  protected assertNotDisposed() {\n    if (this._refCount === 0) {\n      throw new Error(`Container '${this.name}' is already disposed.`);\n    }\n  }\n\n  /**\n   * Attempt to dispose a LayersModel's weights.\n   *\n   * This method decrease the reference count of the LayersModel object by 1.\n   *\n   * A LayersModel is reference-counted. Its reference count is incremented by 1\n   * when it is first constructed and when it is used as a Layer of another\n   * LayersModel.\n   *\n   * If the reference count of a LayersModel becomes 0, the `dispose` method of\n   * all its constituent `Layer`s will be called.\n   *\n   * Note: If the reference count is greater than 0 after the decrement, the\n   * `dispose` method of its constituent `Layer`s will *not* be called.\n   *\n   * After a LayersModel is disposed, it cannot be used in calls such as\n   * 'predict`, `evaluate` or `fit` anymore.\n   *\n   * @returns A DisposeResult Object with the following fields:\n   *   - refCountAfterDispose: The reference count of the LayersModel after this\n   *     `dispose()` call.\n   *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n   *     during this `dispose()` call.\n   * @throws {Error} If the layer is not built yet, or if the LayersModel has\n   *   already been disposed.\n   */\n  dispose(): DisposeResult {\n    this.assertNotDisposed();\n    const result:\n        DisposeResult = {refCountAfterDispose: null, numDisposedVariables: 0};\n    if (--this._refCount === 0) {\n      for (const layer of this.layers) {\n        result.numDisposedVariables += layer.dispose().numDisposedVariables;\n      }\n\n      // Call dispose on each internally created container layer again to ensure\n      // their refCounts hit zero and their tensors are subsequently deleted.\n      for (const container of this.internalContainerRefs) {\n        result.numDisposedVariables += container.dispose().numDisposedVariables;\n      }\n    }\n    result.refCountAfterDispose = this._refCount;\n    return result;\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(trainable: boolean) {\n    this.layers.forEach(layer => {\n      // tslint:disable-next-line:no-any\n      ((layer as any)._trainableWeights as LayerVariable[])\n          .forEach(w => w.trainable = trainable);\n    });\n    this.trainable_ = trainable;\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    // Porting Note: This check below is to prevent errors where the\n    //   _trainableWeights inherited from the parent class (Layer) gets\n    //   inadvertently used.\n    if (this._trainableWeights.length > 0) {\n      throw new ValueError(\n          'Container instance unexpectedly contains _trainableWeights.' +\n          'The trainable weights of a Container are a union of the ' +\n          'trainable weights of its consituent Layers. Its own ' +\n          '_trainableWeights must remain an empty Array.');\n    }\n\n    if (!this.trainable) {\n      return [];\n    }\n    let weights: LayerVariable[] = [];\n    for (const layer of this.layers) {\n      weights = weights.concat(layer.trainableWeights);\n    }\n    return weights;\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    const weights: LayerVariable[] = [];\n    for (const layer of this.layers) {\n      weights.push(...layer.nonTrainableWeights);\n    }\n    if (!this.trainable) {\n      const trainableWeights: LayerVariable[] = [];\n      for (const layer of this.layers) {\n        trainableWeights.push(...layer.trainableWeights);\n      }\n      return trainableWeights.concat(weights);\n    }\n    return weights;\n  }\n\n  get weights(): LayerVariable[] {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  /**\n   * Loads all layer weights from a JSON object.\n   *\n   * Porting Note: HDF5 weight files cannot be directly loaded in JavaScript /\n   *   TypeScript. The utility script at `scripts/pykeras.py` offers means\n   *   to convert them into JSON strings compatible with this method.\n   * Porting Note: TensorFlow.js Layers supports only loading by name currently.\n   *\n   * @param weights A JSON mapping weight names to weight values as nested\n   *   arrays of numbers, or a `NamedTensorMap`, i.e., a JSON mapping weight\n   *   names to `tf.Tensor` objects.\n   * @param strict Require that the provided weights exactly match those\n   *   required by the container.  Default: `true`.  Passing `false` means that\n   *   extra weights and missing weights will be silently ignored.\n   */\n  loadWeights(weights: NamedTensorMap, strict = true) {\n    const nameToWeight: {[name: string]: LayerVariable} = {};\n    let totalWeightsCount = 0;\n    for (const layer of this.layers) {\n      for (const weight of layer.weights) {\n        if (nameToWeight[weight.originalName] != null) {\n          throw new ValueError(`Duplicate weight name: ${weight.originalName}`);\n        }\n        nameToWeight[weight.originalName] = weight;\n        totalWeightsCount++;\n      }\n    }\n\n    const weightValueTuples: Array<[LayerVariable, Tensor]> = [];\n    for (const name in weights) {\n      if (nameToWeight[name] != null) {\n        weightValueTuples.push([nameToWeight[name], weights[name]]);\n      } else if (strict) {\n        throw new ValueError(\n            `Provided weight data has no target variable: ${name}`);\n      }\n      delete nameToWeight[name];\n    }\n\n    if (strict) {\n      // Check that all weights are set.\n      const unsetNames: string[] = [];\n      for (const name in nameToWeight) {\n        unsetNames.push(name);\n      }\n      if (unsetNames.length > 0) {\n        throw new ValueError(\n            `${unsetNames.length} of ${\n                totalWeightsCount} weights are not set: ` +\n            `${unsetNames}`);\n      }\n    }\n\n    batchSetValue(weightValueTuples);\n  }\n\n  /**\n   * Util shared between different serialization methods.\n   * @returns LayersModel config with Keras version information added.\n   */\n  protected updatedConfig(): serialization.ConfigDict {\n    const theConfig = this.getConfig();\n    const modelConfig: serialization.ConfigDict = {};\n    modelConfig['className'] = this.getClassName();\n    modelConfig['config'] = theConfig;\n    modelConfig['kerasVersion'] = `tfjs-layers ${layersVersion}`;\n    // TODO(nielsene): Replace something like K.backend() once\n    // possible.\n    modelConfig['backend'] = 'TensorFlow.js';\n    return modelConfig;\n  }\n\n  /**\n   * Returns a JSON string containing the network configuration.\n   *\n   * To load a network from a JSON save file, use\n   * models.modelFromJSON(jsonString);\n   * @param extraJsonArgs Unused in tfjs-layers, maintained for PyKeras\n   * @param returnString Whether the return value should be stringified\n   *    (default: `true`).\n   * @returns a JSON string if `returnString` (default), or a JSON object if\n   *   `!returnString`.\n   */\n  // tslint:disable-next-line:no-any\n  toJSON(unused?: any, returnString = true): string|PyJsonDict {\n    const modelConfig = convertTsToPythonic(this.updatedConfig()) as PyJsonDict;\n    return returnString ? JSON.stringify(modelConfig) : modelConfig;\n  }\n\n  /**\n   * Call the model on new inputs.\n   *\n   * In this case `call` just reapplies all ops in the graph to the new inputs\n   * (e.g. build a new computational graph from the provided inputs).\n   *\n   * @param inputs A tensor or list of tensors.\n   * @param mask A mask or list of masks. A mask can be either a tensor or null\n   *   (no mask).\n   *\n   * @return A tensor if there is a single output, or a list of tensors if there\n   *   are more than one outputs.\n   */\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = generic_utils.toList(inputs);\n      const feedDict = new FeedDict();\n      for (let i = 0; i < this.inputs.length; ++i) {\n        feedDict.add(this.inputs[i], inputs[i]);\n      }\n      return execute(this.outputs, feedDict, kwargs) as Tensor | Tensor[];\n    });\n  }\n\n  /**\n   * Computes an output mask tensor.\n   *\n   * @param inputs Tensor or list of tensors.\n   * @param mask Tensor or list of tensors.\n   *\n   * @return null or a tensor (or list of tensors, one per output tensor of the\n   * layer).\n   */\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    return tidy(() => {\n      inputs = generic_utils.toList(inputs);\n      let masks: Tensor[];\n      if (mask == null) {\n        masks = generic_utils.pyListRepeat(null, inputs.length);\n      } else {\n        masks = generic_utils.toList(mask);\n      }\n      // TODO(michaelterry): Add support for mask caching.\n      return this.runInternalGraph(inputs, masks)[1];\n    });\n  }\n\n  /**\n   * Computes the output shape of the layer.\n   *\n   * Assumes that the layer will be built to match that input shape provided.\n   *\n   * @param inputShape A shape (tuple of integers) or a list of shape tuples\n   *   (one per output tensor of the layer). Shape tuples can include null for\n   *   free dimensions, instead of an integer.\n   */\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    const inputShapes = types_utils.normalizeShapeList(inputShape);\n    if (inputShapes.length !== this.inputLayers.length) {\n      throw new ValueError(\n          `Invalid inputShape argument ${inputShape}: ` +\n          `model has ${this.inputLayers.length} tensor inputs.`);\n    }\n\n    // TODO(michaelterry): Add caching\n    const layersToOutputShapes: {[shapeKey: string]: Shape} = {};\n    for (let i = 0; i < inputShapes.length; i++) {\n      const layer = this.inputLayers[i];\n      const inputShape = inputShapes[i];\n      // It's an input layer: computeOutputShape is identity,\n      // and there is only one node and one tensor output.\n      const shapeKey = layer.name + '_0_0';\n      layersToOutputShapes[shapeKey] = inputShape;\n    }\n\n    const depthKeys = Object.keys(this.nodesByDepth)\n                          .map(x => parseInt(x, 10))\n                          .sort(generic_utils.reverseNumberCompare);\n    // Iterate over nodes, by depth level.\n    if (depthKeys.length > 1) {\n      for (const depth of depthKeys) {\n        const nodes = this.nodesByDepth[depth];\n        for (const node of nodes) {\n          // This is always a single layer, never a list.\n          const layer = node.outboundLayer;\n          if (this.inputLayers.map(x => x.id).indexOf(layer.id) !== -1) {\n            // We've already covered the input layers a few lines above.\n            continue;\n          }\n          // Potentially redundant list, same size of node.inputTensors.\n          const inputShapes: Shape[] = [];\n          for (let j = 0; j < node.inboundLayers.length; j++) {\n            const inboundLayer = node.inboundLayers[j];\n            const nodeIndex = node.nodeIndices[j];\n            const tensorIndex = node.tensorIndices[j];\n            const shapeKey = `${inboundLayer.name}_${nodeIndex}_${tensorIndex}`;\n            const inputShape = layersToOutputShapes[shapeKey];\n            inputShapes.push(inputShape);\n          }\n\n          const outputShape = layer.computeOutputShape(\n              generic_utils.singletonOrArray(inputShapes));\n\n          const outputShapes = types_utils.normalizeShapeList(outputShape);\n          const nodeIndex = layer.inboundNodes.indexOf(node);\n          for (let j = 0; j < outputShapes.length; j++) {\n            const shapeKey = `${layer.name}_${nodeIndex}_${j}`;\n            layersToOutputShapes[shapeKey] = outputShapes[j];\n          }\n        }\n      }\n    }\n\n    // Read final output shapes from layersToOutputShapes.\n    const outputShapes: Shape[] = [];\n    const outputShapeKeys: string[] = [];\n    for (let i = 0; i < this.outputLayers.length; i++) {\n      const layer = this.outputLayers[i];\n      const nodeIndex = this.outputLayersNodeIndices[i];\n      const tensorIndex = this.outputLayersTensorIndices[i];\n      const shapeKey = `${layer.name}_${nodeIndex}_${tensorIndex}`;\n      outputShapeKeys.push(shapeKey);\n    }\n\n    for (let i = 0; i < outputShapeKeys.length; i++) {\n      const key = outputShapeKeys[i];\n      generic_utils.assert(key in layersToOutputShapes);\n      outputShapes.push(layersToOutputShapes[key]);\n    }\n\n    // TODO(michaelterry): Update cache\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  /**\n   * Computes output tensors for new inputs.\n   *\n   * Note:\n   *   - Expects `inputs` to be a list (potentially with 1 element).\n   *\n   * @param inputs List of tensors\n   * @param masks List of masks (tensors or null).\n   * @return Three lists: outputTensors, outputMasks, outputShapes\n   */\n  protected runInternalGraph(inputs: Tensor[], masks?: Tensor[]):\n      [Tensor[], Tensor[], Shape[]] {\n    if (masks == null) {\n      masks = generic_utils.pyListRepeat(null, inputs.length);\n    }\n\n    // Dictionary mapping reference tensors to tuples\n    // (computed tensor, compute mask)\n    // we assume a 1:1 mapping from tensor to mask\n    // TODO: raise exception when a `.computeMask()` call\n    // does not return a list the same size as `call`\n    const tensorMap: {[tensorID: string]: [Tensor, Tensor]} = {};\n    for (let i = 0; i < this.inputs.length; ++i) {\n      const x = this.inputs[i];\n      const y = inputs[i];\n      const mask = masks[i];\n      tensorMap[x.id] = [y, mask];\n    }\n\n    const depthKeys = Object.keys(this.nodesByDepth)\n                          .map(x => parseInt(x, 10))\n                          .sort(generic_utils.reverseNumberCompare);\n    for (const depth of depthKeys) {\n      const nodes = this.nodesByDepth[depth];\n      for (const node of nodes) {\n        // This is always a single layer, never a list.\n        const layer = node.outboundLayer;\n        const referenceInputTensors = node.inputTensors;\n        const referenceOutputTensors = node.outputTensors;\n\n        // If all previous input tensors are available in tensorMap,\n        // then call node.inboundLayer on them.\n        // List of tuples [input, mask]:\n        const computedData = new Array<[Tensor, Tensor]>();\n        for (const x of referenceInputTensors) {\n          if (x.id in tensorMap) {\n            computedData.push(tensorMap[x.id]);\n          }\n        }\n        if (computedData.length === referenceInputTensors.length) {\n          // TODO(michaelterry): Add K.name_scope here, if we need it.\n          let kwargs: Kwargs = {};\n          let computedTensors: Tensor[];\n          let computedMasks: Tensor[];\n          let outputTensors: Tensor[];\n          let outputMasks: Tensor[];\n          // call layer\n          if (node.callArgs != null) {\n            kwargs = node.callArgs;\n          }\n          if (computedData.length === 1) {\n            const [computedTensor, computedMask] = computedData[0];\n            if (kwargs['mask'] == null) {\n              kwargs['mask'] = computedMask;\n            }\n            outputTensors =\n                generic_utils.toList(layer.call(computedTensor, kwargs));\n            outputMasks = generic_utils.toList(\n                layer.computeMask(computedTensor, computedMask));\n            computedTensors = [computedTensor];\n            computedMasks = [computedMask];\n          } else {\n            computedTensors = computedData.map(x => x[0]);\n            computedMasks = computedData.map(x => x[1]);\n            if (kwargs['mask'] == null) {\n              kwargs['mask'] = computedMasks;\n            }\n            outputTensors =\n                generic_utils.toList(layer.call(computedTensors, kwargs));\n            outputMasks = generic_utils.toList(\n                layer.computeMask(computedTensors, computedMasks));\n          }\n\n          if (layer.activityRegularizer) {\n            throw new NotImplementedError(\n                'LayersModel invocation with concrete Tensor value(s) in the ' +\n                'presence of activity regularizer(s) is not supported yet.');\n          }\n          // TODO(michaelterry): Add model updates and losses\n\n          // Update tensor map.\n          for (let i = 0; i < referenceOutputTensors.length; ++i) {\n            const x = referenceOutputTensors[i];\n            const y = outputTensors[i];\n            const mask = outputMasks[i];\n            tensorMap[x.id] = [y, mask];\n          }\n        }\n      }\n    }\n\n    const outputTensors: Tensor[] = [];\n    const outputMasks: Tensor[] = [];\n    const outputShapes: Shape[] = [];\n    for (const x of this.outputs) {\n      generic_utils.assert(\n          x.id in tensorMap, `Could not compute output ${x.name} : ${x.id}`);\n      const [tensor, mask] = tensorMap[x.id];\n      outputShapes.push(tensor.shape);\n      outputTensors.push(tensor);\n      outputMasks.push(mask);\n    }\n\n    // TODO(michaelterry): Add support for caches.\n    return [outputTensors, outputMasks, outputShapes];\n  }\n\n  /**\n   * Builds a map of internal node keys to node ordering.\n   * Used in serializaion a node orderings may change as unused nodes are\n   * dropped. Porting Note:  This helper method was pulled out of getConfig to\n   * improve readability.\n   * @param layers An array of Layers in the model.\n   * @returns Map of Node Keys to index order within the layer.\n   */\n  private buildNodeConversionMap(layers: Layer[]): {[nodeKey: string]: number} {\n    const nodeConversionMap: {[nodeKey: string]: number} = {};\n    let keptNodes: number;\n    for (const layer of this.layers) {\n      keptNodes = layer instanceof Container ? 1 : 0;\n      for (let originalNodeIndex = 0;\n           originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {\n        const nodeKey = Container.nodeKey(layer, originalNodeIndex);\n        if (this.containerNodes.has(nodeKey)) {\n          // i.e. we mark it to be saved\n          nodeConversionMap[nodeKey] = keptNodes;\n          keptNodes += 1;\n        }\n      }\n    }\n    return nodeConversionMap;\n  }\n\n  /**\n   * Retrieves a layer based on either its name (unique) or index.\n   *\n   * Indices are based on order of horizontal graph traversal (bottom-up).\n   *\n   * If both `name` and `index` are specified, `index` takes precedence.\n   *\n   * @param name Name of layer.\n   * @param index Index of layer.\n   * @returns A Layer instance.\n   * @throws ValueError: In case of invalid layer name or index.\n   */\n  /**\n   * @doc {\n   *    heading: 'Layers',\n   *    subheading: 'Classes',\n   *    namespace: 'layers',\n   *    subclasses: ['LayersModel']\n   * }\n   */\n  getLayer(name?: string, index?: number): Layer {\n    if (index != null) {\n      if (this.layers.length <= index) {\n        throw new ValueError(\n            `Was asked to retrieve layer at index ${index}, but model only ` +\n            `has ${this.layers.length} layer(s).`);\n      } else {\n        return this.layers[index];\n      }\n    } else {\n      if (name == null) {\n        throw new ValueError('Provide either a layer name or layer index');\n      }\n    }\n\n    for (const layer of this.layers) {\n      if (layer.name === name) {\n        return layer;\n      }\n    }\n    throw new ValueError(`No such layer: ${name}`);\n  }\n\n  /**\n   * Retrieves the Container's current loss values.\n   *\n   * Used for regularizers during training.\n   */\n  calculateLosses(): Scalar[] {\n    // Porting Node: This is an augmentation to Container.loss in PyKeras.\n    //   In PyKeras, Container.loss returns symbolic tensors. Here a concrete\n    //   Tensor (specifically Scalar) values are returned. This is due to the\n    //   imperative backend.\n    return tidy(() => {\n      const losses: Scalar[] = [];\n      for (const layer of this.layers) {\n        for (let nodeIndex = 0; nodeIndex < layer.inboundNodes.length;\n             ++nodeIndex) {\n          const nodeKey = Container.nodeKey(layer, nodeIndex);\n          if (this.containerNodes.has(nodeKey)) {\n            losses.push(...layer.calculateLosses());\n          }\n        }\n      }\n      // TODO(cais): Add any unconditional model-level losses?\n      return losses;\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {name: this.name};\n\n    // Build a map from layer unique name (self._node_key)\n    // to the index of the nodes that are saved in the config.\n    // Only nodes in container_nodes are saved.\n    const nodeConversionMap: {[nodeKey: string]: number} =\n        this.buildNodeConversionMap(this.layers);\n\n    // Serialize and save the layers in layerConfigs\n    const layerConfigs = [];\n    for (const layer of this.layers) {\n      const layerClassName = layer.getClassName();\n      const layerConfig = layer.getConfig();\n      const filteredInboundNodes = [];\n      for (let originalNodeIndex = 0;\n           originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {\n        const node = layer.inboundNodes[originalNodeIndex];\n        const nodeKey = Container.nodeKey(layer, originalNodeIndex);\n        let kwargs = {};\n        if (this.containerNodes.has(nodeKey)) {\n          // The node is relevant to the model:\n          // add to filteredInboundNodes.\n          if (node.callArgs) {\n            try {\n              JSON.stringify(node.callArgs);\n              kwargs = node.callArgs;\n            } catch (err) {\n              console.warn(\n                  `Layer ${layer.name} was passed ` +\n                  `non-serializable keyword arguments: ` +\n                  `${node.callArgs}. They will not be included ` +\n                  `in the serialized model (and thus will be ` +\n                  `missing at deserialization time).`);\n              kwargs = {};\n            }\n          }\n          if (node.inboundLayers.length > 0) {\n            const nodeData = [];\n            for (let i = 0; i < node.inboundLayers.length; i++) {\n              const inboundLayer = node.inboundLayers[i];\n              const nodeIndex = node.nodeIndices[i];\n              const tensorIndex = node.tensorIndices[i];\n              const nodeKey = Container.nodeKey(inboundLayer, nodeIndex);\n              let newNodeIndex = nodeConversionMap[nodeKey];\n              if (newNodeIndex == null) {\n                newNodeIndex = 0;\n              }\n              nodeData.push(\n                  [inboundLayer.name, newNodeIndex, tensorIndex, kwargs]);\n            }\n            filteredInboundNodes.push(nodeData);\n          }\n        }\n      }\n      const dict: serialization.ConfigDict = {};\n      dict['name'] = layer.name;\n      dict['className'] = layerClassName;\n      dict['config'] = layerConfig;\n      dict['inboundNodes'] = filteredInboundNodes;\n      layerConfigs.push(dict);\n    }\n    config['layers'] = layerConfigs;\n    // Gather info about inputs and outputs\n    const modelInputs = [];\n    for (let i = 0; i < this.inputLayers.length; i++) {\n      const layer = this.inputLayers[i];\n      const nodeIndex = this.inputLayersNodeIndices[i];\n\n      const nodeKey = Container.nodeKey(layer, nodeIndex);\n      if (!this.containerNodes.has(nodeKey)) {\n        continue;\n      }\n      let newNodeIndex = nodeConversionMap[nodeKey];\n      if (newNodeIndex === null || newNodeIndex === undefined) {\n        newNodeIndex = 0;\n      }\n      const tensorIndex = this.inputLayersTensorIndices[i];\n      modelInputs.push([layer.name, newNodeIndex, tensorIndex]);\n    }\n    config['inputLayers'] = modelInputs;\n\n    const modelOutputs = [];\n    for (let i = 0; i < this.outputLayers.length; i++) {\n      const layer = this.outputLayers[i];\n      const nodeIndex = this.outputLayersNodeIndices[i];\n\n      const nodeKey = Container.nodeKey(layer, nodeIndex);\n      if (!this.containerNodes.has(nodeKey)) {\n        continue;\n      }\n      let newNodeIndex = nodeConversionMap[nodeKey];\n      if (newNodeIndex === null || newNodeIndex === undefined) {\n        newNodeIndex = 0;\n      }\n      const tensorIndex = this.outputLayersTensorIndices[i];\n      modelOutputs.push([layer.name, newNodeIndex, tensorIndex]);\n    }\n    config['outputLayers'] = modelOutputs;\n    return config;\n  }\n\n  /**\n   * Instantiates a LayersModel from its config (output of `get_config()`).\n   * @param cls the class to create\n   * @param config LayersModel config dictionary.\n   * @param customObjects An optional dictionary of custom objects.\n   * @param fastWeightInit Optional flag to use fast weight initialization\n   *   during deserialization. This is applicable to cases in which\n   *   the initialization will be immediately overwritten by loaded weight\n   *   values. Default: `false`.\n   * @returns A LayersModel instance.\n   * @throws ValueError: In case of improperly formatted config dict.\n   */\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict,\n      fastWeightInit = false): T {\n    // Layer instances created during\n    // the graph reconstruction process\n    const createdLayers: {[layerName: string]: Layer} = {};\n\n    // Dictionary mapping layer instances to\n    // node data that specifies a layer call.\n    // It acts as a queue that maintains any unprocessed\n    // layer call until it becomes possible to process it\n    // (i.e. until the input tensors to the call all exist).\n    const unprocessedNodes: {[layer: string]: TensorKeyWithArgsArray[][]} = {};\n    function addUnprocessedNode(\n        layer: Layer, nodeData: TensorKeyWithArgsArray[]) {\n      if (!(layer.name in unprocessedNodes)) {\n        unprocessedNodes[layer.name] = [nodeData];\n      } else {\n        unprocessedNodes[layer.name].push(nodeData);\n      }\n    }\n\n    function processNode(layer: Layer, nodeData: TensorKeyWithArgsArray[]) {\n      const inputTensors: SymbolicTensor[] = [];\n      let kwargs;\n      for (const inputData of nodeData) {\n        const inboundLayerName = inputData[0];\n        const inboundNodeIndex = inputData[1];\n        const inboundTensorIndex = inputData[2];\n\n        kwargs = inputData[3] == null ?\n            {} :\n            inputData[3] as serialization.ConfigDict;\n        if (!(inboundLayerName in createdLayers)) {\n          addUnprocessedNode(layer, nodeData);\n          return;\n        }\n        const inboundLayer = createdLayers[inboundLayerName];\n        if (inboundLayer.inboundNodes.length <= inboundNodeIndex) {\n          addUnprocessedNode(layer, nodeData);\n          return;\n        }\n        const inboundNode = inboundLayer.inboundNodes[inboundNodeIndex];\n        inputTensors.push(inboundNode.outputTensors[inboundTensorIndex]);\n      }\n      // Call layer on its inputs, thus creating the node\n      // and building the layer if needed.\n      // Note: This has Eager vs Graph Implications.\n      if (inputTensors.length > 0) {\n        layer.apply(\n            generic_utils.singletonOrArray(inputTensors),\n            kwargs);  // was ** kwargs\n      }\n    }\n\n    /**\n     * Deserialize a layer, then call it on appropriate inputs.\n     * @param layerData: layer config dict.\n     * @throws ValueError: In case of improperly formatted `layer_data`\n     * dict.\n     */\n    function processLayer(layerData: serialization.ConfigDict|null) {\n      const layerName = layerData['name'] as string;\n      // Instantiate layer.\n      const layer =\n          deserializeLayer(\n              layerData,\n              config['customObjects'] != null ?\n                  config['customObjects'] as serialization.ConfigDict :\n                  {}) as Layer;\n      layer.setFastWeightInitDuringBuild(fastWeightInit);\n      createdLayers[layerName] = layer;\n      // Gather layer inputs.\n      const inboundNodesData =\n          layerData['inboundNodes'] as TensorKeyWithArgsArray[][];\n      inboundNodesData.forEach(nodeData => {\n        if (!(nodeData instanceof Array)) {\n          throw new ValueError(\n              `Corrupted configuration, expected array for nodeData: ${\n                  nodeData}`);\n        }\n        // We don't process nodes (i.e. make layer calls)\n        // on the fly because the inbound node may not yet exist,\n        // in case of layer shared at different topological depths\n        // (e.g.a model such as A(B(A(B(x)))))\n        addUnprocessedNode(layer, nodeData);\n      });\n    }\n\n    // First, we create all layers and enqueue nodes to be processed.\n    const name = config['name'];\n    const layersFromConfig = config['layers'] as serialization.ConfigDict[];\n    for (const layerData of layersFromConfig) {\n      processLayer(layerData);\n    }\n\n    // Then we process nodes in order of layer depth.\n    // Nodes that cannot yet be processed(if the inbound node\n    // does not yet exist) are re - enqueued, and the process\n    // is repeated until all nodes are processed.\n    while (!generic_utils.isObjectEmpty(unprocessedNodes)) {\n      for (const layerData of layersFromConfig) {\n        const layer = createdLayers[layerData['name'] as string];\n        if (layer.name in unprocessedNodes) {\n          const currentUnprocessedNodesForLayer = unprocessedNodes[layer.name];\n          delete unprocessedNodes[layer.name];\n          for (const nodeData of currentUnprocessedNodesForLayer) {\n            processNode(layer, nodeData);\n          }\n        }\n      }\n    }\n\n    const inputTensors: SymbolicTensor[] = [];\n    const outputTensors: SymbolicTensor[] = [];\n    const inputLayersFromConfig =\n        config['inputLayers'] as serialization.ConfigDict[];\n    for (const layerData of inputLayersFromConfig) {\n      const layerName = layerData[0] as string;\n      const nodeIndex = layerData[1] as number;\n      const tensorIndex = layerData[2] as number;\n      generic_utils.assert(layerName in createdLayers);\n      const layer = createdLayers[layerName];\n      const layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;\n      inputTensors.push(layerOutputTensors[tensorIndex]);\n    }\n    const outputLayersFromConfig =\n        config['outputLayers'] as serialization.ConfigDict[];\n    for (const layerData of outputLayersFromConfig) {\n      const layerName = layerData[0] as string;\n      const nodeIndex = layerData[1] as number;\n      const tensorIndex = layerData[2] as number;\n      generic_utils.assert(layerName in createdLayers);\n      const layer = createdLayers[layerName];\n      const layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;\n      outputTensors.push(layerOutputTensors[tensorIndex]);\n    }\n    return new cls({inputs: inputTensors, outputs: outputTensors, name});\n  }\n\n  /**\n   * Determine whether the container is stateful.\n   *\n   * Porting Note: this is the equivalent of the stateful @property of\n   *   the Container class in PyKeras.\n   */\n  get stateful(): boolean {\n    // Porting Note: This check is to prevent inadvertent setting of the\n    //   _stateful property of the Container instance.\n    if (this._stateful) {\n      throw new ValueError(\n          'Container instance unexpectedly has _stateful = true. The ' +\n          'statefulness of a Container is determined by the Layers it ' +\n          'contains. Its _stateful property must remain the default false.');\n    }\n    for (const layer of this.layers) {\n      if (layer.stateful) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Reset the state of all stateful constituent layers (if any).\n   *\n   * Examples of stateful layers include RNN layers whose `stateful` property\n   * is set as `true`.\n   */\n  resetStates() {\n    tidy(() => {\n      this.layers.forEach(layer => {\n        // tslint:disable:no-any\n        if (layer.stateful) {\n          layer.resetStates();\n        }\n        // tslint:enable:no-any\n      });\n    });\n  }\n}\n"]}},"error":null,"hash":"2fa4bc9fb4d30fc23a490352153d904a","cacheData":{"env":{}}}