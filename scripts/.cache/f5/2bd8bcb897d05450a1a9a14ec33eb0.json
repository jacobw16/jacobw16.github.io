{"id":"node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\engine\\training_dataset.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1577649187475},{"name":"@tensorflow/tfjs-core","loc":{"line":51,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../base_callbacks","loc":{"line":52,"column":31},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\base_callbacks.js"},{"name":"../errors","loc":{"line":53,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../logs","loc":{"line":54,"column":21},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\logs.js"},{"name":"../utils/generic_utils","loc":{"line":55,"column":30},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"},{"name":"./training_utils","loc":{"line":56,"column":31},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_dataset.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\training_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar base_callbacks_1 = require(\"../base_callbacks\");\nvar errors_1 = require(\"../errors\");\nvar logs_1 = require(\"../logs\");\nvar generic_utils_1 = require(\"../utils/generic_utils\");\nvar training_utils_1 = require(\"./training_utils\");\n// Default batch size used during tensor-based validation.\nvar DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n    var xs;\n    var ys;\n    var iteratorOutObj = iteratorOut;\n    xs = iteratorOutObj['xs'];\n    ys = iteratorOutObj['ys'];\n    tfc.util.assert(xs != null && ys != null, function () { return 'A Dataset iterator for fitDataset() is expected to generate ' +\n        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n        'string to Tensor.  The provided Dataset instead generates ' +\n        (\"\" + iteratorOut); });\n    var flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n    var flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n    var batchSize = flattenedXs[0].shape[0];\n    tfc.util.assert(flattenedXs.length === model.inputs.length, function () { return \"LayersModel has \" + model.inputs.length + \" inputs, but the dataset \" +\n        (\"provides \" + flattenedXs.length + \" inputs.  (Expected input keys: \") +\n        (JSON.stringify(model.inputNames) + \")\"); });\n    tfc.util.assert(flattenedYs.length === model.outputs.length, function () {\n        return \"LayersModel has \" + model.outputs.length + \" outputs, but the dataset \" +\n            (\"provides \" + flattenedYs.length + \" outputs.  (Expected output keys: \") +\n            (JSON.stringify(model.outputNames) + \")\");\n    });\n    var _loop_1 = function (xIndex) {\n        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, function () { return \"Batch size mismatch: input \" +\n            (model.inputNames[xIndex] + \" has \" + flattenedXs[xIndex].shape[0] + \"; \") +\n            (\"expected  \" + batchSize + \" based on input \" + model.inputNames[0] + \".\"); });\n    };\n    for (var xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n        _loop_1(xIndex);\n    }\n    var _loop_2 = function (yIndex) {\n        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, function () { return \"Batch size mismatch: output \" +\n            (model.outputNames[yIndex] + \" has \" + flattenedYs[yIndex].shape[0] + \"; \") +\n            (\"expected  \" + batchSize + \" based on input \" + model.inputNames[0] + \".\"); });\n    };\n    for (var yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n        _loop_2(yIndex);\n    }\n    return { xs: flattenedXs, ys: flattenedYs };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n    if (values instanceof tfc.Tensor) {\n        return [values];\n    }\n    else if (Array.isArray(values)) {\n        tfc.util.assert(values.length === names.length, function () { return \"Received an array of \" + values.length + \" Tensors, but expected \" + names.length + \" to match the \" + inputOrOutput + \" keys \" + names + \".\"; });\n        return values;\n    }\n    else {\n        var result = [];\n        // Check that all the required keys are available.\n        for (var _i = 0, names_1 = names; _i < names_1.length; _i++) {\n            var name_1 = names_1[_i];\n            if (values[name_1] == null) {\n                throw new errors_1.ValueError(\"The feature data generated by the dataset lacks the required \" +\n                    (inputOrOutput + \" key '\" + name_1 + \"'.\"));\n            }\n            result.push(values[name_1]);\n        }\n        return result;\n    }\n}\nfunction standardizeTensorValidationData(data) {\n    if (data.length === 3) {\n        throw new errors_1.NotImplementedError('Validation with sample weights is not implemented yet.');\n    }\n    return { xs: data[0], ys: data[1] };\n}\nfunction fitDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    return __awaiter(this, void 0, void 0, function () {\n        var hasBatchesPerEpoch, doValidation, valXs, valYs, validationData, trainFunction, outLabels, callbackMetrics, callbacks, verbose, _a, callbackList, history_1, epoch, dataIterator, epochLogs, stepsDone, batchIndex, iteratorOut, _b, xs, ys, batchLogs, sampleWeights, standardClassWeights, i, _c, _d, ins, outs, i, label, out, valOuts, _e, i;\n        return __generator(this, function (_f) {\n            switch (_f.label) {\n                case 0:\n                    hasBatchesPerEpoch = args.batchesPerEpoch != null;\n                    tfc.util.assert(model.optimizer != null, function () { return 'You must compile a model before training/testing. Use ' +\n                        'LayersModel.compile(modelCompileConfig).'; });\n                    tfc.util.assert(args != null, function () { return \"For fitDataset(), the 2nd argument (config) is required, \" +\n                        \"but it is not provided in this call.\"; });\n                    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), function () { return \"For fitDataset(), config.epochs is expected to be a positive \" +\n                        (\"integer, but got \" + args.epochs); });\n                    tfc.util.assert(!hasBatchesPerEpoch ||\n                        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), function () { return \"For fitDataset(), config.batchesPerEpoch is expected to be a \" +\n                        (\"positive integer if specified, but got \" + args.batchesPerEpoch); });\n                    tfc.util.assert(\n                    // tslint:disable-next-line:no-any\n                    args['validationSplit'] == null, function () { return '`validationSplit` is not supported by `fitDataset()`. ' +\n                        'Use validationData instead.'; });\n                    if (model.isTraining) {\n                        throw new Error('Cannot start training because another fit() call is ongoing.');\n                    }\n                    model.isTraining = true;\n                    _f.label = 1;\n                case 1:\n                    _f.trys.push([1, , 26, 27]);\n                    doValidation = args.validationData != null;\n                    valXs = void 0;\n                    valYs = void 0;\n                    if (doValidation) {\n                        if (isDatasetObject(args.validationData)) {\n                            tfc.util.assert(args.validationBatches == null ||\n                                (args.validationBatches > 0 &&\n                                    Number.isInteger(args.validationBatches)), function () { return \"For fitDataset() with dataset-based validation, \" +\n                                \"config.validationBatches is expected not to be provided, \" +\n                                \"or to be a positive integer, \" +\n                                (\"but got \" + args.validationBatches); });\n                        }\n                        else {\n                            validationData = standardizeTensorValidationData(args.validationData);\n                            valXs = validationData.xs;\n                            valYs = validationData.ys;\n                        }\n                    }\n                    trainFunction = model.makeTrainFunction();\n                    outLabels = model.getDedupedMetricsNames();\n                    callbackMetrics = void 0;\n                    if (doValidation) {\n                        callbackMetrics =\n                            outLabels.slice().concat(outLabels.map(function (n) { return 'val_' + n; }));\n                    }\n                    else {\n                        callbackMetrics = outLabels.slice();\n                    }\n                    callbacks = base_callbacks_1.standardizeCallbacks(args.callbacks, args.yieldEvery);\n                    verbose = args.verbose == null ? 1 : args.verbose;\n                    _a = base_callbacks_1.configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n                    doValidation, callbackMetrics), callbackList = _a.callbackList, history_1 = _a.history;\n                    callbackList.setModel(model);\n                    model.history = history_1;\n                    return [4 /*yield*/, callbackList.onTrainBegin()];\n                case 2:\n                    _f.sent();\n                    model.stopTraining_ = false;\n                    epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n                    return [4 /*yield*/, dataset.iterator()];\n                case 3:\n                    dataIterator = _f.sent();\n                    _f.label = 4;\n                case 4:\n                    if (!(epoch < args.epochs)) return [3 /*break*/, 23];\n                    epochLogs = {};\n                    return [4 /*yield*/, callbackList.onEpochBegin(epoch)];\n                case 5:\n                    _f.sent();\n                    stepsDone = 0;\n                    batchIndex = 0;\n                    if (!!hasBatchesPerEpoch) return [3 /*break*/, 7];\n                    return [4 /*yield*/, dataset.iterator()];\n                case 6:\n                    dataIterator = _f.sent();\n                    _f.label = 7;\n                case 7:\n                    if (!(hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true)) return [3 /*break*/, 21];\n                    return [4 /*yield*/, dataIterator.next()];\n                case 8:\n                    iteratorOut = _f.sent();\n                    // If `batchesPerEpoch` is specified, the dataset should not be\n                    // exhausted until all epoches are done.\n                    if (hasBatchesPerEpoch && iteratorOut.done) {\n                        console.warn('You provided `batchesPerEpoch` as ' +\n                            (args.batchesPerEpoch + \", \") +\n                            'but your dataset iterator ran out of data after ' +\n                            (stepsDone + \" batches; \") +\n                            'interrupting training. Make sure that your ' +\n                            'dataset can generate at least `batchesPerEpoch * epochs` ' +\n                            'batches (in this case, ' +\n                            (args.batchesPerEpoch * args.epochs + \" batches). \") +\n                            'You may need to use the repeat() function when building ' +\n                            'your dataset.');\n                        return [3 /*break*/, 21];\n                    }\n                    if (!(iteratorOut.value != null)) return [3 /*break*/, 15];\n                    _b = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _b.xs, ys = _b.ys;\n                    batchLogs = {};\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = xs[0].shape[0];\n                    return [4 /*yield*/, callbackList.onBatchBegin(batchIndex, batchLogs)];\n                case 9:\n                    _f.sent();\n                    sampleWeights = [];\n                    if (!(args.classWeight != null)) return [3 /*break*/, 13];\n                    standardClassWeights = training_utils_1.standardizeClassWeights(args.classWeight, model.outputNames);\n                    i = 0;\n                    _f.label = 10;\n                case 10:\n                    if (!(i < standardClassWeights.length)) return [3 /*break*/, 13];\n                    _d = (_c = sampleWeights).push;\n                    return [4 /*yield*/, training_utils_1.standardizeWeights(ys[i], null, standardClassWeights[i])];\n                case 11:\n                    _d.apply(_c, [_f.sent()]);\n                    _f.label = 12;\n                case 12:\n                    ++i;\n                    return [3 /*break*/, 10];\n                case 13:\n                    ins = xs.concat(ys).concat(sampleWeights);\n                    outs = trainFunction(ins);\n                    tfc.dispose(ins);\n                    for (i = 0; i < outLabels.length; ++i) {\n                        label = outLabels[i];\n                        out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                    }\n                    return [4 /*yield*/, callbackList.onBatchEnd(batchIndex, batchLogs)];\n                case 14:\n                    _f.sent();\n                    logs_1.disposeTensorsInLogs(batchLogs);\n                    batchIndex++;\n                    stepsDone++;\n                    _f.label = 15;\n                case 15:\n                    if (!(hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                        iteratorOut.done)) return [3 /*break*/, 20];\n                    if (!doValidation) return [3 /*break*/, 19];\n                    valOuts = void 0;\n                    if (!isDatasetObject(args.validationData)) return [3 /*break*/, 17];\n                    _e = generic_utils_1.toList;\n                    return [4 /*yield*/, model.evaluateDataset(args.validationData, { batches: args.validationBatches })];\n                case 16:\n                    valOuts = _e.apply(void 0, [_f.sent()]);\n                    return [3 /*break*/, 18];\n                case 17:\n                    valOuts = generic_utils_1.toList(model.evaluate(valXs, valYs, {\n                        batchSize: args.validationBatchSize == null ?\n                            DEFAULT_VALIDATION_BATCH_SIZE :\n                            args.validationBatchSize,\n                        verbose: 0\n                    }));\n                    _f.label = 18;\n                case 18:\n                    for (i = 0; i < model.metricsNames.length; ++i) {\n                        epochLogs[\"val_\" + model.metricsNames[i]] = valOuts[i];\n                    }\n                    _f.label = 19;\n                case 19: \n                // Call `break` to exit one epoch lopp after validation is done. If\n                // config.batchesPerEpoch is specified, an epoch while loop will\n                // stop when `stepsDone >= config.batchesPerEpoch`. When\n                // config.batchesPerEpoch is not provided, the following `break` is\n                // required to exit the while lopp after dataset is exhausted.\n                return [3 /*break*/, 21];\n                case 20:\n                    if (model.stopTraining_) {\n                        return [3 /*break*/, 21];\n                    }\n                    return [3 /*break*/, 7];\n                case 21: return [4 /*yield*/, callbackList.onEpochEnd(epoch, epochLogs)];\n                case 22:\n                    _f.sent();\n                    epoch++;\n                    if (model.stopTraining_) {\n                        return [3 /*break*/, 23];\n                    }\n                    return [3 /*break*/, 4];\n                case 23: return [4 /*yield*/, callbackList.onTrainEnd()];\n                case 24:\n                    _f.sent();\n                    return [4 /*yield*/, model.history.syncData()];\n                case 25:\n                    _f.sent();\n                    return [2 /*return*/, model.history];\n                case 26:\n                    model.isTraining = false;\n                    return [7 /*endfinally*/];\n                case 27: return [2 /*return*/];\n            }\n        });\n    });\n}\nexports.fitDataset = fitDataset;\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch(dataset, args) {\n    // Attempt to determine # of batches in an epoch.\n    var stepsPerEpoch = null;\n    if (args.batchesPerEpoch != null) {\n        stepsPerEpoch = args.batchesPerEpoch;\n    }\n    else if (Number.isFinite(dataset.size)) {\n        stepsPerEpoch = dataset.size;\n    }\n    return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking it's .iterator\n// element.\nfunction isDatasetObject(dataset) {\n    return (typeof dataset.iterator === 'function');\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n    return (typeof iterator.next === 'function');\n}\nfunction evaluateDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    return __awaiter(this, void 0, void 0, function () {\n        var hasBatches, f, outs, dataIterator, _a, numExamples, batch, _loop_3, state_1, i, oldScalar;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0:\n                    args = args || {};\n                    hasBatches = args.batches != null;\n                    f = model.testFunction;\n                    outs = [];\n                    if (args.verbose > 0) {\n                        throw new errors_1.NotImplementedError('Verbose mode is not implemented yet.');\n                    }\n                    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), function () { return 'Test loop expects `batches` to be a positive integer, but ' +\n                        (\"received \" + JSON.stringify(args.batches)); });\n                    if (!isLazyIteratorObject(dataset)) return [3 /*break*/, 1];\n                    _a = dataset;\n                    return [3 /*break*/, 3];\n                case 1: return [4 /*yield*/, dataset.iterator()];\n                case 2:\n                    _a = _b.sent();\n                    _b.label = 3;\n                case 3:\n                    dataIterator = _a;\n                    numExamples = 0;\n                    batch = 0;\n                    _loop_3 = function () {\n                        var iteratorOut;\n                        return __generator(this, function (_a) {\n                            switch (_a.label) {\n                                case 0: return [4 /*yield*/, dataIterator.next()];\n                                case 1:\n                                    iteratorOut = _a.sent();\n                                    outs = tfc.tidy(function () {\n                                        if (iteratorOut.value) {\n                                            // TODO(cais): Once real dataset is available, use\n                                            //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                                            var _a = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _a.xs, ys = _a.ys;\n                                            var xsAndYs_1 = xs.concat(ys);\n                                            var batchOuts = tfc.tidy(function () { return f(xsAndYs_1); });\n                                            tfc.dispose(xsAndYs_1);\n                                            if (batch === 0) {\n                                                for (var i = 0; i < batchOuts.length; ++i) {\n                                                    outs.push(tfjs_core_1.scalar(0));\n                                                }\n                                            }\n                                            var batchSize_1 = xsAndYs_1[0].shape[0];\n                                            var _loop_4 = function (i) {\n                                                var batchOut = batchOuts[i];\n                                                var oldScalar = outs[i];\n                                                outs[i] =\n                                                    tfc.tidy(function () { return tfc.add(outs[i], tfc.mul(batchSize_1, batchOut)); });\n                                                if (batch > 0) {\n                                                    tfc.dispose(oldScalar);\n                                                }\n                                            };\n                                            for (var i = 0; i < batchOuts.length; ++i) {\n                                                _loop_4(i);\n                                            }\n                                            tfc.dispose(batchOuts);\n                                            numExamples += batchSize_1;\n                                            ++batch;\n                                        }\n                                        return outs;\n                                    });\n                                    if (iteratorOut.done) {\n                                        if (hasBatches) {\n                                            console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +\n                                                'Interrupting evalution. Make sure that your ' +\n                                                'dataset can generate at least `batches` ' +\n                                                (\"batches (in this case, \" + args.batches + \" batches). \") +\n                                                'You may need to use the repeat() function when building ' +\n                                                'your dataset.');\n                                        }\n                                        return [2 /*return*/, \"break\"];\n                                    }\n                                    return [2 /*return*/];\n                            }\n                        });\n                    };\n                    _b.label = 4;\n                case 4:\n                    if (!(hasBatches ? batch < args.batches : true)) return [3 /*break*/, 6];\n                    return [5 /*yield**/, _loop_3()];\n                case 5:\n                    state_1 = _b.sent();\n                    if (state_1 === \"break\")\n                        return [3 /*break*/, 6];\n                    return [3 /*break*/, 4];\n                case 6:\n                    for (i = 0; i < outs.length; ++i) {\n                        oldScalar = outs[i];\n                        outs[i] = tfc.div(outs[i], numExamples);\n                        tfc.dispose(oldScalar);\n                    }\n                    return [2 /*return*/, generic_utils_1.singletonOrArray(outs)];\n            }\n        });\n    });\n}\nexports.evaluateDataset = evaluateDataset;\n"},"sourceMaps":{"js":{"version":3,"file":"training_dataset.js","sourceRoot":"","sources":["../../src/engine/training_dataset.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAEH;;GAEG;AAEH,2CAA6C;AAC7C,mDAA6C;AAC7C,oDAAgK;AAChK,oCAA0D;AAC1D,gCAA6D;AAE7D,wDAAgE;AAGhE,mDAA0G;AAiK1G,0DAA0D;AAC1D,IAAM,6BAA6B,GAAG,EAAE,CAAC;AAEzC;;;;;;;;;;;;;GAaG;AACH,SAAS,6BAA6B;AAClC,6DAA6D;AAC7D,eAAe;AACf,kCAAkC;AAClC,KAAU,EAAE,WAAe;IAC7B,IAAI,EAAsB,CAAC;IAC3B,IAAI,EAAsB,CAAC;IAE3B,IAAM,cAAc,GAAG,WAAgC,CAAC;IACxD,EAAE,GAAG,cAAc,CAAC,IAAI,CAAC,CAAC;IAC1B,EAAE,GAAG,cAAc,CAAC,IAAI,CAAC,CAAC;IAC1B,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,EAAE,IAAI,IAAI,IAAI,EAAE,IAAI,IAAI,EACxB,cAAM,OAAA,8DAA8D;QAChE,4DAA4D;QAC5D,8DAA8D;QAC9D,4DAA4D;SAC5D,KAAG,WAAa,CAAA,EAJd,CAIc,CAAC,CAAC;IAE1B,IAAM,WAAW,GACb,yBAAyB,CAAC,OAAO,EAAE,KAAK,CAAC,UAAU,EAAE,EAAE,CAAC,CAAC;IAC7D,IAAM,WAAW,GACb,yBAAyB,CAAC,QAAQ,EAAE,KAAK,CAAC,WAAW,EAAE,EAAE,CAAC,CAAC;IAE/D,IAAM,SAAS,GAAW,WAAW,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IAElD,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,KAAK,KAAK,CAAC,MAAM,CAAC,MAAM,EAC1C,cAAM,OAAA,qBAAmB,KAAK,CAAC,MAAM,CAAC,MAAM,8BAA2B;SACnE,cAAY,WAAW,CAAC,MAAM,qCAAkC,CAAA;SAC7D,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,UAAU,CAAC,MAAG,CAAA,EAFpC,CAEoC,CAAC,CAAC;IAEhD,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,KAAK,KAAK,CAAC,OAAO,CAAC,MAAM,EAC3C;QACI,OAAA,qBAAmB,KAAK,CAAC,OAAO,CAAC,MAAM,+BAA4B;aACnE,cAAY,WAAW,CAAC,MAAM,uCAAoC,CAAA;aAC/D,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,WAAW,CAAC,MAAG,CAAA;IAFvC,CAEuC,CAAC,CAAC;4BAExC,MAAM;QACb,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,SAAS,EAC1C,cAAM,OAAA,6BAA6B;aAC5B,KAAK,CAAC,UAAU,CAAC,MAAM,CAAC,aACrB,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,OAAI,CAAA;aACtC,eAAa,SAAS,wBAAmB,KAAK,CAAC,UAAU,CAAC,CAAC,CAAC,MAAG,CAAA,EAH7D,CAG6D,CAAC,CAAC;;IAN3E,KAAK,IAAI,MAAM,GAAG,CAAC,EAAE,MAAM,GAAG,WAAW,CAAC,MAAM,EAAE,MAAM,EAAE;gBAAjD,MAAM;KAOd;4BAEQ,MAAM;QACb,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,SAAS,EAC1C,cAAM,OAAA,8BAA8B;aAC7B,KAAK,CAAC,WAAW,CAAC,MAAM,CAAC,aACtB,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,OAAI,CAAA;aACtC,eAAa,SAAS,wBAAmB,KAAK,CAAC,UAAU,CAAC,CAAC,CAAC,MAAG,CAAA,EAH7D,CAG6D,CAAC,CAAC;;IAN3E,KAAK,IAAI,MAAM,GAAG,CAAC,EAAE,MAAM,GAAG,WAAW,CAAC,MAAM,EAAE,MAAM,EAAE;gBAAjD,MAAM;KAOd;IAED,OAAO,EAAC,EAAE,EAAE,WAAW,EAAE,EAAE,EAAE,WAAW,EAAC,CAAC;AAC5C,CAAC;AAED,SAAS,yBAAyB,CAC9B,aAAqB,EAAE,KAAe,EAAE,MAA0B;IACpE,IAAI,MAAM,YAAY,GAAG,CAAC,MAAM,EAAE;QAChC,OAAO,CAAC,MAAM,CAAC,CAAC;KACjB;SAAM,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QAChC,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,MAAM,CAAC,MAAM,KAAK,KAAK,CAAC,MAAM,EAC9B,cAAM,OAAA,0BAAwB,MAAM,CAAC,MAAM,+BACvC,KAAK,CAAC,MAAM,sBAAiB,aAAa,cAAS,KAAK,MAAG,EADzD,CACyD,CAAC,CAAC;QACrE,OAAO,MAAM,CAAC;KACf;SAAM;QACL,IAAM,MAAM,GAAiB,EAAE,CAAC;QAChC,kDAAkD;QAClD,KAAmB,UAAK,EAAL,eAAK,EAAL,mBAAK,EAAL,IAAK,EAAE;YAArB,IAAM,MAAI,cAAA;YACb,IAAI,MAAM,CAAC,MAAI,CAAC,IAAI,IAAI,EAAE;gBACxB,MAAM,IAAI,mBAAU,CAChB,+DAA+D;qBAC5D,aAAa,cAAS,MAAI,OAAI,CAAA,CAAC,CAAC;aACxC;YACD,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,MAAI,CAAC,CAAC,CAAC;SAC3B;QACD,OAAO,MAAM,CAAC;KACf;AACH,CAAC;AAED,SAAS,+BAA+B,CACpC,IAIiC;IAEnC,IAAI,IAAI,CAAC,MAAM,KAAK,CAAC,EAAE;QACrB,MAAM,IAAI,4BAAmB,CACzB,wDAAwD,CAAC,CAAC;KAC/D;IACD,OAAO,EAAC,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC,EAAC,CAAC;AACpC,CAAC;AAED,SAAsB,UAAU;AAC5B,6DAA6D;AAC7D,eAAe;AACf,kCAAkC;AAClC,KAAU,EAAE,OAAmB,EAC/B,IAA4B;;;;;;oBACxB,kBAAkB,GAAG,IAAI,CAAC,eAAe,IAAI,IAAI,CAAC;oBACxD,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,KAAK,CAAC,SAAS,IAAI,IAAI,EACvB,cAAM,OAAA,wDAAwD;wBAC1D,0CAA0C,EADxC,CACwC,CAAC,CAAC;oBAEpD,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,IAAI,IAAI,IAAI,EACZ,cAAM,OAAA,2DAA2D;wBAC7D,sCAAsC,EADpC,CACoC,CAAC,CAAC;oBAChD,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,IAAI,CAAC,MAAM,IAAI,IAAI,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,MAAM,CAAC,EACvE,cAAM,OAAA,+DAA+D;yBACjE,sBAAoB,IAAI,CAAC,MAAQ,CAAA,EAD/B,CAC+B,CAAC,CAAC;oBAC3C,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,CAAC,kBAAkB;wBACf,CAAC,IAAI,CAAC,eAAe,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC,EACxE,cAAM,OAAA,+DAA+D;yBACjE,4CAA0C,IAAI,CAAC,eAAiB,CAAA,EAD9D,CAC8D,CAAC,CAAC;oBAC1E,GAAG,CAAC,IAAI,CAAC,MAAM;oBACX,kCAAkC;oBACjC,IAAY,CAAC,iBAAiB,CAAC,IAAI,IAAI,EACxC,cAAM,OAAA,wDAAwD;wBAC1D,6BAA6B,EAD3B,CAC2B,CAAC,CAAC;oBAEvC,IAAI,KAAK,CAAC,UAAU,EAAE;wBACpB,MAAM,IAAI,KAAK,CACX,8DAA8D,CAAC,CAAC;qBACrE;oBACD,KAAK,CAAC,UAAU,GAAG,IAAI,CAAC;;;;oBAGhB,YAAY,GAAG,IAAI,CAAC,cAAc,IAAI,IAAI,CAAC;oBAC7C,KAAK,SAAyB,CAAC;oBAC/B,KAAK,SAAyB,CAAC;oBACnC,IAAI,YAAY,EAAE;wBAChB,IAAI,eAAe,CAAC,IAAI,CAAC,cAAc,CAAC,EAAE;4BACxC,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,IAAI,CAAC,iBAAiB,IAAI,IAAI;gCAC1B,CAAC,IAAI,CAAC,iBAAiB,GAAG,CAAC;oCAC1B,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC,EAC9C,cAAM,OAAA,kDAAkD;gCACpD,2DAA2D;gCAC3D,+BAA+B;iCAC/B,aAAW,IAAI,CAAC,iBAAmB,CAAA,EAHjC,CAGiC,CAAC,CAAC;yBAC9C;6BAAM;4BACC,cAAc,GAAG,+BAA+B,CAClD,IAAI,CAAC,cAKJ,CAAC,CAAC;4BACP,KAAK,GAAG,cAAc,CAAC,EAAE,CAAC;4BAC1B,KAAK,GAAG,cAAc,CAAC,EAAE,CAAC;yBAC3B;qBACF;oBAEK,aAAa,GAAG,KAAK,CAAC,iBAAiB,EAAE,CAAC;oBAC1C,SAAS,GAAG,KAAK,CAAC,sBAAsB,EAAc,CAAC;oBAEzD,eAAe,SAAU,CAAC;oBAC9B,IAAI,YAAY,EAAE;wBAChB,eAAe;4BACX,SAAS,CAAC,KAAK,EAAE,CAAC,MAAM,CAAC,SAAS,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,MAAM,GAAG,CAAC,EAAV,CAAU,CAAC,CAAC,CAAC;qBAC9D;yBAAM;wBACL,eAAe,GAAG,SAAS,CAAC,KAAK,EAAE,CAAC;qBACrC;oBAEK,SAAS,GAAG,qCAAoB,CAAC,IAAI,CAAC,SAAS,EAAE,IAAI,CAAC,UAAU,CAAC,CAAC;oBAClE,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC;oBAClD,KAA0B,mCAAkB,CAC9C,SAAS,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI,EAAE,IAAI,EAC3C,gBAAgB,CAAC,OAAO,EAAE,IAAI,CAAC,EAC/B,IAAI,EAAG,+CAA+C;oBACtD,YAAY,EAAE,eAAe,CAAC,EAJ3B,YAAY,kBAAA,EAAE,sBAAO,CAIO;oBACnC,YAAY,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC;oBAC7B,KAAK,CAAC,OAAO,GAAG,SAAO,CAAC;oBAExB,qBAAM,YAAY,CAAC,YAAY,EAAE,EAAA;;oBAAjC,SAAiC,CAAC;oBAClC,KAAK,CAAC,aAAa,GAAG,KAAK,CAAC;oBACxB,KAAK,GAAG,IAAI,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC;oBAE3C,qBAAM,OAAO,CAAC,QAAQ,EAAE,EAAA;;oBAAvC,YAAY,GAAG,SAAwB;;;yBACpC,CAAA,KAAK,GAAG,IAAI,CAAC,MAAM,CAAA;oBAClB,SAAS,GAAmB,EAAE,CAAC;oBACrC,qBAAM,YAAY,CAAC,YAAY,CAAC,KAAK,CAAC,EAAA;;oBAAtC,SAAsC,CAAC;oBACnC,SAAS,GAAG,CAAC,CAAC;oBACd,UAAU,GAAG,CAAC,CAAC;yBACf,CAAC,kBAAkB,EAAnB,wBAAmB;oBACN,qBAAM,OAAO,CAAC,QAAQ,EAAE,EAAA;;oBAAvC,YAAY,GAAG,SAAwB,CAAC;;;yBAEnC,CAAA,kBAAkB,CAAC,CAAC,CAAC,SAAS,GAAG,IAAI,CAAC,eAAe,CAAC,CAAC,CAAC,IAAI,CAAA;oBAC7C,qBAAM,YAAY,CAAC,IAAI,EAAE,EAAA;;oBAAvC,WAAW,GAAG,SAAyB;oBAE7C,+DAA+D;oBAC/D,wCAAwC;oBACxC,IAAI,kBAAkB,IAAI,WAAW,CAAC,IAAI,EAAE;wBAC1C,OAAO,CAAC,IAAI,CACR,oCAAoC;6BACjC,IAAI,CAAC,eAAe,OAAI,CAAA;4BAC3B,kDAAkD;6BAC/C,SAAS,eAAY,CAAA;4BACxB,6CAA6C;4BAC7C,2DAA2D;4BAC3D,yBAAyB;6BACtB,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,MAAM,gBAAa,CAAA;4BAClD,0DAA0D;4BAC1D,eAAe,CAAC,CAAC;wBACrB,yBAAM;qBACP;yBAEG,CAAA,WAAW,CAAC,KAAK,IAAI,IAAI,CAAA,EAAzB,yBAAyB;oBACrB,KACF,6BAA6B,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,CAAC,EADpD,EAAE,QAAA,EAAE,EAAE,QAAA,CAC+C;oBACtD,SAAS,GAAmB,EAAE,CAAC;oBACrC,SAAS,CAAC,OAAO,CAAC,GAAG,UAAU,CAAC;oBAChC,SAAS,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;oBAEnC,qBAAM,YAAY,CAAC,YAAY,CAAC,UAAU,EAAE,SAAS,CAAC,EAAA;;oBAAtD,SAAsD,CAAC;oBAEjD,aAAa,GAAiB,EAAE,CAAC;yBACnC,CAAA,IAAI,CAAC,WAAW,IAAI,IAAI,CAAA,EAAxB,yBAAwB;oBACpB,oBAAoB,GACtB,wCAAuB,CAAC,IAAI,CAAC,WAAW,EAAE,KAAK,CAAC,WAAW,CAAC,CAAC;oBACxD,CAAC,GAAG,CAAC;;;yBAAE,CAAA,CAAC,GAAG,oBAAoB,CAAC,MAAM,CAAA;oBAC7C,KAAA,CAAA,KAAA,aAAa,CAAA,CAAC,IAAI,CAAA;oBAAC,qBAAM,mCAAkB,CACvC,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,oBAAoB,CAAC,CAAC,CAAC,CAAC,EAAA;;oBADzC,cAAmB,SACsB,EAAC,CAAC;;;oBAFI,EAAE,CAAC,CAAA;;;oBAOhD,GAAG,GAAG,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC,MAAM,CAAC,aAAa,CAAC,CAAC;oBAC1C,IAAI,GAAG,aAAa,CAAC,GAAG,CAAC,CAAC;oBAChC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;oBACjB,KAAS,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;wBACnC,KAAK,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;wBACrB,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;wBACpB,SAAS,CAAC,KAAK,CAAC,GAAG,GAAG,CAAC;wBACvB,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;qBACf;oBAED,qBAAM,YAAY,CAAC,UAAU,CAAC,UAAU,EAAE,SAAS,CAAC,EAAA;;oBAApD,SAAoD,CAAC;oBACrD,2BAAoB,CAAC,SAAS,CAAC,CAAC;oBAEhC,UAAU,EAAE,CAAC;oBACb,SAAS,EAAE,CAAC;;;yBAGV,CAAA,kBAAkB,CAAC,CAAC,CAAC,SAAS,IAAI,IAAI,CAAC,eAAe,CAAC,CAAC;wBACnC,WAAW,CAAC,IAAI,CAAA,EADrC,yBACqC;yBAEnC,YAAY,EAAZ,yBAAY;oBACV,OAAO,SAAc,CAAC;yBACtB,eAAe,CAAC,IAAI,CAAC,cAAc,CAAC,EAApC,yBAAoC;oBAC5B,KAAA,sBAAM,CAAA;oBAAC,qBAAM,KAAK,CAAC,eAAe,CACxC,IAAI,CAAC,cAAc,EAAE,EAAC,OAAO,EAAE,IAAI,CAAC,iBAAiB,EAAC,CAAC,EAAA;;oBAD3D,OAAO,GAAG,kBAAO,SAC0C,EAAC,CAAC;;;oBAE7D,OAAO,GAAG,sBAAM,CAAC,KAAK,CAAC,QAAQ,CAAC,KAAK,EAAE,KAAK,EAAE;wBAC5C,SAAS,EAAE,IAAI,CAAC,mBAAmB,IAAI,IAAI,CAAC,CAAC;4BACzC,6BAA6B,CAAC,CAAC;4BAC/B,IAAI,CAAC,mBAAmB;wBAC5B,OAAO,EAAE,CAAC;qBACX,CAAC,CAAC,CAAC;;;oBAEN,KAAS,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;wBAClD,SAAS,CAAC,SAAO,KAAK,CAAC,YAAY,CAAC,CAAC,CAAG,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;qBACxD;;;gBAEH,mEAAmE;gBACnE,gEAAgE;gBAChE,wDAAwD;gBACxD,mEAAmE;gBACnE,8DAA8D;gBAC9D,yBAAM;;oBAGR,IAAI,KAAK,CAAC,aAAa,EAAE;wBACvB,yBAAM;qBACP;;yBAEH,qBAAM,YAAY,CAAC,UAAU,CAAC,KAAK,EAAE,SAAS,CAAC,EAAA;;oBAA/C,SAA+C,CAAC;oBAChD,KAAK,EAAE,CAAC;oBACR,IAAI,KAAK,CAAC,aAAa,EAAE;wBACvB,yBAAM;qBACP;;yBAEH,qBAAM,YAAY,CAAC,UAAU,EAAE,EAAA;;oBAA/B,SAA+B,CAAC;oBAChC,qBAAM,KAAK,CAAC,OAAO,CAAC,QAAQ,EAAE,EAAA;;oBAA9B,SAA8B,CAAC;oBAC/B,sBAAO,KAAK,CAAC,OAAO,EAAC;;oBAErB,KAAK,CAAC,UAAU,GAAG,KAAK,CAAC;;;;;;CAE5B;AAvMD,gCAuMC;AAED,2EAA2E;AAC3E,SAAS,gBAAgB,CACrB,OAAmB,EAAE,IAA4B;IACnD,iDAAiD;IACjD,IAAI,aAAa,GAAW,IAAI,CAAC;IACjC,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;QAChC,aAAa,GAAG,IAAI,CAAC,eAAe,CAAC;KACtC;SAAM,IAAI,MAAM,CAAC,QAAQ,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;QACxC,aAAa,GAAG,OAAO,CAAC,IAAI,CAAC;KAC9B;IACD,OAAO,aAAa,CAAC;AACvB,CAAC;AAED,0EAA0E;AAC1E,WAAW;AACX,SAAS,eAAe,CACpB,OAIU;IACZ,OAAO,CAAC,OAAQ,OAAsB,CAAC,QAAQ,KAAK,UAAU,CAAC,CAAC;AAClE,CAAC;AAED,2EAA2E;AAC3E,WAAW;AACX,SAAS,oBAAoB,CAAI,QACe;IAC9C,OAAO,CAAC,OAAQ,QAA4B,CAAC,IAAI,KAAK,UAAU,CAAC,CAAC;AACpE,CAAC;AAED,SAAsB,eAAe;AACjC,6DAA6D;AAC7D,eAAe;AACf,kCAAkC;AAClC,KAAU,EAAE,OAAmC,EAC/C,IAA8B;;;;;;oBAChC,IAAI,GAAG,IAAI,IAAI,EAAE,CAAC;oBACZ,UAAU,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC;oBAClC,CAAC,GAAG,KAAK,CAAC,YAAY,CAAC;oBACzB,IAAI,GAAiB,EAAE,CAAC;oBAC5B,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,EAAE;wBACpB,MAAM,IAAI,4BAAmB,CAAC,sCAAsC,CAAC,CAAC;qBACvE;oBAED,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,CAAC,UAAU,IAAI,CAAC,IAAI,CAAC,OAAO,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,EACnE,cAAM,OAAA,4DAA4D;yBAC9D,cAAY,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,OAAO,CAAG,CAAA,EADxC,CACwC,CAAC,CAAC;yBAC/B,oBAAoB,CAAC,OAAO,CAAC,EAA7B,wBAA6B;oBAC9C,KAAA,OAA0B,CAAA;;wBAC1B,qBAAO,OAAsB,CAAC,QAAQ,EAAE,EAAA;;oBAAxC,KAAA,SAAwC,CAAA;;;oBAFtC,YAAY,KAE0B;oBAExC,WAAW,GAAG,CAAC,CAAC;oBAChB,KAAK,GAAG,CAAC,CAAC;;;;;wCAGQ,qBAAM,YAAY,CAAC,IAAI,EAAE,EAAA;;oCAAvC,WAAW,GAAG,SAAyB;oCAC7C,IAAI,GAAG,GAAG,CAAC,IAAI,CAAC;wCACd,IAAI,WAAW,CAAC,KAAK,EAAE;4CACrB,kDAAkD;4CAClD,+DAA+D;4CACzD,IAAA,4DACqD,EADpD,UAAE,EAAE,UACgD,CAAC;4CAC5D,IAAM,SAAO,GAAG,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC;4CAC9B,IAAM,SAAS,GAAG,GAAG,CAAC,IAAI,CAAC,cAAM,OAAA,CAAC,CAAC,SAAO,CAAC,EAAV,CAAU,CAAC,CAAC;4CAC7C,GAAG,CAAC,OAAO,CAAC,SAAO,CAAC,CAAC;4CAErB,IAAI,KAAK,KAAK,CAAC,EAAE;gDACf,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;oDACzC,IAAI,CAAC,IAAI,CAAC,kBAAM,CAAC,CAAC,CAAC,CAAC,CAAC;iDACtB;6CACF;4CAED,IAAM,WAAS,GAAG,SAAO,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;oEAC7B,CAAC;gDACR,IAAM,QAAQ,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gDAC9B,IAAM,SAAS,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;gDAC1B,IAAI,CAAC,CAAC,CAAC;oDACH,GAAG,CAAC,IAAI,CAAC,cAAM,OAAA,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,WAAS,EAAE,QAAQ,CAAC,CAAC,EAA9C,CAA8C,CAAC,CAAC;gDACnE,IAAI,KAAK,GAAG,CAAC,EAAE;oDACb,GAAG,CAAC,OAAO,CAAC,SAAS,CAAC,CAAC;iDACxB;;4CAPH,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC;wDAAhC,CAAC;6CAQT;4CACD,GAAG,CAAC,OAAO,CAAC,SAAS,CAAC,CAAC;4CACvB,WAAW,IAAI,WAAS,CAAC;4CAEzB,EAAE,KAAK,CAAC;yCACT;wCACD,OAAO,IAAI,CAAC;oCACd,CAAC,CAAC,CAAC;oCAEH,IAAI,WAAW,CAAC,IAAI,EAAE;wCACpB,IAAI,UAAU,EAAE;4CACd,OAAO,CAAC,IAAI,CACR,kEAAkE;gDAClE,8CAA8C;gDAC9C,0CAA0C;iDAC1C,4BAA0B,IAAI,CAAC,OAAO,gBAAa,CAAA;gDACnD,0DAA0D;gDAC1D,eAAe,CAAC,CAAC;yCACtB;;qCAEF;;;;;;;yBA/CI,CAAA,UAAU,CAAC,CAAC,CAAC,KAAK,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,IAAI,CAAA;;;;;;;;oBAkD/C,KAAS,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;wBAC9B,SAAS,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;wBAC1B,IAAI,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC;wBACxC,GAAG,CAAC,OAAO,CAAC,SAAS,CAAC,CAAC;qBACxB;oBAED,sBAAO,gCAAgB,CAAC,IAAI,CAAC,EAAC;;;;CAC/B;AAlFD,0CAkFC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {scalar} from '@tensorflow/tfjs-core';\nimport {BaseCallback, configureCallbacks, CustomCallbackArgs, History, ModelLoggingVerbosity, standardizeCallbacks, YieldEveryOptions} from '../base_callbacks';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {disposeTensorsInLogs, UnresolvedLogs} from '../logs';\nimport {TensorOrArrayOrMap} from '../types';\nimport {singletonOrArray, toList} from '../utils/generic_utils';\n\nimport {Dataset, LazyIterator} from './dataset_stub';\nimport {ClassWeight, ClassWeightMap, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Interface for configuring model training based on a dataset object.\n */\nexport interface ModelFitDatasetArgs<T> {\n  /**\n   * (Optional) Total number of steps (batches of samples) before\n   * declaring one epoch finished and starting the next epoch. It should\n   * typically be equal to the number of samples of your dataset divided by\n   * the batch size, so that `fitDataset`() call can utilize the entire dataset.\n   * If it is not provided, use `done` return value in `iterator.next()` as\n   * signal to finish an epoch.\n   */\n  batchesPerEpoch?: number;\n\n  /**\n   * The number of times to iterate over the training dataset.\n   *\n   * An integer.\n   */\n  epochs: number;\n\n  /**\n   * Verbosity level.\n   *\n   * Expected to be 0, 1, or 2. Default: 1.\n   *\n   * 0 - No printed message during fit() call.\n   * 1 - In Node.js (tfjs-node), prints the progress bar, together with\n   *     real-time updates of loss and metric values and training speed.\n   *     In the browser: no action. This is the default.\n   * 2 - Not implemented yet.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * List of callbacks to be called during training.\n   * Can have one or more of the following callbacks:\n   *   - `onTrainBegin(logs)`: called when training starts.\n   *   - `onTrainEnd(logs)`: called when training ends.\n   *   - `onEpochBegin(epoch, logs)`: called at the start of every epoch.\n   *   - `onEpochEnd(epoch, logs)`: called at the end of every epoch.\n   *   - `onBatchBegin(batch, logs)`: called at the start of every batch.\n   *   - `onBatchEnd(batch, logs)`: called at the end of every batch.\n   *   - `onYield(epoch, batch, logs)`: called every `yieldEvery` milliseconds\n   *      with the current epoch, batch and logs. The logs are the same\n   *      as in `onBatchEnd()`. Note that `onYield` can skip batches or\n   *      epochs. See also docs for `yieldEvery` below.\n   */\n  callbacks?: BaseCallback[]|CustomCallbackArgs|CustomCallbackArgs[];\n\n  /**\n   * Data on which to evaluate the loss and any model\n   * metrics at the end of each epoch. The model will not be trained on this\n   * data. This could be any of the following:\n   *\n   *   - An array `[xVal, yVal]`, where the two values may be `tf.Tensor`,\n   *     an array of Tensors, or a map of string to Tensor.\n   *   - Similarly, an array ` [xVal, yVal, valSampleWeights]`\n   *     (not implemented yet).\n   *   - a `Dataset` object with elements of the form `{xs: xVal, ys: yVal}`,\n   *     where `xs` and `ys` are the feature and label tensors, respectively.\n   *\n   * If `validationData` is an Array of Tensor objects, each `tf.Tensor` will be\n   * sliced into batches during validation, using the parameter\n   * `validationBatchSize` (which defaults to 32). The entirety of the\n   * `tf.Tensor` objects will be used in the validation.\n   *\n   * If `validationData` is a dataset object, and the `validationBatches`\n   * parameter is specified, the validation will use `validationBatches` batches\n   * drawn from the dataset object. If `validationBatches` parameter is not\n   * specified, the validation will stop when the dataset is exhausted.\n   *\n   * The model will not be trained on this data.\n   */\n  validationData?: [\n    TensorOrArrayOrMap, TensorOrArrayOrMap\n  ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|Dataset<T>;\n\n  /**\n   * Optional batch size for validation.\n   *\n   * Used only if `validationData` is an array of `tf.Tensor` objects, i.e., not\n   * a dataset object.\n   *\n   * If not specified, its value defaults to 32.\n   */\n  validationBatchSize?: number;\n\n  /**\n   * (Optional) Only relevant if `validationData` is specified and is a dataset\n   * object.\n   *\n   * Total number of batches of samples to draw from `validationData` for\n   * validation purpose before stopping at the end of every epoch. If not\n   * specified, `evaluateDataset` will use `iterator.next().done` as signal to\n   * stop validation.\n   */\n  validationBatches?: number;\n\n  /**\n   * Configures the frequency of yielding the main thread to other tasks.\n   *\n   * In the browser environment, yielding the main thread can improve the\n   * responsiveness of the page during training. In the Node.js environment,\n   * it can ensure tasks queued in the event loop can be handled in a timely\n   * manner.\n   *\n   * The value can be one of the following:\n   *   - `'auto'`: The yielding happens at a certain frame rate (currently set\n   *               at 125ms). This is the default.\n   *   - `'batch'`: yield every batch.\n   *   - `'epoch'`: yield every epoch.\n   *   - a `number`: Will yield every `number` milliseconds.\n   *   - `'never'`: never yield. (But yielding can still happen through `await\n   *      nextFrame()` calls in custom callbacks.)\n   */\n  yieldEvery?: YieldEveryOptions;\n\n  /**\n   * Epoch at which to start training (useful for resuming a previous training\n   * run).\n   */\n  initialEpoch?: number;\n\n  /**\n   * Optional object mapping class indices (integers) to\n   * a weight (float) to apply to the model's loss for the samples from this\n   * class during training. This can be useful to tell the model to \"pay more\n   * attention\" to samples from an under-represented class.\n   *\n   * If the model has multiple outputs, a class weight can be specified for\n   * each of the outputs by setting this field an array of weight object\n   * or a object that maps model output names (e.g., `model.outputNames[0]`)\n   * to weight objects.\n   */\n  classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap;\n}\n\nexport interface FitDatasetElement {\n  xs: TensorOrArrayOrMap;\n  ys: TensorOrArrayOrMap;\n}\n\n/**\n * Interface for configuring model evaluation based on a dataset object.\n */\nexport interface ModelEvaluateDatasetArgs {\n  /**\n   * Number of batches to draw from the dataset object before ending the\n   * evaluation.\n   */\n  batches?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n}\n\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, iteratorOut: {}): {xs: tfc.Tensor[], ys: tfc.Tensor[]} {\n  let xs: TensorOrArrayOrMap;\n  let ys: TensorOrArrayOrMap;\n\n  const iteratorOutObj = iteratorOut as FitDatasetElement;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(\n      xs != null && ys != null,\n      () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n          'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n          'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n          'string to Tensor.  The provided Dataset instead generates ' +\n          `${iteratorOut}`);\n\n  const flattenedXs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n\n  const batchSize: number = flattenedXs[0].shape[0];\n\n  tfc.util.assert(\n      flattenedXs.length === model.inputs.length,\n      () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n          `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n          `${JSON.stringify(model.inputNames)})`);\n\n  tfc.util.assert(\n      flattenedYs.length === model.outputs.length,\n      () =>\n          `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n          `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n          `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(\n        flattenedXs[xIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${\n                  flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(\n        flattenedYs[yIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${\n                  flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {xs: flattenedXs, ys: flattenedYs};\n}\n\nfunction flattenTensorOrArrayOrMap(\n    inputOrOutput: string, names: string[], values: TensorOrArrayOrMap) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(\n        values.length === names.length,\n        () => `Received an array of ${values.length} Tensors, but expected ${\n            names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result: tfc.Tensor[] = [];\n    // Check that all the required keys are available.\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(\n            `The feature data generated by the dataset lacks the required ` +\n            `${inputOrOutput} key '${name}'.`);\n      }\n      result.push(values[name]);\n    }\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData<T>(\n    data:\n        [\n          tfc.Tensor|tfc.Tensor[], tfc.Tensor|tfc.Tensor[]\n        ]|[tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n           tfc.Tensor | tfc.Tensor[]]):\n    {xs: tfc.Tensor|tfc.Tensor[], ys: tfc.Tensor|tfc.Tensor[]} {\n  if (data.length === 3) {\n    throw new NotImplementedError(\n        'Validation with sample weights is not implemented yet.');\n  }\n  return {xs: data[0], ys: data[1]};\n}\n\nexport async function fitDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>,\n    args: ModelFitDatasetArgs<T>): Promise<History> {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(\n      model.optimizer != null,\n      () => 'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileConfig).');\n\n  tfc.util.assert(\n      args != null,\n      () => `For fitDataset(), the 2nd argument (config) is required, ` +\n          `but it is not provided in this call.`);\n  tfc.util.assert(\n      args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs),\n      () => `For fitDataset(), config.epochs is expected to be a positive ` +\n          `integer, but got ${args.epochs}`);\n  tfc.util.assert(\n      !hasBatchesPerEpoch ||\n          (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)),\n      () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n          `positive integer if specified, but got ${args.batchesPerEpoch}`);\n  tfc.util.assert(\n      // tslint:disable-next-line:no-any\n      (args as any)['validationSplit'] == null,\n      () => '`validationSplit` is not supported by `fitDataset()`. ' +\n          'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error(\n        'Cannot start training because another fit() call is ongoing.');\n  }\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs: tfc.Tensor|tfc.Tensor[];\n    let valYs: tfc.Tensor|tfc.Tensor[];\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(\n            args.validationBatches == null ||\n                (args.validationBatches > 0 &&\n                 Number.isInteger(args.validationBatches)),\n            () => `For fitDataset() with dataset-based validation, ` +\n                `config.validationBatches is expected not to be provided, ` +\n                `or to be a positive integer, ` +\n                `but got ${args.validationBatches}`);\n      } else {\n        const validationData = standardizeTensorValidationData(\n            args.validationData as\n                    [tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[]] |\n            [\n              tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n              tfc.Tensor | tfc.Tensor[]\n            ]);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames() as string[];\n\n    let callbackMetrics: string[];\n    if (doValidation) {\n      callbackMetrics =\n          outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, args.epochs, null, null,\n        getStepsPerEpoch(dataset, args),\n        null,  // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n\n    let dataIterator = await dataset.iterator();\n    while (epoch < args.epochs) {\n      const epochLogs: UnresolvedLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next();\n\n        // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn(\n              'You provided `batchesPerEpoch` as ' +\n              `${args.batchesPerEpoch}, ` +\n              'but your dataset iterator ran out of data after ' +\n              `${stepsDone} batches; ` +\n              'interrupting training. Make sure that your ' +\n              'dataset can generate at least `batchesPerEpoch * epochs` ' +\n              'batches (in this case, ' +\n              `${args.batchesPerEpoch * args.epochs} batches). ` +\n              'You may need to use the repeat() function when building ' +\n              'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const {xs, ys} =\n              standardizeDataIteratorOutput(model, iteratorOut.value);\n          const batchLogs: UnresolvedLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          const sampleWeights: tfc.Tensor[] = [];\n          if (args.classWeight != null) {\n            const standardClassWeights =\n                standardizeClassWeights(args.classWeight, model.outputNames);\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push(await standardizeWeights(\n                  ys[i], null, standardClassWeights[i]));\n            }\n          }\n\n          // Train on batch.\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                                 iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts: tfc.Scalar[];\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList(await model.evaluateDataset(\n                  args.validationData, {batches: args.validationBatches}));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ?\n                    DEFAULT_VALIDATION_BATCH_SIZE :\n                    args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n            }\n          }\n          // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch<T>(\n    dataset: Dataset<T>, args: ModelFitDatasetArgs<T>): number {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch: number = null;\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n  return stepsPerEpoch;\n}\n\n// Check if provided object is a Dataset object by checking it's .iterator\n// element.\nfunction isDatasetObject<T>(\n    dataset:\n        [\n          TensorOrArrayOrMap, TensorOrArrayOrMap\n        ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|\n    Dataset<T>): boolean {\n  return (typeof (dataset as Dataset<T>).iterator === 'function');\n}\n\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject<T>(iterator: Dataset<T>|\n                                 LazyIterator<T>): boolean {\n  return (typeof (iterator as LazyIterator<T>).next === 'function');\n}\n\nexport async function evaluateDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>|LazyIterator<T>,\n    args: ModelEvaluateDatasetArgs): Promise<tfc.Scalar|tfc.Scalar[]> {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs: tfc.Scalar[] = [];\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(\n      !hasBatches || (args.batches > 0 && Number.isInteger(args.batches)),\n      () => 'Test loop expects `batches` to be a positive integer, but ' +\n          `received ${JSON.stringify(args.batches)}`);\n  const dataIterator = isLazyIteratorObject(dataset) ?\n      dataset as LazyIterator<T>:\n      await (dataset as Dataset<T>).iterator();\n  // Keeps track of number of examples used in this evaluation.\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const {xs, ys} =\n            standardizeDataIteratorOutput(model, iteratorOut.value);\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] =\n              tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n\n        ++batch;\n      }\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn(\n            'Your dataset iterator ran out of data during evaluateDataset(). ' +\n            'Interrupting evalution. Make sure that your ' +\n            'dataset can generate at least `batches` ' +\n            `batches (in this case, ${args.batches} batches). ` +\n            'You may need to use the repeat() function when building ' +\n            'your dataset.');\n      }\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}\n"]}},"error":null,"hash":"a42354d2a81717ff5f91acfcfcae6c7e","cacheData":{"env":{}}}