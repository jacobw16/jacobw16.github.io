{"id":"node_modules/@tensorflow/tfjs-layers/dist/exports_layers.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\exports_layers.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581896610560},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"./engine/input_layer","loc":{"line":12,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\input_layer.js"},{"name":"./engine/topology","loc":{"line":13,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"./exports","loc":{"line":15,"column":24},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports.js"},{"name":"./layers/advanced_activations","loc":{"line":17,"column":37},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\advanced_activations.js"},{"name":"./layers/convolutional","loc":{"line":18,"column":30},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\convolutional.js"},{"name":"./layers/convolutional_depthwise","loc":{"line":19,"column":40},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\convolutional_depthwise.js"},{"name":"./layers/core","loc":{"line":20,"column":21},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\core.js"},{"name":"./layers/embeddings","loc":{"line":21,"column":27},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js"},{"name":"./layers/merge","loc":{"line":22,"column":22},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js"},{"name":"./layers/noise","loc":{"line":23,"column":22},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\noise.js"},{"name":"./layers/normalization","loc":{"line":24,"column":30},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js"},{"name":"./layers/padding","loc":{"line":25,"column":24},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\padding.js"},{"name":"./layers/pooling","loc":{"line":26,"column":24},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\pooling.js"},{"name":"./layers/recurrent","loc":{"line":27,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\recurrent.js"},{"name":"./layers/wrappers","loc":{"line":30,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\exports_layers.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\wrappers.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar input_layer_1 = require(\"./engine/input_layer\");\nvar topology_1 = require(\"./engine/topology\");\nexports.Layer = topology_1.Layer;\nvar exports_1 = require(\"./exports\");\nexports.input = exports_1.input;\nvar advanced_activations_1 = require(\"./layers/advanced_activations\");\nvar convolutional_1 = require(\"./layers/convolutional\");\nvar convolutional_depthwise_1 = require(\"./layers/convolutional_depthwise\");\nvar core_1 = require(\"./layers/core\");\nvar embeddings_1 = require(\"./layers/embeddings\");\nvar merge_1 = require(\"./layers/merge\");\nvar noise_1 = require(\"./layers/noise\");\nvar normalization_1 = require(\"./layers/normalization\");\nvar padding_1 = require(\"./layers/padding\");\nvar pooling_1 = require(\"./layers/pooling\");\nvar recurrent_1 = require(\"./layers/recurrent\");\nexports.RNN = recurrent_1.RNN;\nexports.RNNCell = recurrent_1.RNNCell;\nvar wrappers_1 = require(\"./layers/wrappers\");\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n// Input Layer.\n/**\n * An input layer is an entry point into a `tf.LayersModel`.\n *\n * `InputLayer` is generated automatically for `tf.Sequential`` models by\n * specifying the `inputshape` or `batchInputShape` for the first layer.  It\n * should not be specified explicitly. However, it can be useful sometimes,\n * e.g., when constructing a sequential model from a subset of another\n * sequential model's layers. Like the code snippet below shows.\n *\n * ```js\n * // Define a model which simply adds two inputs.\n * const model1 = tf.sequential();\n * model1.add(tf.layers.dense({inputShape: [4], units: 3, activation: 'relu'}));\n * model1.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\n * model1.summary();\n * model1.predict(tf.zeros([1, 4])).print();\n *\n * // Construct another model, reusing the second layer of `model1` while\n * // not using the first layer of `model1`. Note that you cannot add the second\n * // layer of `model` directly as the first layer of the new sequential model,\n * // because doing so will lead to an error related to the fact that the layer\n * // is not an input layer. Instead, you need to create an `inputLayer` and add\n * // it to the new sequential model before adding the reused layer.\n * const model2 = tf.sequential();\n * // Use an inputShape that matches the input shape of `model1`'s second\n * // layer.\n * model2.add(tf.layers.inputLayer({inputShape: [3]}));\n * model2.add(model1.layers[1]);\n * model2.summary();\n * model2.predict(tf.zeros([1, 3])).print();\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Inputs', namespace: 'layers'} */\nfunction inputLayer(args) {\n    return new input_layer_1.InputLayer(args);\n}\nexports.inputLayer = inputLayer;\n// Advanced Activation Layers.\n/**\n * Exponetial Linear Unit (ELU).\n *\n * It follows:\n * `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n * `f(x) = x for x >= 0`.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * References:\n *   - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n * (ELUs)](https://arxiv.org/abs/1511.07289v1)\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nfunction elu(args) {\n    return new advanced_activations_1.ELU(args);\n}\nexports.elu = elu;\n/**\n * Rectified Linear Unit activation function.\n *\n * Input shape:\n *   Arbitrary. Use the config field `inputShape` (Array of integers, does\n *   not include the sample axis) when using this layer as the first layer\n *   in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nfunction reLU(args) {\n    return new advanced_activations_1.ReLU(args);\n}\nexports.reLU = reLU;\n/**\n * Leaky version of a rectified linear unit.\n *\n * It allows a small gradient when the unit is not active:\n * `f(x) = alpha * x for x < 0.`\n * `f(x) = x for x >= 0.`\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nfunction leakyReLU(args) {\n    return new advanced_activations_1.LeakyReLU(args);\n}\nexports.leakyReLU = leakyReLU;\n/**\n * Parameterized version of a leaky rectified linear unit.\n *\n * It follows\n * `f(x) = alpha * x for x < 0.`\n * `f(x) = x for x >= 0.`\n * wherein `alpha` is a trainable weight.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nfunction prelu(args) {\n    return new advanced_activations_1.PReLU(args);\n}\nexports.prelu = prelu;\n/**\n * Softmax activation layer.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nfunction softmax(args) {\n    return new advanced_activations_1.Softmax(args);\n}\nexports.softmax = softmax;\n/**\n * Thresholded Rectified Linear Unit.\n *\n * It follows:\n * `f(x) = x for x > theta`,\n * `f(x) = 0 otherwise`.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * References:\n *   - [Zero-Bias Autoencoders and the Benefits of Co-Adapting\n * Features](http://arxiv.org/abs/1402.3337)\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nfunction thresholdedReLU(args) {\n    return new advanced_activations_1.ThresholdedReLU(args);\n}\nexports.thresholdedReLU = thresholdedReLU;\n// Convolutional Layers.\n/**\n * 1D convolution layer (e.g., temporal convolution).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input over a single spatial (or temporal) dimension\n * to produce a tensor of outputs.\n *\n * If `use_bias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model, provide an\n * `inputShape` argument `Array` or `null`.\n *\n * For example, `inputShape` would be:\n * - `[10, 128]` for sequences of 10 vectors of 128-dimensional vectors\n * - `[null, 128]` for variable-length sequences of 128-dimensional vectors.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional',  namespace: 'layers'}\n */\nfunction conv1d(args) {\n    return new convolutional_1.Conv1D(args);\n}\nexports.conv1d = conv1d;\n/**\n * 2D convolution layer (e.g. spatial convolution over images).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input to produce a tensor of outputs.\n *\n * If `useBias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model,\n * provide the keyword argument `inputShape`\n * (Array of integers, does not include the sample axis),\n * e.g. `inputShape=[128, 128, 3]` for 128x128 RGB pictures\n * in `dataFormat='channelsLast'`.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction conv2d(args) {\n    return new convolutional_1.Conv2D(args);\n}\nexports.conv2d = conv2d;\n/**\n * Transposed convolutional layer (sometimes called Deconvolution).\n *\n * The need for transposed convolutions generally arises\n * from the desire to use a transformation going in the opposite direction of\n * a normal convolution, i.e., from something that has the shape of the output\n * of some convolution to something that has the shape of its input while\n * maintaining a connectivity pattern that is compatible with said\n * convolution.\n *\n * When using this layer as the first layer in a model, provide the\n * configuration `inputShape` (`Array` of integers, does not include the\n * sample axis), e.g., `inputShape: [128, 128, 3]` for 128x128 RGB pictures in\n * `dataFormat: 'channelsLast'`.\n *\n * Input shape:\n *   4D tensor with shape:\n *   `[batch, channels, rows, cols]` if `dataFormat` is `'channelsFirst'`.\n *   or 4D tensor with shape\n *   `[batch, rows, cols, channels]` if `dataFormat` is `'channelsLast`.\n *\n * Output shape:\n *   4D tensor with shape:\n *   `[batch, filters, newRows, newCols]` if `dataFormat` is\n * `'channelsFirst'`. or 4D tensor with shape:\n *   `[batch, newRows, newCols, filters]` if `dataFormat` is `'channelsLast'`.\n *\n * References:\n *   - [A guide to convolution arithmetic for deep\n * learning](https://arxiv.org/abs/1603.07285v1)\n *   - [Deconvolutional\n * Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction conv2dTranspose(args) {\n    return new convolutional_1.Conv2DTranspose(args);\n}\nexports.conv2dTranspose = conv2dTranspose;\n/**\n * 3D convolution layer (e.g. spatial convolution over volumes).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input to produce a tensor of outputs.\n *\n * If `useBias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model,\n * provide the keyword argument `inputShape`\n * (Array of integers, does not include the sample axis),\n * e.g. `inputShape=[128, 128, 128, 1]` for 128x128x128 grayscale volumes\n * in `dataFormat='channelsLast'`.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction conv3d(args) {\n    return new convolutional_1.Conv3D(args);\n}\nexports.conv3d = conv3d;\n/**\n * Depthwise separable 2D convolution.\n *\n * Separable convolution consists of first performing\n * a depthwise spatial convolution\n * (which acts on each input channel separately)\n * followed by a pointwise convolution which mixes together the resulting\n * output channels. The `depthMultiplier` argument controls how many\n * output channels are generated per input channel in the depthwise step.\n *\n * Intuitively, separable convolutions can be understood as\n * a way to factorize a convolution kernel into two smaller kernels,\n * or as an extreme version of an Inception block.\n *\n * Input shape:\n *   4D tensor with shape:\n *     `[batch, channels, rows, cols]` if data_format='channelsFirst'\n *   or 4D tensor with shape:\n *     `[batch, rows, cols, channels]` if data_format='channelsLast'.\n *\n * Output shape:\n *   4D tensor with shape:\n *     `[batch, filters, newRows, newCols]` if data_format='channelsFirst'\n *   or 4D tensor with shape:\n *     `[batch, newRows, newCols, filters]` if data_format='channelsLast'.\n *     `rows` and `cols` values might have changed due to padding.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction separableConv2d(args) {\n    return new convolutional_1.SeparableConv2D(args);\n}\nexports.separableConv2d = separableConv2d;\n/**\n * Cropping layer for 2D input (e.g., image).\n *\n * This layer can crop an input\n * at the top, bottom, left and right side of an image tensor.\n *\n * Input shape:\n *   4D tensor with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, rows, cols, channels]`\n *   - If `data_format` is `\"channels_first\"`:\n *     `[batch, channels, rows, cols]`.\n *\n * Output shape:\n *   4D with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, croppedRows, croppedCols, channels]`\n *    - If `dataFormat` is `\"channelsFirst\"`:\n *     `[batch, channels, croppedRows, croppedCols]`.\n *\n * Examples\n * ```js\n *\n * const model = tf.sequential();\n * model.add(tf.layers.cropping2D({cropping:[[2, 2], [2, 2]],\n *                                inputShape: [128, 128, 3]}));\n * //now output shape is [batch, 124, 124, 3]\n * ```\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction cropping2D(args) {\n    return new convolutional_1.Cropping2D(args);\n}\nexports.cropping2D = cropping2D;\n/**\n * Upsampling layer for 2D inputs.\n *\n * Repeats the rows and columns of the data\n * by size[0] and size[1] respectively.\n *\n *\n * Input shape:\n *    4D tensor with shape:\n *     - If `dataFormat` is `\"channelsLast\"`:\n *         `[batch, rows, cols, channels]`\n *     - If `dataFormat` is `\"channelsFirst\"`:\n *        `[batch, channels, rows, cols]`\n *\n * Output shape:\n *     4D tensor with shape:\n *     - If `dataFormat` is `\"channelsLast\"`:\n *        `[batch, upsampledRows, upsampledCols, channels]`\n *     - If `dataFormat` is `\"channelsFirst\"`:\n *         `[batch, channels, upsampledRows, upsampledCols]`\n *\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction upSampling2d(args) {\n    return new convolutional_1.UpSampling2D(args);\n}\nexports.upSampling2d = upSampling2d;\n// Convolutional(depthwise) Layers.\n/**\n * Depthwise separable 2D convolution.\n *\n * Depthwise Separable convolutions consists in performing just the first step\n * in a depthwise spatial convolution (which acts on each input channel\n * separately). The `depthMultplier` argument controls how many output channels\n * are generated per input channel in the depthwise step.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nfunction depthwiseConv2d(args) {\n    return new convolutional_depthwise_1.DepthwiseConv2D(args);\n}\nexports.depthwiseConv2d = depthwiseConv2d;\n// Basic Layers.\n/**\n * Applies an activation function to an output.\n *\n * This layer applies element-wise activation function.  Other layers, notably\n * `dense` can also apply activation functions.  Use this isolated activation\n * function to extract the values before and after the\n * activation. For instance:\n *\n * ```js\n * const input = tf.input({shape: [5]});\n * const denseLayer = tf.layers.dense({units: 1});\n * const activationLayer = tf.layers.activation({activation: 'relu6'});\n *\n * // Obtain the output symbolic tensors by applying the layers in order.\n * const denseOutput = denseLayer.apply(input);\n * const activationOutput = activationLayer.apply(denseOutput);\n *\n * // Create the model based on the inputs.\n * const model = tf.model({\n *     inputs: input,\n *     outputs: [denseOutput, activationOutput]\n * });\n *\n * // Collect both outputs and print separately.\n * const [denseOut, activationOut] = model.predict(tf.randomNormal([6, 5]));\n * denseOut.print();\n * activationOut.print();\n * ```\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nfunction activation(args) {\n    return new core_1.Activation(args);\n}\nexports.activation = activation;\n/**\n * Creates a dense (fully connected) layer.\n *\n * This layer implements the operation:\n *   `output = activation(dot(input, kernel) + bias)`\n *\n * `activation` is the element-wise activation function\n *   passed as the `activation` argument.\n *\n * `kernel` is a weights matrix created by the layer.\n *\n * `bias` is a bias vector created by the layer (only applicable if `useBias`\n * is `true`).\n *\n * **Input shape:**\n *\n *   nD `tf.Tensor` with shape: `(batchSize, ..., inputDim)`.\n *\n *   The most common situation would be\n *   a 2D input with shape `(batchSize, inputDim)`.\n *\n * **Output shape:**\n *\n *   nD tensor with shape: `(batchSize, ..., units)`.\n *\n *   For instance, for a 2D input with shape `(batchSize, inputDim)`,\n *   the output would have shape `(batchSize, units)`.\n *\n * Note: if the input to the layer has a rank greater than 2, then it is\n * flattened prior to the initial dot product with the kernel.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction dense(args) {\n    return new core_1.Dense(args);\n}\nexports.dense = dense;\n/**\n * Applies\n * [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) to\n * the input.\n *\n * Dropout consists in randomly setting a fraction `rate` of input units to 0 at\n * each update during training time, which helps prevent overfitting.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction dropout(args) {\n    return new core_1.Dropout(args);\n}\nexports.dropout = dropout;\n/**\n * Spatial 1D version of Dropout.\n *\n * This Layer type performs the same function as the Dropout layer, but it drops\n * entire 1D feature maps instead of individual elements. For example, if an\n * input example consists of 3 timesteps and the feature map for each timestep\n * has a size of 4, a `spatialDropout1d` layer may zero out the feature maps\n * of the 1st timesteps and 2nd timesteps completely while sparing all feature\n * elements of the 3rd timestep.\n *\n * If adjacent frames (timesteps) are strongly correlated (as is normally the\n * case in early convolution layers), regular dropout will not regularize the\n * activation and will otherwise just result in merely an effective learning\n * rate decrease. In this case, `spatialDropout1d` will help promote\n * independence among feature maps and should be used instead.\n *\n * **Arguments:**\n *   rate: A floating-point number >=0 and <=1. Fraction of the input elements\n *     to drop.\n *\n * **Input shape:**\n *   3D tensor with shape `(samples, timesteps, channels)`.\n *\n * **Output shape:**\n *   Same as the input shape.\n *\n * References:\n *   - [Efficient Object Localization Using Convolutional\n *      Networks](https://arxiv.org/abs/1411.4280)\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction spatialDropout1d(args) {\n    return new core_1.SpatialDropout1D(args);\n}\nexports.spatialDropout1d = spatialDropout1d;\n/**\n * Flattens the input. Does not affect the batch size.\n *\n * A `Flatten` layer flattens each batch in its inputs to 1D (making the output\n * 2D).\n *\n * For example:\n *\n * ```js\n * const input = tf.input({shape: [4, 3]});\n * const flattenLayer = tf.layers.flatten();\n * // Inspect the inferred output shape of the flatten layer, which\n * // equals `[null, 12]`. The 2nd dimension is 4 * 3, i.e., the result of the\n * // flattening. (The 1st dimension is the undermined batch size.)\n * console.log(JSON.stringify(flattenLayer.apply(input).shape));\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction flatten(args) {\n    return new core_1.Flatten(args);\n}\nexports.flatten = flatten;\n/**\n * Repeats the input n times in a new dimension.\n *\n * ```js\n *  const model = tf.sequential();\n *  model.add(tf.layers.repeatVector({n: 4, inputShape: [2]}));\n *  const x = tf.tensor2d([[10, 20]]);\n *  // Use the model to do inference on a data point the model hasn't see\n *  model.predict(x).print();\n *  // output shape is now [batch, 2, 4]\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction repeatVector(args) {\n    return new core_1.RepeatVector(args);\n}\nexports.repeatVector = repeatVector;\n/**\n * Reshapes an input to a certain shape.\n *\n * ```js\n * const input = tf.input({shape: [4, 3]});\n * const reshapeLayer = tf.layers.reshape({targetShape: [2, 6]});\n * // Inspect the inferred output shape of the Reshape layer, which\n * // equals `[null, 2, 6]`. (The 1st dimension is the undermined batch size.)\n * console.log(JSON.stringify(reshapeLayer.apply(input).shape));\n * ```\n *\n * Input shape:\n *   Arbitrary, although all dimensions in the input shape must be fixed.\n *   Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n *\n * Output shape:\n *   [batchSize, targetShape[0], targetShape[1], ...,\n *    targetShape[targetShape.length - 1]].\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction reshape(args) {\n    return new core_1.Reshape(args);\n}\nexports.reshape = reshape;\n/**\n * Permutes the dimensions of the input according to a given pattern.\n *\n * Useful for, e.g., connecting RNNs and convnets together.\n *\n * Example:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.permute({\n *   dims: [2, 1],\n *   inputShape: [10, 64]\n * }));\n * console.log(model.outputShape);\n * // Now model's output shape is [null, 64, 10], where null is the\n * // unpermuted sample (batch) dimension.\n * ```\n *\n * Input shape:\n *   Arbitrary. Use the configuration field `inputShape` when using this\n *   layer as the first layer in a model.\n *\n * Output shape:\n *   Same rank as the input shape, but with the dimensions re-ordered (i.e.,\n *   permuted) according to the `dims` configuration of this layer.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction permute(args) {\n    return new core_1.Permute(args);\n}\nexports.permute = permute;\n/**\n * Maps positive integers (indices) into dense vectors of fixed size.\n * eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n *\n * **Input shape:** 2D tensor with shape: `[batchSize, sequenceLength]`.\n *\n * **Output shape:** 3D tensor with shape: `[batchSize, sequenceLength,\n * outputDim]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nfunction embedding(args) {\n    return new embeddings_1.Embedding(args);\n}\nexports.embedding = embedding;\n// Merge Layers.\n/**\n * Layer that performs element-wise addition on an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). The inputs are specified as an\n * `Array` when the `apply` method of the `Add` layer instance is called. For\n * example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const addLayer = tf.layers.add();\n * const sum = addLayer.apply([input1, input2]);\n * console.log(JSON.stringify(sum.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction add(args) {\n    return new merge_1.Add(args);\n}\nexports.add = add;\n/**\n * Layer that performs element-wise averaging on an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const averageLayer = tf.layers.average();\n * const average = averageLayer.apply([input1, input2]);\n * console.log(JSON.stringify(average.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction average(args) {\n    return new merge_1.Average(args);\n}\nexports.average = average;\n/**\n * Layer that concatenates an `Array` of inputs.\n *\n * It takes a list of tensors, all of the same shape except for the\n * concatenation axis, and returns a single tensor, the concatenation\n * of all inputs. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 3]});\n * const concatLayer = tf.layers.concatenate();\n * const output = concatLayer.apply([input1, input2]);\n * console.log(JSON.stringify(output.shape));\n * // You get [null, 2, 5], with the first dimension as the undetermined batch\n * // dimension. The last dimension (5) is the result of concatenating the\n * // last dimensions of the inputs (2 and 3).\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction concatenate(args) {\n    return new merge_1.Concatenate(args);\n}\nexports.concatenate = concatenate;\n/**\n * Layer that computes the element-wise maximum an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const maxLayer = tf.layers.maximum();\n * const max = maxLayer.apply([input1, input2]);\n * console.log(JSON.stringify(max.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction maximum(args) {\n    return new merge_1.Maximum(args);\n}\nexports.maximum = maximum;\n/**\n * Layer that computes the element-wise minimum of an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const minLayer = tf.layers.minimum();\n * const min = minLayer.apply([input1, input2]);\n * console.log(JSON.stringify(min.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction minimum(args) {\n    return new merge_1.Minimum(args);\n}\nexports.minimum = minimum;\n/**\n * Layer that multiplies (element-wise) an `Array` of inputs.\n *\n * It takes as input an Array of tensors, all of the same\n * shape, and returns a single tensor (also of the same shape).\n * For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const input3 = tf.input({shape: [2, 2]});\n * const multiplyLayer = tf.layers.multiply();\n * const product = multiplyLayer.apply([input1, input2, input3]);\n * console.log(product.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction multiply(args) {\n    return new merge_1.Multiply(args);\n}\nexports.multiply = multiply;\n/**\n * Layer that computes a dot product between samples in two tensors.\n *\n * E.g., if applied to a list of two tensors `a` and `b` both of shape\n * `[batchSize, n]`, the output will be a tensor of shape `[batchSize, 1]`,\n * where each entry at index `[i, 0]` will be the dot product between\n * `a[i, :]` and `b[i, :]`.\n *\n * Example:\n *\n * ```js\n * const dotLayer = tf.layers.dot({axes: -1});\n * const x1 = tf.tensor2d([[10, 20], [30, 40]]);\n * const x2 = tf.tensor2d([[-1, -2], [-3, -4]]);\n *\n * // Invoke the layer's apply() method in eager (imperative) mode.\n * const y = dotLayer.apply([x1, x2]);\n * y.print();\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nfunction dot(args) {\n    return new merge_1.Dot(args);\n}\nexports.dot = dot;\n// Normalization Layers.\n/**\n * Batch normalization layer (Ioffe and Szegedy, 2014).\n *\n * Normalize the activations of the previous layer at each batch,\n * i.e. applies a transformation that maintains the mean activation\n * close to 0 and the activation standard deviation close to 1.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape` (Array of integers, does\n *   not include the sample axis) when calling the constructor of this class,\n *   if this layer is used as a first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Batch Normalization: Accelerating Deep Network Training by Reducing\n * Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n */\nfunction batchNormalization(args) {\n    return new normalization_1.BatchNormalization(args);\n}\nexports.batchNormalization = batchNormalization;\n/**\n * Layer-normalization layer (Ba et al., 2016).\n *\n * Normalizes the activations of the previous layer for each given example in a\n * batch independently, instead of across a batch like in `batchNormalization`.\n * In other words, this layer applies a transformation that maintanis the mean\n * activation within each example close to0 and activation variance close to 1.\n *\n * Input shape:\n *   Arbitrary. Use the argument `inputShape` when using this layer as the first\n *   layer in a model.\n *\n * Output shape:\n *   Same as input.\n *\n * References:\n *   - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n */\nfunction layerNormalization(args) {\n    return new normalization_1.LayerNormalization(args);\n}\nexports.layerNormalization = layerNormalization;\n// Padding Layers.\n/**\n * Zero-padding layer for 2D input (e.g., image).\n *\n * This layer can add rows and columns of zeros\n * at the top, bottom, left and right side of an image tensor.\n *\n * Input shape:\n *   4D tensor with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, rows, cols, channels]`\n *   - If `data_format` is `\"channels_first\"`:\n *     `[batch, channels, rows, cols]`.\n *\n * Output shape:\n *   4D with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, paddedRows, paddedCols, channels]`\n *    - If `dataFormat` is `\"channelsFirst\"`:\n *     `[batch, channels, paddedRows, paddedCols]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Padding', namespace: 'layers'} */\nfunction zeroPadding2d(args) {\n    return new padding_1.ZeroPadding2D(args);\n}\nexports.zeroPadding2d = zeroPadding2d;\n// Pooling Layers.\n/**\n * Average pooling operation for spatial data.\n *\n * Input shape: `[batchSize, inLength, channels]`\n *\n * Output shape: `[batchSize, pooledLength, channels]`\n *\n * `tf.avgPool1d` is an alias.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction averagePooling1d(args) {\n    return new pooling_1.AveragePooling1D(args);\n}\nexports.averagePooling1d = averagePooling1d;\nfunction avgPool1d(args) {\n    return averagePooling1d(args);\n}\nexports.avgPool1d = avgPool1d;\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nfunction avgPooling1d(args) {\n    return averagePooling1d(args);\n}\nexports.avgPooling1d = avgPooling1d;\n/**\n * Average pooling operation for spatial data.\n *\n * Input shape:\n *  - If `dataFormat === CHANNEL_LAST`:\n *      4D tensor with shape:\n *      `[batchSize, rows, cols, channels]`\n *  - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *      `[batchSize, channels, rows, cols]`\n *\n * Output shape\n *  - If `dataFormat === CHANNEL_LAST`:\n *      4D tensor with shape:\n *      `[batchSize, pooleRows, pooledCols, channels]`\n *  - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *      `[batchSize, channels, pooleRows, pooledCols]`\n *\n * `tf.avgPool2d` is an alias.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction averagePooling2d(args) {\n    return new pooling_1.AveragePooling2D(args);\n}\nexports.averagePooling2d = averagePooling2d;\nfunction avgPool2d(args) {\n    return averagePooling2d(args);\n}\nexports.avgPool2d = avgPool2d;\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nfunction avgPooling2d(args) {\n    return averagePooling2d(args);\n}\nexports.avgPooling2d = avgPooling2d;\n/**\n * Average pooling operation for 3D data.\n *\n * Input shape\n *   - If `dataFormat === channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, depths, rows, cols, channels]`\n *   - If `dataFormat === channelsFirst`:\n *      4D tensor with shape:\n *       `[batchSize, channels, depths, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=channelsFirst`:\n *       5D tensor with shape:\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction averagePooling3d(args) {\n    return new pooling_1.AveragePooling3D(args);\n}\nexports.averagePooling3d = averagePooling3d;\nfunction avgPool3d(args) {\n    return averagePooling3d(args);\n}\nexports.avgPool3d = avgPool3d;\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nfunction avgPooling3d(args) {\n    return averagePooling3d(args);\n}\nexports.avgPooling3d = avgPooling3d;\n/**\n * Global average pooling operation for temporal data.\n *\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n *\n * Output Shape:2D tensor with shape: `[batchSize, features]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction globalAveragePooling1d(args) {\n    return new pooling_1.GlobalAveragePooling1D(args);\n}\nexports.globalAveragePooling1d = globalAveragePooling1d;\n/**\n * Global average pooling operation for spatial data.\n *\n * Input shape:\n *   - If `dataFormat` is `CHANNEL_LAST`:\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n *   - If `dataFormat` is `CHANNEL_FIRST`:\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n *\n * Output shape:\n *   2D tensor with shape: `[batchSize, channels]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction globalAveragePooling2d(args) {\n    return new pooling_1.GlobalAveragePooling2D(args);\n}\nexports.globalAveragePooling2d = globalAveragePooling2d;\n/**\n * Global max pooling operation for temporal data.\n *\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n *\n * Output Shape:2D tensor with shape: `[batchSize, features]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction globalMaxPooling1d(args) {\n    return new pooling_1.GlobalMaxPooling1D(args);\n}\nexports.globalMaxPooling1d = globalMaxPooling1d;\n/**\n * Global max pooling operation for spatial data.\n *\n * Input shape:\n *   - If `dataFormat` is `CHANNEL_LAST`:\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n *   - If `dataFormat` is `CHANNEL_FIRST`:\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n *\n * Output shape:\n *   2D tensor with shape: `[batchSize, channels]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction globalMaxPooling2d(args) {\n    return new pooling_1.GlobalMaxPooling2D(args);\n}\nexports.globalMaxPooling2d = globalMaxPooling2d;\n/**\n * Max pooling operation for temporal data.\n *\n * Input shape:  `[batchSize, inLength, channels]`\n *\n * Output shape: `[batchSize, pooledLength, channels]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction maxPooling1d(args) {\n    return new pooling_1.MaxPooling1D(args);\n}\nexports.maxPooling1d = maxPooling1d;\n/**\n * Max pooling operation for spatial data.\n *\n * Input shape\n *   - If `dataFormat === CHANNEL_LAST`:\n *       4D tensor with shape:\n *       `[batchSize, rows, cols, channels]`\n *   - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *       `[batchSize, channels, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=CHANNEL_LAST`:\n *       4D tensor with shape:\n *       `[batchSize, pooleRows, pooledCols, channels]`\n *   - If `dataFormat=CHANNEL_FIRST`:\n *       4D tensor with shape:\n *       `[batchSize, channels, pooleRows, pooledCols]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction maxPooling2d(args) {\n    return new pooling_1.MaxPooling2D(args);\n}\nexports.maxPooling2d = maxPooling2d;\n/**\n * Max pooling operation for 3D data.\n *\n * Input shape\n *   - If `dataFormat === channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, depths, rows, cols, channels]`\n *   - If `dataFormat === channelsFirst`:\n *      5D tensor with shape:\n *       `[batchSize, channels, depths, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=channelsFirst`:\n *       5D tensor with shape:\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nfunction maxPooling3d(args) {\n    return new pooling_1.MaxPooling3D(args);\n}\nexports.maxPooling3d = maxPooling3d;\n// Recurrent Layers.\n/**\n * Gated Recurrent Unit - Cho et al. 2014.\n *\n * This is an `RNN` layer consisting of one `GRUCell`. However, unlike\n * the underlying `GRUCell`, the `apply` method of `SimpleRNN` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const rnn = tf.layers.gru({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `GRUCell`'s number of units.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction gru(args) {\n    return new recurrent_1.GRU(args);\n}\nexports.gru = gru;\n/**\n * Cell class for `GRU`.\n *\n * `GRUCell` is distinct from the `RNN` subclass `GRU` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `GRU` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.gruCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `GRUCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.gruCell({units: 4}),\n *   tf.layers.gruCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `gruCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `GRUCell`, use the\n * `tf.layers.gru`.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction gruCell(args) {\n    return new recurrent_1.GRUCell(args);\n}\nexports.gruCell = gruCell;\n/**\n * Long-Short Term Memory layer - Hochreiter 1997.\n *\n * This is an `RNN` layer consisting of one `LSTMCell`. However, unlike\n * the underlying `LSTMCell`, the `apply` method of `LSTM` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const lstm = tf.layers.lstm({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = lstm.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `LSTMCell`'s number of units.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction lstm(args) {\n    return new recurrent_1.LSTM(args);\n}\nexports.lstm = lstm;\n/**\n * Cell class for `LSTM`.\n *\n * `LSTMCell` is distinct from the `RNN` subclass `LSTM` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `LSTM` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.lstmCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `LSTMCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.lstmCell({units: 4}),\n *   tf.layers.lstmCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `lstmCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `LSTMCell`, use the\n * `tf.layers.lstm`.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction lstmCell(args) {\n    return new recurrent_1.LSTMCell(args);\n}\nexports.lstmCell = lstmCell;\n/**\n * Fully-connected RNN where the output is to be fed back to input.\n *\n * This is an `RNN` layer consisting of one `SimpleRNNCell`. However, unlike\n * the underlying `SimpleRNNCell`, the `apply` method of `SimpleRNN` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const rnn = tf.layers.simpleRNN({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `SimpleRNNCell`'s number of units.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction simpleRNN(args) {\n    return new recurrent_1.SimpleRNN(args);\n}\nexports.simpleRNN = simpleRNN;\n/**\n * Cell class for `SimpleRNN`.\n *\n * `SimpleRNNCell` is distinct from the `RNN` subclass `SimpleRNN` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `SimpleRNN` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.simpleRNNCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `SimpleRNNCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.simpleRNNCell({units: 4}),\n *   tf.layers.simpleRNNCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `SimpleRNNCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `SimpleRNNCell`, use the\n * `tf.layers.simpleRNN`.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction simpleRNNCell(args) {\n    return new recurrent_1.SimpleRNNCell(args);\n}\nexports.simpleRNNCell = simpleRNNCell;\n/**\n * Base class for recurrent layers.\n *\n * Input shape:\n *   3D tensor with shape `[batchSize, timeSteps, inputDim]`.\n *\n * Output shape:\n *   - if `returnState`, an Array of tensors (i.e., `tf.Tensor`s). The first\n *     tensor is the output. The remaining tensors are the states at the\n *     last time step, each with shape `[batchSize, units]`.\n *   - if `returnSequences`, the output will have shape\n *     `[batchSize, timeSteps, units]`.\n *   - else, the output will have shape `[batchSize, units]`.\n *\n * Masking:\n *   This layer supports masking for input data with a variable number\n *   of timesteps. To introduce masks to your data,\n *   use an embedding layer with the `mask_zero` parameter\n *   set to `True`.\n *\n * Notes on using statefulness in RNNs:\n *   You can set RNN layers to be 'stateful', which means that the states\n *   computed for the samples in one batch will be reused as initial states\n *   for the samples in the next batch. This assumes a one-to-one mapping\n *   between samples in different successive batches.\n *\n *   To enable statefulness:\n *     - specify `stateful: true` in the layer constructor.\n *     - specify a fixed batch size for your model, by passing\n *       if sequential model:\n *         `batchInputShape=[...]` to the first layer in your model.\n *       else for functional model with 1 or more Input layers:\n *         `batchShape=[...]` to all the first layers in your model.\n *       This is the expected shape of your inputs *including the batch size*.\n *       It should be a tuple of integers, e.g. `(32, 10, 100)`.\n *     - specify `shuffle=False` when calling fit().\n *\n *   To reset the states of your model, call `.resetStates()` on either\n *   a specific layer, or on your entire model.\n *\n * Note on specifying the initial state of RNNs\n *   You can specify the initial state of RNN layers symbolically by\n *   calling them with the option `initialState`. The value of\n *   `initialState` should be a tensor or list of tensors representing\n *   the initial state of the RNN layer.\n *\n *   You can specify the initial state of RNN layers numerically by\n *   calling `resetStates` with the keyword argument `states`. The value of\n *   `states` should be a numpy array or list of numpy arrays representing\n *   the initial state of the RNN layer.\n *\n * Note on passing external constants to RNNs\n *   You can pass \"external\" constants to the cell using the `constants`\n *   keyword argument of `RNN.call` method. This requires that the `cell.call`\n *   method accepts the same keyword argument `constants`. Such constants\n *   can be used to conditon the cell transformation on additional static inputs\n *   (not changing over time), a.k.a an attention mechanism.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction rnn(args) {\n    return new recurrent_1.RNN(args);\n}\nexports.rnn = rnn;\n/**\n * Wrapper allowing a stack of RNN cells to behave as a single cell.\n *\n * Used to implement efficient stacked RNNs.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nfunction stackedRNNCells(args) {\n    return new recurrent_1.StackedRNNCells(args);\n}\nexports.stackedRNNCells = stackedRNNCells;\n// Wrapper Layers.\n/** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\nfunction bidirectional(args) {\n    return new wrappers_1.Bidirectional(args);\n}\nexports.bidirectional = bidirectional;\n/**\n * This wrapper applies a layer to every temporal slice of an input.\n *\n * The input should be at least 3D,  and the dimension of the index `1` will be\n * considered to be the temporal dimension.\n *\n * Consider a batch of 32 samples, where each sample is a sequence of 10 vectors\n * of 16 dimensions. The batch input shape of the layer is then `[32,  10,\n * 16]`, and the `inputShape`, not including the sample dimension, is\n * `[10, 16]`.\n *\n * You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10\n * timesteps, independently:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.timeDistributed({\n *   layer: tf.layers.dense({units: 8}),\n *   inputShape: [10, 16],\n * }));\n *\n * // Now model.outputShape = [null, 10, 8].\n * // The output will then have shape `[32, 10, 8]`.\n *\n * // In subsequent layers, there is no need for `inputShape`:\n * model.add(tf.layers.timeDistributed({layer: tf.layers.dense({units: 32})}));\n * console.log(JSON.stringify(model.outputs[0].shape));\n * // Now model.outputShape = [null, 10, 32].\n * ```\n *\n * The output will then have shape `[32, 10, 32]`.\n *\n * `TimeDistributed` can be used with arbitrary layers, not just `Dense`, for\n * instance a `Conv2D` layer.\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.timeDistributed({\n *   layer: tf.layers.conv2d({filters: 64, kernelSize: [3, 3]}),\n *   inputShape: [10, 299, 299, 3],\n * }));\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\nfunction timeDistributed(args) {\n    return new wrappers_1.TimeDistributed(args);\n}\nexports.timeDistributed = timeDistributed;\n// Aliases for pooling.\nexports.globalMaxPool1d = globalMaxPooling1d;\nexports.globalMaxPool2d = globalMaxPooling2d;\nexports.maxPool1d = maxPooling1d;\nexports.maxPool2d = maxPooling2d;\n/**\n * Apply additive zero-centered Gaussian noise.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * This is useful to mitigate overfitting\n * (you could see it as a form of random data augmentation).\n * Gaussian Noise (GS) is a natural choice as corruption process\n * for real valued inputs.\n *\n * # Arguments\n *     stddev: float, standard deviation of the noise distribution.\n *\n * # Input shape\n *         Arbitrary. Use the keyword argument `input_shape`\n *         (tuple of integers, does not include the samples axis)\n *         when using this layer as the first layer in a model.\n *\n * # Output shape\n *         Same shape as input.\n */\n/** @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'} */\nfunction gaussianNoise(args) {\n    return new noise_1.GaussianNoise(args);\n}\nexports.gaussianNoise = gaussianNoise;\n/**\n * Apply multiplicative 1-centered Gaussian noise.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n *      http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n *\n */\n/** @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'} */\nfunction gaussianDropout(args) {\n    return new noise_1.GaussianDropout(args);\n}\nexports.gaussianDropout = gaussianDropout;\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\n/** @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'} */\nfunction alphaDropout(args) {\n    return new noise_1.AlphaDropout(args);\n}\nexports.alphaDropout = alphaDropout;\n/**\n * Masks a sequence by using a mask value to skip timesteps.\n *\n * If all features for a given sample timestep are equal to `mask_value`,\n * then the sample timestep will be masked (skipped) in all downstream layers\n * (as long as they support masking).\n *\n * If any downstream layer does not support masking yet receives such\n * an input mask, an exception will be raised.\n *\n * Arguments:\n *   - `maskValue`: Either None or mask value to skip.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n */\n/** @doc {heading: 'Layers', subheading: 'Mask', namespace: 'layers'} */\nfunction masking(args) {\n    return new core_1.Masking(args);\n}\nexports.masking = masking;\n"},"sourceMaps":{"js":{"version":3,"file":"exports_layers.js","sourceRoot":"","sources":["../src/exports_layers.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;AAEH,oDAAgE;AAChE,8CAAmD;AAs8C3C,gBAt8CA,gBAAK,CAs8CA;AAr8Cb,qCAAgC;AAq8CH,gBAr8CrB,eAAK,CAq8CqB;AAp8ClC,sEAAiN;AACjN,wDAA6M;AAC7M,4EAA2F;AAC3F,sCAA2S;AAC3S,kDAAkE;AAClE,wCAA8H;AAC9H,wCAAsI;AACtI,wDAAwI;AACxI,4CAAuE;AACvE,4CAA8S;AAC9S,gDAA0Q;AA07C3P,cA17CyF,eAAG,CA07CzF;AAAE,kBA17CyF,mBAAO,CA07CzF;AAz7C3B,8CAA2G;AAE3G,wEAAwE;AACxE,wEAAwE;AACxE,kBAAkB;AAElB,eAAe;AACf;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA+BG;AACH,0EAA0E;AAC1E,SAAgB,UAAU,CAAC,IAAoB;IAC7C,OAAO,IAAI,wBAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AAFD,gCAEC;AAED,8BAA8B;AAE9B;;;;;;;;;;;;;;;;;GAiBG;AACH;;;;;;GAMG;AACH,SAAgB,GAAG,CAAC,IAAmB;IACrC,OAAO,IAAI,0BAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAFD,kBAEC;AAED;;;;;;;;;;GAUG;AACH;;;;;;GAMG;AACH,SAAgB,IAAI,CAAC,IAAoB;IACvC,OAAO,IAAI,2BAAI,CAAC,IAAI,CAAC,CAAC;AACxB,CAAC;AAFD,oBAEC;AAED;;;;;;;;;;;;;GAaG;AACH;;;;;;GAMG;AACH,SAAgB,SAAS,CAAC,IAAyB;IACjD,OAAO,IAAI,gCAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAFD,8BAEC;AAED;;;;;;;;;;;;;;GAcG;AACH;;;;;;GAMG;AACH,SAAgB,KAAK,CAAC,IAAqB;IACzC,OAAO,IAAI,4BAAK,CAAC,IAAI,CAAC,CAAC;AACzB,CAAC;AAFD,sBAEC;AAED;;;;;;;;;GASG;AACH;;;;;;GAMG;AACH,SAAgB,OAAO,CAAC,IAAuB;IAC7C,OAAO,IAAI,8BAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;;;GAiBG;AACH;;;;;;GAMG;AACH,SAAgB,eAAe,CAAC,IAA+B;IAC7D,OAAO,IAAI,sCAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED,wBAAwB;AAExB;;;;;;;;;;;;;;;;;GAiBG;AACH;;GAEG;AACH,SAAgB,MAAM,CAAC,IAAmB;IACxC,OAAO,IAAI,sBAAM,CAAC,IAAI,CAAC,CAAC;AAC1B,CAAC;AAFD,wBAEC;AAED;;;;;;;;;;;;;;;GAeG;AACH;;GAEG;AACH,SAAgB,MAAM,CAAC,IAAmB;IACxC,OAAO,IAAI,sBAAM,CAAC,IAAI,CAAC,CAAC;AAC1B,CAAC;AAFD,wBAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAgCG;AACH;;GAEG;AACH,SAAgB,eAAe,CAAC,IAAmB;IACjD,OAAO,IAAI,+BAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED;;;;;;;;;;;;;;;GAeG;AACH;;GAEG;AACH,SAAgB,MAAM,CAAC,IAAmB;IACxC,OAAO,IAAI,sBAAM,CAAC,IAAI,CAAC,CAAC;AAC1B,CAAC;AAFD,wBAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;GA0BG;AACH;;GAEG;AACH,SAAgB,eAAe,CAAC,IAA4B;IAC1D,OAAO,IAAI,+BAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA4BG;AACH;;GAEG;AACH,SAAgB,UAAU,CAAC,IAAyB;IAClD,OAAO,IAAI,0BAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AAFD,gCAEC;AAED;;;;;;;;;;;;;;;;;;;;;GAqBG;AACH;;GAEG;AACH,SAAgB,YAAY,CAAC,IAA2B;IACtD,OAAO,IAAI,4BAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED,mCAAmC;AAEnC;;;;;;;GAOG;AACH;;GAEG;AACH,SAAgB,eAAe,CAAC,IAA8B;IAC5D,OAAO,IAAI,yCAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED,gBAAgB;AAEhB;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA4BG;AACH;;GAEG;AACH,SAAgB,UAAU,CAAC,IAAyB;IAClD,OAAO,IAAI,iBAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AAFD,gCAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA8BG;AACH,yEAAyE;AACzE,SAAgB,KAAK,CAAC,IAAoB;IACxC,OAAO,IAAI,YAAK,CAAC,IAAI,CAAC,CAAC;AACzB,CAAC;AAFD,sBAEC;AAED;;;;;;;GAOG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAsB;IAC5C,OAAO,IAAI,cAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6BG;AACH,yEAAyE;AACzE,SAAgB,gBAAgB,CAAC,IAAiC;IAChE,OAAO,IAAI,uBAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AAFD,4CAEC;AAED;;;;;;;;;;;;;;;;GAgBG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAuB;IAC7C,OAAO,IAAI,cAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;GAWG;AACH,yEAAyE;AACzE,SAAgB,YAAY,CAAC,IAA2B;IACtD,OAAO,IAAI,mBAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAsB;IAC5C,OAAO,IAAI,cAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;GAyBG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAsB;IAC5C,OAAO,IAAI,cAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;GAQG;AACH,yEAAyE;AACzE,SAAgB,SAAS,CAAC,IAAwB;IAChD,OAAO,IAAI,sBAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAFD,8BAEC;AAED,gBAAgB;AAEhB;;;;;;;;;;;;;;;;;GAiBG;AACH,yEAAyE;AACzE,SAAgB,GAAG,CAAC,IAAgB;IAClC,OAAO,IAAI,WAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAFD,kBAEC;AAED;;;;;;;;;;;;;;;GAeG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAgB;IACtC,OAAO,IAAI,eAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;;;GAiBG;AACH,yEAAyE;AACzE,SAAgB,WAAW,CAAC,IAA2B;IACrD,OAAO,IAAI,mBAAW,CAAC,IAAI,CAAC,CAAC;AAC/B,CAAC;AAFD,kCAEC;AAED;;;;;;;;;;;;;;;GAeG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAgB;IACtC,OAAO,IAAI,eAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;GAeG;AACH,yEAAyE;AACzE,SAAgB,OAAO,CAAC,IAAgB;IACtC,OAAO,IAAI,eAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;;GAgBG;AACH,yEAAyE;AACzE,SAAgB,QAAQ,CAAC,IAAgB;IACvC,OAAO,IAAI,gBAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAFD,4BAEC;AAED;;;;;;;;;;;;;;;;;;;GAmBG;AACH,yEAAyE;AACzE,SAAgB,GAAG,CAAC,IAAkB;IACpC,OAAO,IAAI,WAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAFD,kBAEC;AAED,wBAAwB;AAExB;;;;;;;;;;;;;;;;;;GAkBG;AACH;;GAEG;AACH,SAAgB,kBAAkB,CAAC,IAAkC;IACnE,OAAO,IAAI,kCAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAFD,gDAEC;AAED;;;;;;;;;;;;;;;;;GAiBG;AACH;;GAEG;AACH,SAAgB,kBAAkB,CAAC,IAAkC;IACnE,OAAO,IAAI,kCAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAFD,gDAEC;AAED,kBAAkB;AAElB;;;;;;;;;;;;;;;;;;;GAmBG;AACH,2EAA2E;AAC3E,SAAgB,aAAa,CAAC,IAA6B;IACzD,OAAO,IAAI,uBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAFD,sCAEC;AAED,kBAAkB;AAElB;;;;;;;;GAQG;AACH,2EAA2E;AAC3E,SAAgB,gBAAgB,CAAC,IAAwB;IACvD,OAAO,IAAI,0BAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AAFD,4CAEC;AACD,SAAgB,SAAS,CAAC,IAAwB;IAChD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,8BAEC;AACD,+BAA+B;AAC/B,oDAAoD;AACpD,SAAgB,YAAY,CAAC,IAAwB;IACnD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,2EAA2E;AAC3E,SAAgB,gBAAgB,CAAC,IAAwB;IACvD,OAAO,IAAI,0BAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AAFD,4CAEC;AACD,SAAgB,SAAS,CAAC,IAAwB;IAChD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,8BAEC;AACD,+BAA+B;AAC/B,oDAAoD;AACpD,SAAgB,YAAY,CAAC,IAAwB;IACnD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;;;;;;;;;;;;;GAkBG;AACH,2EAA2E;AAC3E,SAAgB,gBAAgB,CAAC,IAAwB;IACvD,OAAO,IAAI,0BAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AAFD,4CAEC;AACD,SAAgB,SAAS,CAAC,IAAwB;IAChD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,8BAEC;AACD,+BAA+B;AAC/B,oDAAoD;AACpD,SAAgB,YAAY,CAAC,IAAwB;IACnD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;GAMG;AACH,2EAA2E;AAC3E,SAAgB,sBAAsB,CAAC,IAAgB;IACrD,OAAO,IAAI,gCAAsB,CAAC,IAAI,CAAC,CAAC;AAC1C,CAAC;AAFD,wDAEC;AAED;;;;;;;;;;;GAWG;AACH,2EAA2E;AAC3E,SAAgB,sBAAsB,CAAC,IAA8B;IACnE,OAAO,IAAI,gCAAsB,CAAC,IAAI,CAAC,CAAC;AAC1C,CAAC;AAFD,wDAEC;AAED;;;;;;GAMG;AACH,2EAA2E;AAC3E,SAAgB,kBAAkB,CAAC,IAAgB;IACjD,OAAO,IAAI,4BAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAFD,gDAEC;AAED;;;;;;;;;;;GAWG;AACH,2EAA2E;AAC3E,SAAgB,kBAAkB,CAAC,IAA8B;IAC/D,OAAO,IAAI,4BAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAFD,gDAEC;AAED;;;;;;GAMG;AACH,2EAA2E;AAC3E,SAAgB,YAAY,CAAC,IAAwB;IACnD,OAAO,IAAI,sBAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;;;;;;;;;;;;;GAkBG;AACH,2EAA2E;AAC3E,SAAgB,YAAY,CAAC,IAAwB;IACnD,OAAO,IAAI,sBAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;;;;;;;;;;;;;GAkBG;AACH,2EAA2E;AAC3E,SAAgB,YAAY,CAAC,IAAwB;IACnD,OAAO,IAAI,sBAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED,oBAAoB;AAEpB;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,6EAA6E;AAC7E,SAAgB,GAAG,CAAC,IAAkB;IACpC,OAAO,IAAI,eAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAFD,kBAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA0CG;AACH,6EAA6E;AAC7E,SAAgB,OAAO,CAAC,IAAsB;IAC5C,OAAO,IAAI,mBAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC;AAED;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,6EAA6E;AAC7E,SAAgB,IAAI,CAAC,IAAmB;IACtC,OAAO,IAAI,gBAAI,CAAC,IAAI,CAAC,CAAC;AACxB,CAAC;AAFD,oBAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA0CG;AACH,6EAA6E;AAC7E,SAAgB,QAAQ,CAAC,IAAuB;IAC9C,OAAO,IAAI,oBAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAFD,4BAEC;AAED;;;;;;;;;;;;;;;;;;;;;GAqBG;AACH,6EAA6E;AAC7E,SAAgB,SAAS,CAAC,IAAwB;IAChD,OAAO,IAAI,qBAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAFD,8BAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA0CG;AACH,6EAA6E;AAC7E,SAAgB,aAAa,CAAC,IAA4B;IACxD,OAAO,IAAI,yBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAFD,sCAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAyDG;AACH,6EAA6E;AAC7E,SAAgB,GAAG,CAAC,IAAkB;IACpC,OAAO,IAAI,eAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAFD,kBAEC;AAED;;;;GAIG;AACH,6EAA6E;AAC7E,SAAgB,eAAe,CAAC,IAAyB;IACvD,OAAO,IAAI,2BAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED,kBAAkB;AAElB,2EAA2E;AAC3E,SAAgB,aAAa,CAAC,IAA4B;IACxD,OAAO,IAAI,wBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAFD,sCAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA2CG;AACH,2EAA2E;AAC3E,SAAgB,eAAe,CAAC,IAAsB;IACpD,OAAO,IAAI,0BAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED,uBAAuB;AACV,QAAA,eAAe,GAAG,kBAAkB,CAAC;AACrC,QAAA,eAAe,GAAG,kBAAkB,CAAC;AACrC,QAAA,SAAS,GAAG,YAAY,CAAC;AACzB,QAAA,SAAS,GAAG,YAAY,CAAC;AAItC;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,yEAAyE;AACzE,SAAgB,aAAa,CAAC,IAAuB;IACnD,OAAO,IAAI,qBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAFD,sCAEC;AAED;;;;;;;;;;;;;;;;;;;;;;GAsBG;AACH,yEAAyE;AACzE,SAAgB,eAAe,CAAC,IAAyB;IACvD,OAAO,IAAI,uBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAFD,0CAEC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA4BG;AACH,yEAAyE;AACzE,SAAgB,YAAY,CAAC,IAAsB;IACjD,OAAO,IAAI,oBAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAFD,oCAEC;AAED;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,wEAAwE;AACxE,SAAgB,OAAO,CAAC,IAAkB;IACxC,OAAO,IAAI,cAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAFD,0BAEC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {InputLayer, InputLayerArgs} from './engine/input_layer';\nimport {Layer, LayerArgs} from './engine/topology';\nimport {input} from './exports';\nimport {ELU, ELULayerArgs, LeakyReLU, LeakyReLULayerArgs, PReLU, PReLULayerArgs, ReLU, ReLULayerArgs, Softmax, SoftmaxLayerArgs, ThresholdedReLU, ThresholdedReLULayerArgs} from './layers/advanced_activations';\nimport {Conv1D, Conv2D, Conv2DTranspose, Conv3D, ConvLayerArgs, Cropping2D, Cropping2DLayerArgs, SeparableConv2D, SeparableConvLayerArgs, UpSampling2D, UpSampling2DLayerArgs} from './layers/convolutional';\nimport {DepthwiseConv2D, DepthwiseConv2DLayerArgs} from './layers/convolutional_depthwise';\nimport {Activation, ActivationLayerArgs, Dense, DenseLayerArgs, Dropout, DropoutLayerArgs, Flatten, FlattenLayerArgs, Masking, MaskingArgs, Permute, PermuteLayerArgs, RepeatVector, RepeatVectorLayerArgs, Reshape, ReshapeLayerArgs, SpatialDropout1D, SpatialDropout1DLayerConfig} from './layers/core';\nimport {Embedding, EmbeddingLayerArgs} from './layers/embeddings';\nimport {Add, Average, Concatenate, ConcatenateLayerArgs, Dot, DotLayerArgs, Maximum, Minimum, Multiply} from './layers/merge';\nimport {AlphaDropout, AlphaDropoutArgs, GaussianDropout, GaussianDropoutArgs, GaussianNoise, GaussianNoiseArgs} from './layers/noise';\nimport {BatchNormalization, BatchNormalizationLayerArgs, LayerNormalization, LayerNormalizationLayerArgs} from './layers/normalization';\nimport {ZeroPadding2D, ZeroPadding2DLayerArgs} from './layers/padding';\nimport {AveragePooling1D, AveragePooling2D, AveragePooling3D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalPooling2DLayerArgs, MaxPooling1D, MaxPooling2D, MaxPooling3D, Pooling1DLayerArgs, Pooling2DLayerArgs, Pooling3DLayerArgs} from './layers/pooling';\nimport {GRU, GRUCell, GRUCellLayerArgs, GRULayerArgs, LSTM, LSTMCell, LSTMCellLayerArgs, LSTMLayerArgs, RNN, RNNCell, RNNLayerArgs, SimpleRNN, SimpleRNNCell, SimpleRNNCellLayerArgs, SimpleRNNLayerArgs, StackedRNNCells, StackedRNNCellsArgs} from './layers/recurrent';\nimport {Bidirectional, BidirectionalLayerArgs, TimeDistributed, WrapperLayerArgs} from './layers/wrappers';\n\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n\n// Input Layer.\n/**\n * An input layer is an entry point into a `tf.LayersModel`.\n *\n * `InputLayer` is generated automatically for `tf.Sequential`` models by\n * specifying the `inputshape` or `batchInputShape` for the first layer.  It\n * should not be specified explicitly. However, it can be useful sometimes,\n * e.g., when constructing a sequential model from a subset of another\n * sequential model's layers. Like the code snippet below shows.\n *\n * ```js\n * // Define a model which simply adds two inputs.\n * const model1 = tf.sequential();\n * model1.add(tf.layers.dense({inputShape: [4], units: 3, activation: 'relu'}));\n * model1.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\n * model1.summary();\n * model1.predict(tf.zeros([1, 4])).print();\n *\n * // Construct another model, reusing the second layer of `model1` while\n * // not using the first layer of `model1`. Note that you cannot add the second\n * // layer of `model` directly as the first layer of the new sequential model,\n * // because doing so will lead to an error related to the fact that the layer\n * // is not an input layer. Instead, you need to create an `inputLayer` and add\n * // it to the new sequential model before adding the reused layer.\n * const model2 = tf.sequential();\n * // Use an inputShape that matches the input shape of `model1`'s second\n * // layer.\n * model2.add(tf.layers.inputLayer({inputShape: [3]}));\n * model2.add(model1.layers[1]);\n * model2.summary();\n * model2.predict(tf.zeros([1, 3])).print();\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Inputs', namespace: 'layers'} */\nexport function inputLayer(args: InputLayerArgs): Layer {\n  return new InputLayer(args);\n}\n\n// Advanced Activation Layers.\n\n/**\n * Exponetial Linear Unit (ELU).\n *\n * It follows:\n * `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n * `f(x) = x for x >= 0`.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * References:\n *   - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n * (ELUs)](https://arxiv.org/abs/1511.07289v1)\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function elu(args?: ELULayerArgs): Layer {\n  return new ELU(args);\n}\n\n/**\n * Rectified Linear Unit activation function.\n *\n * Input shape:\n *   Arbitrary. Use the config field `inputShape` (Array of integers, does\n *   not include the sample axis) when using this layer as the first layer\n *   in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function reLU(args?: ReLULayerArgs): Layer {\n  return new ReLU(args);\n}\n\n/**\n * Leaky version of a rectified linear unit.\n *\n * It allows a small gradient when the unit is not active:\n * `f(x) = alpha * x for x < 0.`\n * `f(x) = x for x >= 0.`\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function leakyReLU(args?: LeakyReLULayerArgs): Layer {\n  return new LeakyReLU(args);\n}\n\n/**\n * Parameterized version of a leaky rectified linear unit.\n *\n * It follows\n * `f(x) = alpha * x for x < 0.`\n * `f(x) = x for x >= 0.`\n * wherein `alpha` is a trainable weight.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function prelu(args?: PReLULayerArgs): Layer {\n  return new PReLU(args);\n}\n\n/**\n * Softmax activation layer.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function softmax(args?: SoftmaxLayerArgs): Layer {\n  return new Softmax(args);\n}\n\n/**\n * Thresholded Rectified Linear Unit.\n *\n * It follows:\n * `f(x) = x for x > theta`,\n * `f(x) = 0 otherwise`.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * References:\n *   - [Zero-Bias Autoencoders and the Benefits of Co-Adapting\n * Features](http://arxiv.org/abs/1402.3337)\n */\n/**\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function thresholdedReLU(args?: ThresholdedReLULayerArgs): Layer {\n  return new ThresholdedReLU(args);\n}\n\n// Convolutional Layers.\n\n/**\n * 1D convolution layer (e.g., temporal convolution).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input over a single spatial (or temporal) dimension\n * to produce a tensor of outputs.\n *\n * If `use_bias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model, provide an\n * `inputShape` argument `Array` or `null`.\n *\n * For example, `inputShape` would be:\n * - `[10, 128]` for sequences of 10 vectors of 128-dimensional vectors\n * - `[null, 128]` for variable-length sequences of 128-dimensional vectors.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional',  namespace: 'layers'}\n */\nexport function conv1d(args: ConvLayerArgs): Layer {\n  return new Conv1D(args);\n}\n\n/**\n * 2D convolution layer (e.g. spatial convolution over images).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input to produce a tensor of outputs.\n *\n * If `useBias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model,\n * provide the keyword argument `inputShape`\n * (Array of integers, does not include the sample axis),\n * e.g. `inputShape=[128, 128, 3]` for 128x128 RGB pictures\n * in `dataFormat='channelsLast'`.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function conv2d(args: ConvLayerArgs): Layer {\n  return new Conv2D(args);\n}\n\n/**\n * Transposed convolutional layer (sometimes called Deconvolution).\n *\n * The need for transposed convolutions generally arises\n * from the desire to use a transformation going in the opposite direction of\n * a normal convolution, i.e., from something that has the shape of the output\n * of some convolution to something that has the shape of its input while\n * maintaining a connectivity pattern that is compatible with said\n * convolution.\n *\n * When using this layer as the first layer in a model, provide the\n * configuration `inputShape` (`Array` of integers, does not include the\n * sample axis), e.g., `inputShape: [128, 128, 3]` for 128x128 RGB pictures in\n * `dataFormat: 'channelsLast'`.\n *\n * Input shape:\n *   4D tensor with shape:\n *   `[batch, channels, rows, cols]` if `dataFormat` is `'channelsFirst'`.\n *   or 4D tensor with shape\n *   `[batch, rows, cols, channels]` if `dataFormat` is `'channelsLast`.\n *\n * Output shape:\n *   4D tensor with shape:\n *   `[batch, filters, newRows, newCols]` if `dataFormat` is\n * `'channelsFirst'`. or 4D tensor with shape:\n *   `[batch, newRows, newCols, filters]` if `dataFormat` is `'channelsLast'`.\n *\n * References:\n *   - [A guide to convolution arithmetic for deep\n * learning](https://arxiv.org/abs/1603.07285v1)\n *   - [Deconvolutional\n * Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function conv2dTranspose(args: ConvLayerArgs): Layer {\n  return new Conv2DTranspose(args);\n}\n\n/**\n * 3D convolution layer (e.g. spatial convolution over volumes).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input to produce a tensor of outputs.\n *\n * If `useBias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model,\n * provide the keyword argument `inputShape`\n * (Array of integers, does not include the sample axis),\n * e.g. `inputShape=[128, 128, 128, 1]` for 128x128x128 grayscale volumes\n * in `dataFormat='channelsLast'`.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function conv3d(args: ConvLayerArgs): Layer {\n  return new Conv3D(args);\n}\n\n/**\n * Depthwise separable 2D convolution.\n *\n * Separable convolution consists of first performing\n * a depthwise spatial convolution\n * (which acts on each input channel separately)\n * followed by a pointwise convolution which mixes together the resulting\n * output channels. The `depthMultiplier` argument controls how many\n * output channels are generated per input channel in the depthwise step.\n *\n * Intuitively, separable convolutions can be understood as\n * a way to factorize a convolution kernel into two smaller kernels,\n * or as an extreme version of an Inception block.\n *\n * Input shape:\n *   4D tensor with shape:\n *     `[batch, channels, rows, cols]` if data_format='channelsFirst'\n *   or 4D tensor with shape:\n *     `[batch, rows, cols, channels]` if data_format='channelsLast'.\n *\n * Output shape:\n *   4D tensor with shape:\n *     `[batch, filters, newRows, newCols]` if data_format='channelsFirst'\n *   or 4D tensor with shape:\n *     `[batch, newRows, newCols, filters]` if data_format='channelsLast'.\n *     `rows` and `cols` values might have changed due to padding.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function separableConv2d(args: SeparableConvLayerArgs): Layer {\n  return new SeparableConv2D(args);\n}\n\n/**\n * Cropping layer for 2D input (e.g., image).\n *\n * This layer can crop an input\n * at the top, bottom, left and right side of an image tensor.\n *\n * Input shape:\n *   4D tensor with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, rows, cols, channels]`\n *   - If `data_format` is `\"channels_first\"`:\n *     `[batch, channels, rows, cols]`.\n *\n * Output shape:\n *   4D with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, croppedRows, croppedCols, channels]`\n *    - If `dataFormat` is `\"channelsFirst\"`:\n *     `[batch, channels, croppedRows, croppedCols]`.\n *\n * Examples\n * ```js\n *\n * const model = tf.sequential();\n * model.add(tf.layers.cropping2D({cropping:[[2, 2], [2, 2]],\n *                                inputShape: [128, 128, 3]}));\n * //now output shape is [batch, 124, 124, 3]\n * ```\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function cropping2D(args: Cropping2DLayerArgs): Layer {\n  return new Cropping2D(args);\n}\n\n/**\n * Upsampling layer for 2D inputs.\n *\n * Repeats the rows and columns of the data\n * by size[0] and size[1] respectively.\n *\n *\n * Input shape:\n *    4D tensor with shape:\n *     - If `dataFormat` is `\"channelsLast\"`:\n *         `[batch, rows, cols, channels]`\n *     - If `dataFormat` is `\"channelsFirst\"`:\n *        `[batch, channels, rows, cols]`\n *\n * Output shape:\n *     4D tensor with shape:\n *     - If `dataFormat` is `\"channelsLast\"`:\n *        `[batch, upsampledRows, upsampledCols, channels]`\n *     - If `dataFormat` is `\"channelsFirst\"`:\n *         `[batch, channels, upsampledRows, upsampledCols]`\n *\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function upSampling2d(args: UpSampling2DLayerArgs): Layer {\n  return new UpSampling2D(args);\n}\n\n// Convolutional(depthwise) Layers.\n\n/**\n * Depthwise separable 2D convolution.\n *\n * Depthwise Separable convolutions consists in performing just the first step\n * in a depthwise spatial convolution (which acts on each input channel\n * separately). The `depthMultplier` argument controls how many output channels\n * are generated per input channel in the depthwise step.\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function depthwiseConv2d(args: DepthwiseConv2DLayerArgs): Layer {\n  return new DepthwiseConv2D(args);\n}\n\n// Basic Layers.\n\n/**\n * Applies an activation function to an output.\n *\n * This layer applies element-wise activation function.  Other layers, notably\n * `dense` can also apply activation functions.  Use this isolated activation\n * function to extract the values before and after the\n * activation. For instance:\n *\n * ```js\n * const input = tf.input({shape: [5]});\n * const denseLayer = tf.layers.dense({units: 1});\n * const activationLayer = tf.layers.activation({activation: 'relu6'});\n *\n * // Obtain the output symbolic tensors by applying the layers in order.\n * const denseOutput = denseLayer.apply(input);\n * const activationOutput = activationLayer.apply(denseOutput);\n *\n * // Create the model based on the inputs.\n * const model = tf.model({\n *     inputs: input,\n *     outputs: [denseOutput, activationOutput]\n * });\n *\n * // Collect both outputs and print separately.\n * const [denseOut, activationOut] = model.predict(tf.randomNormal([6, 5]));\n * denseOut.print();\n * activationOut.print();\n * ```\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function activation(args: ActivationLayerArgs): Layer {\n  return new Activation(args);\n}\n\n/**\n * Creates a dense (fully connected) layer.\n *\n * This layer implements the operation:\n *   `output = activation(dot(input, kernel) + bias)`\n *\n * `activation` is the element-wise activation function\n *   passed as the `activation` argument.\n *\n * `kernel` is a weights matrix created by the layer.\n *\n * `bias` is a bias vector created by the layer (only applicable if `useBias`\n * is `true`).\n *\n * **Input shape:**\n *\n *   nD `tf.Tensor` with shape: `(batchSize, ..., inputDim)`.\n *\n *   The most common situation would be\n *   a 2D input with shape `(batchSize, inputDim)`.\n *\n * **Output shape:**\n *\n *   nD tensor with shape: `(batchSize, ..., units)`.\n *\n *   For instance, for a 2D input with shape `(batchSize, inputDim)`,\n *   the output would have shape `(batchSize, units)`.\n *\n * Note: if the input to the layer has a rank greater than 2, then it is\n * flattened prior to the initial dot product with the kernel.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function dense(args: DenseLayerArgs): Layer {\n  return new Dense(args);\n}\n\n/**\n * Applies\n * [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) to\n * the input.\n *\n * Dropout consists in randomly setting a fraction `rate` of input units to 0 at\n * each update during training time, which helps prevent overfitting.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function dropout(args: DropoutLayerArgs): Layer {\n  return new Dropout(args);\n}\n\n/**\n * Spatial 1D version of Dropout.\n *\n * This Layer type performs the same function as the Dropout layer, but it drops\n * entire 1D feature maps instead of individual elements. For example, if an\n * input example consists of 3 timesteps and the feature map for each timestep\n * has a size of 4, a `spatialDropout1d` layer may zero out the feature maps\n * of the 1st timesteps and 2nd timesteps completely while sparing all feature\n * elements of the 3rd timestep.\n *\n * If adjacent frames (timesteps) are strongly correlated (as is normally the\n * case in early convolution layers), regular dropout will not regularize the\n * activation and will otherwise just result in merely an effective learning\n * rate decrease. In this case, `spatialDropout1d` will help promote\n * independence among feature maps and should be used instead.\n *\n * **Arguments:**\n *   rate: A floating-point number >=0 and <=1. Fraction of the input elements\n *     to drop.\n *\n * **Input shape:**\n *   3D tensor with shape `(samples, timesteps, channels)`.\n *\n * **Output shape:**\n *   Same as the input shape.\n *\n * References:\n *   - [Efficient Object Localization Using Convolutional\n *      Networks](https://arxiv.org/abs/1411.4280)\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function spatialDropout1d(args: SpatialDropout1DLayerConfig): Layer {\n  return new SpatialDropout1D(args);\n}\n\n/**\n * Flattens the input. Does not affect the batch size.\n *\n * A `Flatten` layer flattens each batch in its inputs to 1D (making the output\n * 2D).\n *\n * For example:\n *\n * ```js\n * const input = tf.input({shape: [4, 3]});\n * const flattenLayer = tf.layers.flatten();\n * // Inspect the inferred output shape of the flatten layer, which\n * // equals `[null, 12]`. The 2nd dimension is 4 * 3, i.e., the result of the\n * // flattening. (The 1st dimension is the undermined batch size.)\n * console.log(JSON.stringify(flattenLayer.apply(input).shape));\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function flatten(args?: FlattenLayerArgs): Layer {\n  return new Flatten(args);\n}\n\n/**\n * Repeats the input n times in a new dimension.\n *\n * ```js\n *  const model = tf.sequential();\n *  model.add(tf.layers.repeatVector({n: 4, inputShape: [2]}));\n *  const x = tf.tensor2d([[10, 20]]);\n *  // Use the model to do inference on a data point the model hasn't see\n *  model.predict(x).print();\n *  // output shape is now [batch, 2, 4]\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function repeatVector(args: RepeatVectorLayerArgs): Layer {\n  return new RepeatVector(args);\n}\n\n/**\n * Reshapes an input to a certain shape.\n *\n * ```js\n * const input = tf.input({shape: [4, 3]});\n * const reshapeLayer = tf.layers.reshape({targetShape: [2, 6]});\n * // Inspect the inferred output shape of the Reshape layer, which\n * // equals `[null, 2, 6]`. (The 1st dimension is the undermined batch size.)\n * console.log(JSON.stringify(reshapeLayer.apply(input).shape));\n * ```\n *\n * Input shape:\n *   Arbitrary, although all dimensions in the input shape must be fixed.\n *   Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n *\n * Output shape:\n *   [batchSize, targetShape[0], targetShape[1], ...,\n *    targetShape[targetShape.length - 1]].\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function reshape(args: ReshapeLayerArgs): Layer {\n  return new Reshape(args);\n}\n\n/**\n * Permutes the dimensions of the input according to a given pattern.\n *\n * Useful for, e.g., connecting RNNs and convnets together.\n *\n * Example:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.permute({\n *   dims: [2, 1],\n *   inputShape: [10, 64]\n * }));\n * console.log(model.outputShape);\n * // Now model's output shape is [null, 64, 10], where null is the\n * // unpermuted sample (batch) dimension.\n * ```\n *\n * Input shape:\n *   Arbitrary. Use the configuration field `inputShape` when using this\n *   layer as the first layer in a model.\n *\n * Output shape:\n *   Same rank as the input shape, but with the dimensions re-ordered (i.e.,\n *   permuted) according to the `dims` configuration of this layer.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function permute(args: PermuteLayerArgs): Layer {\n  return new Permute(args);\n}\n\n/**\n * Maps positive integers (indices) into dense vectors of fixed size.\n * eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n *\n * **Input shape:** 2D tensor with shape: `[batchSize, sequenceLength]`.\n *\n * **Output shape:** 3D tensor with shape: `[batchSize, sequenceLength,\n * outputDim]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'} */\nexport function embedding(args: EmbeddingLayerArgs): Layer {\n  return new Embedding(args);\n}\n\n// Merge Layers.\n\n/**\n * Layer that performs element-wise addition on an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). The inputs are specified as an\n * `Array` when the `apply` method of the `Add` layer instance is called. For\n * example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const addLayer = tf.layers.add();\n * const sum = addLayer.apply([input1, input2]);\n * console.log(JSON.stringify(sum.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function add(args?: LayerArgs): Layer {\n  return new Add(args);\n}\n\n/**\n * Layer that performs element-wise averaging on an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const averageLayer = tf.layers.average();\n * const average = averageLayer.apply([input1, input2]);\n * console.log(JSON.stringify(average.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function average(args?: LayerArgs): Layer {\n  return new Average(args);\n}\n\n/**\n * Layer that concatenates an `Array` of inputs.\n *\n * It takes a list of tensors, all of the same shape except for the\n * concatenation axis, and returns a single tensor, the concatenation\n * of all inputs. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 3]});\n * const concatLayer = tf.layers.concatenate();\n * const output = concatLayer.apply([input1, input2]);\n * console.log(JSON.stringify(output.shape));\n * // You get [null, 2, 5], with the first dimension as the undetermined batch\n * // dimension. The last dimension (5) is the result of concatenating the\n * // last dimensions of the inputs (2 and 3).\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function concatenate(args?: ConcatenateLayerArgs): Layer {\n  return new Concatenate(args);\n}\n\n/**\n * Layer that computes the element-wise maximum an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const maxLayer = tf.layers.maximum();\n * const max = maxLayer.apply([input1, input2]);\n * console.log(JSON.stringify(max.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function maximum(args?: LayerArgs): Layer {\n  return new Maximum(args);\n}\n\n/**\n * Layer that computes the element-wise minimum of an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const minLayer = tf.layers.minimum();\n * const min = minLayer.apply([input1, input2]);\n * console.log(JSON.stringify(min.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function minimum(args?: LayerArgs): Layer {\n  return new Minimum(args);\n}\n\n/**\n * Layer that multiplies (element-wise) an `Array` of inputs.\n *\n * It takes as input an Array of tensors, all of the same\n * shape, and returns a single tensor (also of the same shape).\n * For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const input3 = tf.input({shape: [2, 2]});\n * const multiplyLayer = tf.layers.multiply();\n * const product = multiplyLayer.apply([input1, input2, input3]);\n * console.log(product.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function multiply(args?: LayerArgs): Layer {\n  return new Multiply(args);\n}\n\n/**\n * Layer that computes a dot product between samples in two tensors.\n *\n * E.g., if applied to a list of two tensors `a` and `b` both of shape\n * `[batchSize, n]`, the output will be a tensor of shape `[batchSize, 1]`,\n * where each entry at index `[i, 0]` will be the dot product between\n * `a[i, :]` and `b[i, :]`.\n *\n * Example:\n *\n * ```js\n * const dotLayer = tf.layers.dot({axes: -1});\n * const x1 = tf.tensor2d([[10, 20], [30, 40]]);\n * const x2 = tf.tensor2d([[-1, -2], [-3, -4]]);\n *\n * // Invoke the layer's apply() method in eager (imperative) mode.\n * const y = dotLayer.apply([x1, x2]);\n * y.print();\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'} */\nexport function dot(args: DotLayerArgs): Layer {\n  return new Dot(args);\n}\n\n// Normalization Layers.\n\n/**\n * Batch normalization layer (Ioffe and Szegedy, 2014).\n *\n * Normalize the activations of the previous layer at each batch,\n * i.e. applies a transformation that maintains the mean activation\n * close to 0 and the activation standard deviation close to 1.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape` (Array of integers, does\n *   not include the sample axis) when calling the constructor of this class,\n *   if this layer is used as a first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Batch Normalization: Accelerating Deep Network Training by Reducing\n * Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n */\nexport function batchNormalization(args?: BatchNormalizationLayerArgs): Layer {\n  return new BatchNormalization(args);\n}\n\n/**\n * Layer-normalization layer (Ba et al., 2016).\n *\n * Normalizes the activations of the previous layer for each given example in a\n * batch independently, instead of across a batch like in `batchNormalization`.\n * In other words, this layer applies a transformation that maintanis the mean\n * activation within each example close to0 and activation variance close to 1.\n *\n * Input shape:\n *   Arbitrary. Use the argument `inputShape` when using this layer as the first\n *   layer in a model.\n *\n * Output shape:\n *   Same as input.\n *\n * References:\n *   - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n */\n/**\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n */\nexport function layerNormalization(args?: LayerNormalizationLayerArgs): Layer {\n  return new LayerNormalization(args);\n}\n\n// Padding Layers.\n\n/**\n * Zero-padding layer for 2D input (e.g., image).\n *\n * This layer can add rows and columns of zeros\n * at the top, bottom, left and right side of an image tensor.\n *\n * Input shape:\n *   4D tensor with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, rows, cols, channels]`\n *   - If `data_format` is `\"channels_first\"`:\n *     `[batch, channels, rows, cols]`.\n *\n * Output shape:\n *   4D with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, paddedRows, paddedCols, channels]`\n *    - If `dataFormat` is `\"channelsFirst\"`:\n *     `[batch, channels, paddedRows, paddedCols]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Padding', namespace: 'layers'} */\nexport function zeroPadding2d(args?: ZeroPadding2DLayerArgs): Layer {\n  return new ZeroPadding2D(args);\n}\n\n// Pooling Layers.\n\n/**\n * Average pooling operation for spatial data.\n *\n * Input shape: `[batchSize, inLength, channels]`\n *\n * Output shape: `[batchSize, pooledLength, channels]`\n *\n * `tf.avgPool1d` is an alias.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function averagePooling1d(args: Pooling1DLayerArgs): Layer {\n  return new AveragePooling1D(args);\n}\nexport function avgPool1d(args: Pooling1DLayerArgs): Layer {\n  return averagePooling1d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling1d(args: Pooling1DLayerArgs): Layer {\n  return averagePooling1d(args);\n}\n\n/**\n * Average pooling operation for spatial data.\n *\n * Input shape:\n *  - If `dataFormat === CHANNEL_LAST`:\n *      4D tensor with shape:\n *      `[batchSize, rows, cols, channels]`\n *  - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *      `[batchSize, channels, rows, cols]`\n *\n * Output shape\n *  - If `dataFormat === CHANNEL_LAST`:\n *      4D tensor with shape:\n *      `[batchSize, pooleRows, pooledCols, channels]`\n *  - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *      `[batchSize, channels, pooleRows, pooledCols]`\n *\n * `tf.avgPool2d` is an alias.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function averagePooling2d(args: Pooling2DLayerArgs): Layer {\n  return new AveragePooling2D(args);\n}\nexport function avgPool2d(args: Pooling2DLayerArgs): Layer {\n  return averagePooling2d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling2d(args: Pooling2DLayerArgs): Layer {\n  return averagePooling2d(args);\n}\n\n/**\n * Average pooling operation for 3D data.\n *\n * Input shape\n *   - If `dataFormat === channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, depths, rows, cols, channels]`\n *   - If `dataFormat === channelsFirst`:\n *      4D tensor with shape:\n *       `[batchSize, channels, depths, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=channelsFirst`:\n *       5D tensor with shape:\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function averagePooling3d(args: Pooling3DLayerArgs): Layer {\n  return new AveragePooling3D(args);\n}\nexport function avgPool3d(args: Pooling3DLayerArgs): Layer {\n  return averagePooling3d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling3d(args: Pooling3DLayerArgs): Layer {\n  return averagePooling3d(args);\n}\n\n/**\n * Global average pooling operation for temporal data.\n *\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n *\n * Output Shape:2D tensor with shape: `[batchSize, features]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function globalAveragePooling1d(args?: LayerArgs): Layer {\n  return new GlobalAveragePooling1D(args);\n}\n\n/**\n * Global average pooling operation for spatial data.\n *\n * Input shape:\n *   - If `dataFormat` is `CHANNEL_LAST`:\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n *   - If `dataFormat` is `CHANNEL_FIRST`:\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n *\n * Output shape:\n *   2D tensor with shape: `[batchSize, channels]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function globalAveragePooling2d(args: GlobalPooling2DLayerArgs): Layer {\n  return new GlobalAveragePooling2D(args);\n}\n\n/**\n * Global max pooling operation for temporal data.\n *\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n *\n * Output Shape:2D tensor with shape: `[batchSize, features]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function globalMaxPooling1d(args?: LayerArgs): Layer {\n  return new GlobalMaxPooling1D(args);\n}\n\n/**\n * Global max pooling operation for spatial data.\n *\n * Input shape:\n *   - If `dataFormat` is `CHANNEL_LAST`:\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n *   - If `dataFormat` is `CHANNEL_FIRST`:\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n *\n * Output shape:\n *   2D tensor with shape: `[batchSize, channels]`.\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function globalMaxPooling2d(args: GlobalPooling2DLayerArgs): Layer {\n  return new GlobalMaxPooling2D(args);\n}\n\n/**\n * Max pooling operation for temporal data.\n *\n * Input shape:  `[batchSize, inLength, channels]`\n *\n * Output shape: `[batchSize, pooledLength, channels]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function maxPooling1d(args: Pooling1DLayerArgs): Layer {\n  return new MaxPooling1D(args);\n}\n\n/**\n * Max pooling operation for spatial data.\n *\n * Input shape\n *   - If `dataFormat === CHANNEL_LAST`:\n *       4D tensor with shape:\n *       `[batchSize, rows, cols, channels]`\n *   - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *       `[batchSize, channels, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=CHANNEL_LAST`:\n *       4D tensor with shape:\n *       `[batchSize, pooleRows, pooledCols, channels]`\n *   - If `dataFormat=CHANNEL_FIRST`:\n *       4D tensor with shape:\n *       `[batchSize, channels, pooleRows, pooledCols]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function maxPooling2d(args: Pooling2DLayerArgs): Layer {\n  return new MaxPooling2D(args);\n}\n\n/**\n * Max pooling operation for 3D data.\n *\n * Input shape\n *   - If `dataFormat === channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, depths, rows, cols, channels]`\n *   - If `dataFormat === channelsFirst`:\n *      5D tensor with shape:\n *       `[batchSize, channels, depths, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=channelsFirst`:\n *       5D tensor with shape:\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n */\n/** @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'} */\nexport function maxPooling3d(args: Pooling3DLayerArgs): Layer {\n  return new MaxPooling3D(args);\n}\n\n// Recurrent Layers.\n\n/**\n * Gated Recurrent Unit - Cho et al. 2014.\n *\n * This is an `RNN` layer consisting of one `GRUCell`. However, unlike\n * the underlying `GRUCell`, the `apply` method of `SimpleRNN` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const rnn = tf.layers.gru({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `GRUCell`'s number of units.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function gru(args: GRULayerArgs): Layer {\n  return new GRU(args);\n}\n\n/**\n * Cell class for `GRU`.\n *\n * `GRUCell` is distinct from the `RNN` subclass `GRU` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `GRU` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.gruCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `GRUCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.gruCell({units: 4}),\n *   tf.layers.gruCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `gruCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `GRUCell`, use the\n * `tf.layers.gru`.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function gruCell(args: GRUCellLayerArgs): RNNCell {\n  return new GRUCell(args);\n}\n\n/**\n * Long-Short Term Memory layer - Hochreiter 1997.\n *\n * This is an `RNN` layer consisting of one `LSTMCell`. However, unlike\n * the underlying `LSTMCell`, the `apply` method of `LSTM` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const lstm = tf.layers.lstm({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = lstm.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `LSTMCell`'s number of units.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function lstm(args: LSTMLayerArgs): Layer {\n  return new LSTM(args);\n}\n\n/**\n * Cell class for `LSTM`.\n *\n * `LSTMCell` is distinct from the `RNN` subclass `LSTM` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `LSTM` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.lstmCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `LSTMCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.lstmCell({units: 4}),\n *   tf.layers.lstmCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `lstmCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `LSTMCell`, use the\n * `tf.layers.lstm`.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function lstmCell(args: LSTMCellLayerArgs): RNNCell {\n  return new LSTMCell(args);\n}\n\n/**\n * Fully-connected RNN where the output is to be fed back to input.\n *\n * This is an `RNN` layer consisting of one `SimpleRNNCell`. However, unlike\n * the underlying `SimpleRNNCell`, the `apply` method of `SimpleRNN` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const rnn = tf.layers.simpleRNN({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `SimpleRNNCell`'s number of units.\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function simpleRNN(args: SimpleRNNLayerArgs): Layer {\n  return new SimpleRNN(args);\n}\n\n/**\n * Cell class for `SimpleRNN`.\n *\n * `SimpleRNNCell` is distinct from the `RNN` subclass `SimpleRNN` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `SimpleRNN` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.simpleRNNCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `SimpleRNNCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.simpleRNNCell({units: 4}),\n *   tf.layers.simpleRNNCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `SimpleRNNCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `SimpleRNNCell`, use the\n * `tf.layers.simpleRNN`.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function simpleRNNCell(args: SimpleRNNCellLayerArgs): RNNCell {\n  return new SimpleRNNCell(args);\n}\n\n/**\n * Base class for recurrent layers.\n *\n * Input shape:\n *   3D tensor with shape `[batchSize, timeSteps, inputDim]`.\n *\n * Output shape:\n *   - if `returnState`, an Array of tensors (i.e., `tf.Tensor`s). The first\n *     tensor is the output. The remaining tensors are the states at the\n *     last time step, each with shape `[batchSize, units]`.\n *   - if `returnSequences`, the output will have shape\n *     `[batchSize, timeSteps, units]`.\n *   - else, the output will have shape `[batchSize, units]`.\n *\n * Masking:\n *   This layer supports masking for input data with a variable number\n *   of timesteps. To introduce masks to your data,\n *   use an embedding layer with the `mask_zero` parameter\n *   set to `True`.\n *\n * Notes on using statefulness in RNNs:\n *   You can set RNN layers to be 'stateful', which means that the states\n *   computed for the samples in one batch will be reused as initial states\n *   for the samples in the next batch. This assumes a one-to-one mapping\n *   between samples in different successive batches.\n *\n *   To enable statefulness:\n *     - specify `stateful: true` in the layer constructor.\n *     - specify a fixed batch size for your model, by passing\n *       if sequential model:\n *         `batchInputShape=[...]` to the first layer in your model.\n *       else for functional model with 1 or more Input layers:\n *         `batchShape=[...]` to all the first layers in your model.\n *       This is the expected shape of your inputs *including the batch size*.\n *       It should be a tuple of integers, e.g. `(32, 10, 100)`.\n *     - specify `shuffle=False` when calling fit().\n *\n *   To reset the states of your model, call `.resetStates()` on either\n *   a specific layer, or on your entire model.\n *\n * Note on specifying the initial state of RNNs\n *   You can specify the initial state of RNN layers symbolically by\n *   calling them with the option `initialState`. The value of\n *   `initialState` should be a tensor or list of tensors representing\n *   the initial state of the RNN layer.\n *\n *   You can specify the initial state of RNN layers numerically by\n *   calling `resetStates` with the keyword argument `states`. The value of\n *   `states` should be a numpy array or list of numpy arrays representing\n *   the initial state of the RNN layer.\n *\n * Note on passing external constants to RNNs\n *   You can pass \"external\" constants to the cell using the `constants`\n *   keyword argument of `RNN.call` method. This requires that the `cell.call`\n *   method accepts the same keyword argument `constants`. Such constants\n *   can be used to conditon the cell transformation on additional static inputs\n *   (not changing over time), a.k.a an attention mechanism.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function rnn(args: RNNLayerArgs): Layer {\n  return new RNN(args);\n}\n\n/**\n * Wrapper allowing a stack of RNN cells to behave as a single cell.\n *\n * Used to implement efficient stacked RNNs.\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function stackedRNNCells(args: StackedRNNCellsArgs): RNNCell {\n  return new StackedRNNCells(args);\n}\n\n// Wrapper Layers.\n\n/** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\nexport function bidirectional(args: BidirectionalLayerArgs): Bidirectional {\n  return new Bidirectional(args);\n}\n\n/**\n * This wrapper applies a layer to every temporal slice of an input.\n *\n * The input should be at least 3D,  and the dimension of the index `1` will be\n * considered to be the temporal dimension.\n *\n * Consider a batch of 32 samples, where each sample is a sequence of 10 vectors\n * of 16 dimensions. The batch input shape of the layer is then `[32,  10,\n * 16]`, and the `inputShape`, not including the sample dimension, is\n * `[10, 16]`.\n *\n * You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10\n * timesteps, independently:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.timeDistributed({\n *   layer: tf.layers.dense({units: 8}),\n *   inputShape: [10, 16],\n * }));\n *\n * // Now model.outputShape = [null, 10, 8].\n * // The output will then have shape `[32, 10, 8]`.\n *\n * // In subsequent layers, there is no need for `inputShape`:\n * model.add(tf.layers.timeDistributed({layer: tf.layers.dense({units: 32})}));\n * console.log(JSON.stringify(model.outputs[0].shape));\n * // Now model.outputShape = [null, 10, 32].\n * ```\n *\n * The output will then have shape `[32, 10, 32]`.\n *\n * `TimeDistributed` can be used with arbitrary layers, not just `Dense`, for\n * instance a `Conv2D` layer.\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.timeDistributed({\n *   layer: tf.layers.conv2d({filters: 64, kernelSize: [3, 3]}),\n *   inputShape: [10, 299, 299, 3],\n * }));\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\nexport function timeDistributed(args: WrapperLayerArgs): Layer {\n  return new TimeDistributed(args);\n}\n\n// Aliases for pooling.\nexport const globalMaxPool1d = globalMaxPooling1d;\nexport const globalMaxPool2d = globalMaxPooling2d;\nexport const maxPool1d = maxPooling1d;\nexport const maxPool2d = maxPooling2d;\n\nexport {Layer, RNN, RNNCell, input /* alias for tf.input */};\n\n/**\n * Apply additive zero-centered Gaussian noise.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * This is useful to mitigate overfitting\n * (you could see it as a form of random data augmentation).\n * Gaussian Noise (GS) is a natural choice as corruption process\n * for real valued inputs.\n *\n * # Arguments\n *     stddev: float, standard deviation of the noise distribution.\n *\n * # Input shape\n *         Arbitrary. Use the keyword argument `input_shape`\n *         (tuple of integers, does not include the samples axis)\n *         when using this layer as the first layer in a model.\n *\n * # Output shape\n *         Same shape as input.\n */\n/** @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'} */\nexport function gaussianNoise(args: GaussianNoiseArgs): GaussianNoise {\n  return new GaussianNoise(args);\n}\n\n/**\n * Apply multiplicative 1-centered Gaussian noise.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n *      http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n *\n */\n/** @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'} */\nexport function gaussianDropout(args: GaussianDropoutArgs): GaussianDropout {\n  return new GaussianDropout(args);\n}\n\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\n/** @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'} */\nexport function alphaDropout(args: AlphaDropoutArgs): AlphaDropout {\n  return new AlphaDropout(args);\n}\n\n/**\n * Masks a sequence by using a mask value to skip timesteps.\n *\n * If all features for a given sample timestep are equal to `mask_value`,\n * then the sample timestep will be masked (skipped) in all downstream layers\n * (as long as they support masking).\n *\n * If any downstream layer does not support masking yet receives such\n * an input mask, an exception will be raised.\n *\n * Arguments:\n *   - `maskValue`: Either None or mask value to skip.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n */\n/** @doc {heading: 'Layers', subheading: 'Mask', namespace: 'layers'} */\nexport function masking(args?: MaskingArgs): Layer {\n  return new Masking(args);\n}\n"]}},"error":null,"hash":"a0a6ad5783fd046195e888592dd2152c","cacheData":{"env":{}}}