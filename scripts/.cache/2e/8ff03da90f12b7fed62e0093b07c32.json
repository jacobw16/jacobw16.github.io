{"id":"node_modules/@tensorflow/tfjs-layers/dist/layers/embeddings.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\embeddings.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1582861032163},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":30,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../backend/tfjs_backend","loc":{"line":31,"column":16},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"../constraints","loc":{"line":32,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\constraints.js"},{"name":"../engine/topology","loc":{"line":33,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"../errors","loc":{"line":34,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../initializers","loc":{"line":35,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\initializers.js"},{"name":"../regularizers","loc":{"line":36,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\regularizers.js"},{"name":"../utils/generic_utils","loc":{"line":37,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"},{"name":"../utils/types_utils","loc":{"line":38,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\embeddings.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * TensorFlow.js Layers: Embedding Layer.\n *\n * Original source: keras/constraints.py\n */\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"../backend/tfjs_backend\");\nvar constraints_1 = require(\"../constraints\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar initializers_1 = require(\"../initializers\");\nvar regularizers_1 = require(\"../regularizers\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar types_utils_1 = require(\"../utils/types_utils\");\nvar Embedding = /** @class */ (function (_super) {\n    __extends(Embedding, _super);\n    function Embedding(args) {\n        var _this = _super.call(this, args) || this;\n        _this.embeddings = null;\n        _this.DEFAULT_EMBEDDINGS_INITIALIZER = 'randomUniform';\n        if (args.batchInputShape == null && args.inputShape == null) {\n            // Porting Note: This logic is copied from Layer's constructor, since we\n            // can't do exactly what the Python constructor does for Embedding().\n            // Specifically, the super constructor can not be called after the\n            // mutation of the `config` argument.\n            var batchSize = null;\n            if (args.batchSize != null) {\n                batchSize = args.batchSize;\n            }\n            if (args.inputLength == null) {\n                // Fix super-constructor to what it would have done if\n                // 'config.inputShape' were (None, )\n                _this.batchInputShape = [batchSize, null];\n            }\n            else {\n                // Fix super-constructor to what it would have done if\n                // 'config.inputShape' were (config.inputLength, )\n                _this.batchInputShape =\n                    [batchSize].concat(generic_utils.toList(args.inputLength));\n            }\n        }\n        _this.inputDim = args.inputDim;\n        generic_utils.assertPositiveInteger(_this.inputDim, 'inputDim');\n        _this.outputDim = args.outputDim;\n        generic_utils.assertPositiveInteger(_this.outputDim, 'outputDim');\n        _this.embeddingsInitializer = initializers_1.getInitializer(args.embeddingsInitializer || _this.DEFAULT_EMBEDDINGS_INITIALIZER);\n        _this.embeddingsRegularizer = regularizers_1.getRegularizer(args.embeddingsRegularizer);\n        _this.activityRegularizer = regularizers_1.getRegularizer(args.activityRegularizer);\n        _this.embeddingsConstraint = constraints_1.getConstraint(args.embeddingsConstraint);\n        _this.maskZero = args.maskZero;\n        _this.supportsMasking = args.maskZero;\n        _this.inputLength = args.inputLength;\n        return _this;\n    }\n    Embedding.prototype.build = function (inputShape) {\n        this.embeddings = this.addWeight('embeddings', [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, true, this.embeddingsConstraint);\n        this.built = true;\n    };\n    // Override warnOnIncompatibleInputShape because an embedding layer allows\n    // the input to have varying ranks.\n    Embedding.prototype.warnOnIncompatibleInputShape = function (inputShape) { };\n    Embedding.prototype.computeMask = function (inputs, mask) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            if (!_this.maskZero) {\n                return null;\n            }\n            else {\n                inputs = types_utils_1.getExactlyOneTensor(inputs);\n                return tfjs_core_1.notEqual(inputs, tfjs_core_1.zerosLike(inputs));\n            }\n        });\n    };\n    Embedding.prototype.computeOutputShape = function (inputShape) {\n        inputShape = types_utils_1.getExactlyOneShape(inputShape);\n        if (this.inputLength == null) {\n            return inputShape.concat([this.outputDim]);\n        }\n        // inputLength can be an array if input is 3D or higher.\n        var inLens = generic_utils.toList(this.inputLength);\n        if (inLens.length !== inputShape.length - 1) {\n            throw new errors_1.ValueError(\"\\\"inputLength\\\" is \" + this.inputLength + \", but received \" +\n                (\"input shape has shape \" + inputShape));\n        }\n        else {\n            var i = 0;\n            for (var k = 0; k < inLens.length; ++k) {\n                var s1 = inLens[k];\n                var s2 = inputShape[k + 1];\n                if ((s1 != null) && (s2 != null) && (s1 !== s2)) {\n                    throw new errors_1.ValueError(\"\\\"inputLength\\\" is \" + this.inputLength + \", but received \" +\n                        (\"input shape has shape \" + inputShape));\n                }\n                else if (s1 == null) {\n                    inLens[i] = s2;\n                }\n                i++;\n            }\n        }\n        return [inputShape[0]].concat(inLens, [this.outputDim]);\n    };\n    Embedding.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            _this.invokeCallHook(inputs, kwargs);\n            // Embedding layer accepts only a single input.\n            var input = types_utils_1.getExactlyOneTensor(inputs);\n            if (input.dtype !== 'int32') {\n                input = K.cast(input, 'int32');\n            }\n            var output = K.gather(_this.embeddings.read(), input.as1D());\n            return output.reshape(types_utils_1.getExactlyOneShape(_this.computeOutputShape(input.shape)));\n        });\n    };\n    Embedding.prototype.getConfig = function () {\n        var config = {\n            inputDim: this.inputDim,\n            outputDim: this.outputDim,\n            embeddingsInitializer: initializers_1.serializeInitializer(this.embeddingsInitializer),\n            embeddingsRegularizer: regularizers_1.serializeRegularizer(this.embeddingsRegularizer),\n            activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n            embeddingsConstraint: constraints_1.serializeConstraint(this.embeddingsConstraint),\n            maskZero: this.maskZero,\n            inputLength: this.inputLength\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    Embedding.className = 'Embedding';\n    return Embedding;\n}(topology_1.Layer));\nexports.Embedding = Embedding;\ntfjs_core_1.serialization.registerClass(Embedding);\n"},"sourceMaps":{"js":{"version":3,"file":"embeddings.js","sourceRoot":"","sources":["../../src/layers/embeddings.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH;;;;GAIG;AACH,mDAAuF;AAEvF,2CAA6C;AAC7C,8CAAoG;AACpG,+CAAoD;AACpD,oCAAqC;AACrC,gDAAyG;AAEzG,gDAAyG;AAEzG,sDAAwD;AACxD,oDAA6E;AAiD7E;IAA+B,6BAAK;IAgBlC,mBAAY,IAAwB;QAApC,YACE,kBAAM,IAAI,CAAC,SAiCZ;QAzCO,gBAAU,GAAkB,IAAI,CAAC;QAEhC,oCAA8B,GACnC,eAAe,CAAC;QAMlB,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3D,wEAAwE;YACxE,qEAAqE;YACrE,kEAAkE;YAClE,qCAAqC;YACrC,IAAI,SAAS,GAAW,IAAI,CAAC;YAC7B,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;gBAC1B,SAAS,GAAG,IAAI,CAAC,SAAS,CAAC;aAC5B;YACD,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;gBAC5B,sDAAsD;gBACtD,oCAAoC;gBACpC,KAAI,CAAC,eAAe,GAAG,CAAC,SAAS,EAAE,IAAI,CAAC,CAAC;aAC1C;iBAAM;gBACL,sDAAsD;gBACtD,kDAAkD;gBAClD,KAAI,CAAC,eAAe;oBAChB,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,aAAa,CAAC,MAAM,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC;aAChE;SACF;QACD,KAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,CAAC;QAC9B,aAAa,CAAC,qBAAqB,CAAC,KAAI,CAAC,QAAQ,EAAE,UAAU,CAAC,CAAC;QAC/D,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,SAAS,CAAC;QAChC,aAAa,CAAC,qBAAqB,CAAC,KAAI,CAAC,SAAS,EAAE,WAAW,CAAC,CAAC;QACjE,KAAI,CAAC,qBAAqB,GAAG,6BAAc,CACvC,IAAI,CAAC,qBAAqB,IAAI,KAAI,CAAC,8BAA8B,CAAC,CAAC;QACvE,KAAI,CAAC,qBAAqB,GAAG,6BAAc,CAAC,IAAI,CAAC,qBAAqB,CAAC,CAAC;QACxE,KAAI,CAAC,mBAAmB,GAAG,6BAAc,CAAC,IAAI,CAAC,mBAAmB,CAAC,CAAC;QACpE,KAAI,CAAC,oBAAoB,GAAG,2BAAa,CAAC,IAAI,CAAC,oBAAoB,CAAC,CAAC;QACrE,KAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,CAAC;QAC9B,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC,QAAQ,CAAC;QACrC,KAAI,CAAC,WAAW,GAAG,IAAI,CAAC,WAAW,CAAC;;IACtC,CAAC;IAEM,yBAAK,GAAZ,UAAa,UAAyB;QACpC,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,SAAS,CAC5B,YAAY,EAAE,CAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,SAAS,CAAC,EAAE,IAAI,CAAC,KAAK,EACzD,IAAI,CAAC,qBAAqB,EAAE,IAAI,CAAC,qBAAqB,EAAE,IAAI,EAC5D,IAAI,CAAC,oBAAoB,CAAC,CAAC;QAC/B,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAED,0EAA0E;IAC1E,mCAAmC;IACzB,gDAA4B,GAAtC,UAAuC,UAAiB,IAAG,CAAC;IAE5D,+BAAW,GAAX,UAAY,MAAuB,EAAE,IAAsB;QAA3D,iBASC;QARC,OAAO,gBAAI,CAAC;YACV,IAAI,CAAC,KAAI,CAAC,QAAQ,EAAE;gBAClB,OAAO,IAAI,CAAC;aACb;iBAAM;gBACL,MAAM,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;gBACrC,OAAO,oBAAQ,CAAC,MAAM,EAAE,qBAAS,CAAC,MAAM,CAAC,CAAC,CAAC;aAC5C;QACH,CAAC,CAAC,CAAC;IACL,CAAC;IAED,sCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,UAAU,GAAG,gCAAkB,CAAC,UAAU,CAAC,CAAC;QAC5C,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;YAC5B,OAAW,UAAU,SAAE,IAAI,CAAC,SAAS,GAAE;SACxC;QACD,wDAAwD;QACxD,IAAM,MAAM,GAAa,aAAa,CAAC,MAAM,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;QAChE,IAAI,MAAM,CAAC,MAAM,KAAK,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;YAC3C,MAAM,IAAI,mBAAU,CAChB,wBAAoB,IAAI,CAAC,WAAW,oBAAiB;iBACrD,2BAAyB,UAAY,CAAA,CAAC,CAAC;SAC5C;aAAM;YACL,IAAI,CAAC,GAAG,CAAC,CAAC;YACV,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,IAAM,EAAE,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;gBACrB,IAAM,EAAE,GAAG,UAAU,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;gBAC7B,IAAI,CAAC,EAAE,IAAI,IAAI,CAAC,IAAI,CAAC,EAAE,IAAI,IAAI,CAAC,IAAI,CAAC,EAAE,KAAK,EAAE,CAAC,EAAE;oBAC/C,MAAM,IAAI,mBAAU,CAChB,wBAAoB,IAAI,CAAC,WAAW,oBAAiB;yBACrD,2BAAyB,UAAY,CAAA,CAAC,CAAC;iBAC5C;qBAAM,IAAI,EAAE,IAAI,IAAI,EAAE;oBACrB,MAAM,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC;iBAChB;gBACD,CAAC,EAAE,CAAC;aACL;SACF;QACD,QAAQ,UAAU,CAAC,CAAC,CAAC,SAAK,MAAM,GAAE,IAAI,CAAC,SAAS,GAAE;IACpD,CAAC;IAED,wBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAYC;QAXC,OAAO,gBAAI,CAAC;YACV,KAAI,CAAC,cAAc,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;YACpC,+CAA+C;YAC/C,IAAI,KAAK,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;YACxC,IAAI,KAAK,CAAC,KAAK,KAAK,OAAO,EAAE;gBAC3B,KAAK,GAAG,CAAC,CAAC,IAAI,CAAC,KAAK,EAAE,OAAO,CAAC,CAAC;aAChC;YACD,IAAM,MAAM,GAAG,CAAC,CAAC,MAAM,CAAC,KAAI,CAAC,UAAU,CAAC,IAAI,EAAE,EAAE,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YAC9D,OAAO,MAAM,CAAC,OAAO,CACjB,gCAAkB,CAAC,KAAI,CAAC,kBAAkB,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QAChE,CAAC,CAAC,CAAC;IACL,CAAC;IAED,6BAAS,GAAT;QACE,IAAM,MAAM,GAAG;YACb,QAAQ,EAAE,IAAI,CAAC,QAAQ;YACvB,SAAS,EAAE,IAAI,CAAC,SAAS;YACzB,qBAAqB,EAAE,mCAAoB,CAAC,IAAI,CAAC,qBAAqB,CAAC;YACvE,qBAAqB,EAAE,mCAAoB,CAAC,IAAI,CAAC,qBAAqB,CAAC;YACvE,mBAAmB,EAAE,mCAAoB,CAAC,IAAI,CAAC,mBAAmB,CAAC;YACnE,oBAAoB,EAAE,iCAAmB,CAAC,IAAI,CAAC,oBAAoB,CAAC;YACpE,QAAQ,EAAE,IAAI,CAAC,QAAQ;YACvB,WAAW,EAAE,IAAI,CAAC,WAAW;SAC9B,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAnID,kBAAkB;IACX,mBAAS,GAAG,WAAW,CAAC;IAmIjC,gBAAC;CAAA,AArID,CAA+B,gBAAK,GAqInC;AArIY,8BAAS;AAsItB,yBAAa,CAAC,aAAa,CAAC,SAAS,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Embedding Layer.\n *\n * Original source: keras/constraints.py\n */\nimport {notEqual, serialization, Tensor, tidy, zerosLike} from '@tensorflow/tfjs-core';\n\nimport * as K from '../backend/tfjs_backend';\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {Layer, LayerArgs} from '../engine/topology';\nimport {ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nexport declare interface EmbeddingLayerArgs extends LayerArgs {\n  /**\n   * Integer > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n   */\n  inputDim: number;\n  /**\n   * Integer >= 0. Dimension of the dense embedding.\n   */\n  outputDim: number;\n  /**\n   * Initializer for the `embeddings` matrix.\n   */\n  embeddingsInitializer?: InitializerIdentifier|Initializer;\n  /**\n   * Regularizer function applied to the `embeddings` matrix.\n   */\n  embeddingsRegularizer?: RegularizerIdentifier|Regularizer;\n  /**\n   * Regularizer function applied to the activation.\n   */\n  activityRegularizer?: RegularizerIdentifier|Regularizer;\n  /**\n   * Constraint function applied to the `embeddings` matrix.\n   */\n  embeddingsConstraint?: ConstraintIdentifier|Constraint;\n  /**\n   * Whether the input value 0 is a special \"padding\" value that should be\n   * masked out. This is useful when using recurrent layers which may take\n   * variable length input.\n   *\n   * If this is `True` then all subsequent layers in the model need to support\n   * masking or an exception will be raised. If maskZero is set to `True`, as a\n   * consequence, index 0 cannot be used in the vocabulary (inputDim should\n   * equal size of vocabulary + 1).\n   */\n  maskZero?: boolean;\n  /**\n   * Length of input sequences, when it is constant.\n   *\n   * This argument is required if you are going to connect `flatten` then\n   * `dense` layers upstream (without it, the shape of the dense outputs cannot\n   * be computed).\n   */\n  inputLength?: number|number[];\n}\n\nexport class Embedding extends Layer {\n  /** @nocollapse */\n  static className = 'Embedding';\n  private inputDim: number;\n  private outputDim: number;\n  private embeddingsInitializer: Initializer;\n  private maskZero: boolean;\n  private inputLength: number|number[];\n\n  private embeddings: LayerVariable = null;\n\n  readonly DEFAULT_EMBEDDINGS_INITIALIZER: InitializerIdentifier =\n      'randomUniform';\n  private readonly embeddingsRegularizer?: Regularizer;\n  private readonly embeddingsConstraint?: Constraint;\n\n  constructor(args: EmbeddingLayerArgs) {\n    super(args);\n    if (args.batchInputShape == null && args.inputShape == null) {\n      // Porting Note: This logic is copied from Layer's constructor, since we\n      // can't do exactly what the Python constructor does for Embedding().\n      // Specifically, the super constructor can not be called after the\n      // mutation of the `config` argument.\n      let batchSize: number = null;\n      if (args.batchSize != null) {\n        batchSize = args.batchSize;\n      }\n      if (args.inputLength == null) {\n        // Fix super-constructor to what it would have done if\n        // 'config.inputShape' were (None, )\n        this.batchInputShape = [batchSize, null];\n      } else {\n        // Fix super-constructor to what it would have done if\n        // 'config.inputShape' were (config.inputLength, )\n        this.batchInputShape =\n            [batchSize].concat(generic_utils.toList(args.inputLength));\n      }\n    }\n    this.inputDim = args.inputDim;\n    generic_utils.assertPositiveInteger(this.inputDim, 'inputDim');\n    this.outputDim = args.outputDim;\n    generic_utils.assertPositiveInteger(this.outputDim, 'outputDim');\n    this.embeddingsInitializer = getInitializer(\n        args.embeddingsInitializer || this.DEFAULT_EMBEDDINGS_INITIALIZER);\n    this.embeddingsRegularizer = getRegularizer(args.embeddingsRegularizer);\n    this.activityRegularizer = getRegularizer(args.activityRegularizer);\n    this.embeddingsConstraint = getConstraint(args.embeddingsConstraint);\n    this.maskZero = args.maskZero;\n    this.supportsMasking = args.maskZero;\n    this.inputLength = args.inputLength;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    this.embeddings = this.addWeight(\n        'embeddings', [this.inputDim, this.outputDim], this.dtype,\n        this.embeddingsInitializer, this.embeddingsRegularizer, true,\n        this.embeddingsConstraint);\n    this.built = true;\n  }\n\n  // Override warnOnIncompatibleInputShape because an embedding layer allows\n  // the input to have varying ranks.\n  protected warnOnIncompatibleInputShape(inputShape: Shape) {}\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor {\n    return tidy(() => {\n      if (!this.maskZero) {\n        return null;\n      } else {\n        inputs = getExactlyOneTensor(inputs);\n        return notEqual(inputs, zerosLike(inputs));\n      }\n    });\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    if (this.inputLength == null) {\n      return [...inputShape, this.outputDim];\n    }\n    // inputLength can be an array if input is 3D or higher.\n    const inLens: number[] = generic_utils.toList(this.inputLength);\n    if (inLens.length !== inputShape.length - 1) {\n      throw new ValueError(\n          `\"inputLength\" is ${this.inputLength}, but received ` +\n          `input shape has shape ${inputShape}`);\n    } else {\n      let i = 0;\n      for (let k = 0; k < inLens.length; ++k) {\n        const s1 = inLens[k];\n        const s2 = inputShape[k + 1];\n        if ((s1 != null) && (s2 != null) && (s1 !== s2)) {\n          throw new ValueError(\n              `\"inputLength\" is ${this.inputLength}, but received ` +\n              `input shape has shape ${inputShape}`);\n        } else if (s1 == null) {\n          inLens[i] = s2;\n        }\n        i++;\n      }\n    }\n    return [inputShape[0], ...inLens, this.outputDim];\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      // Embedding layer accepts only a single input.\n      let input = getExactlyOneTensor(inputs);\n      if (input.dtype !== 'int32') {\n        input = K.cast(input, 'int32');\n      }\n      const output = K.gather(this.embeddings.read(), input.as1D());\n      return output.reshape(\n          getExactlyOneShape(this.computeOutputShape(input.shape)));\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config = {\n      inputDim: this.inputDim,\n      outputDim: this.outputDim,\n      embeddingsInitializer: serializeInitializer(this.embeddingsInitializer),\n      embeddingsRegularizer: serializeRegularizer(this.embeddingsRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      embeddingsConstraint: serializeConstraint(this.embeddingsConstraint),\n      maskZero: this.maskZero,\n      inputLength: this.inputLength\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Embedding);\n"]}},"error":null,"hash":"cf0672fc8681e2d50a1788aaf0871b5e","cacheData":{"env":{}}}