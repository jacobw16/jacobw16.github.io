{"id":"node_modules/@tensorflow/tfjs-layers/dist/activations.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\activations.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\activations.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581030063848},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":27,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"./backend/tfjs_backend","loc":{"line":28,"column":16},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"./utils/generic_utils","loc":{"line":29,"column":30},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\activations.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n// Layer activation functions\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"./backend/tfjs_backend\");\nvar generic_utils_1 = require(\"./utils/generic_utils\");\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nvar Activation = /** @class */ (function (_super) {\n    __extends(Activation, _super);\n    function Activation() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Activation.prototype.getConfig = function () {\n        return {};\n    };\n    return Activation;\n}(tfjs_core_1.serialization.Serializable));\nexports.Activation = Activation;\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nvar Elu = /** @class */ (function (_super) {\n    __extends(Elu, _super);\n    function Elu() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    /**\n     * Calculate the activation function.\n     *\n     * @param x: Input.\n     * @param alpha: Scaling factor the negative section.\n     * @return Output of the ELU activation.\n     */\n    Elu.prototype.apply = function (x, alpha) {\n        if (alpha === void 0) { alpha = 1; }\n        return K.elu(x, alpha);\n    };\n    /** @nocollapse */\n    Elu.className = 'elu';\n    return Elu;\n}(Activation));\nexports.Elu = Elu;\ntfjs_core_1.serialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nvar Selu = /** @class */ (function (_super) {\n    __extends(Selu, _super);\n    function Selu() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Selu.prototype.apply = function (x) {\n        return tfc.selu(x);\n    };\n    /** @nocollapse */\n    Selu.className = 'selu';\n    return Selu;\n}(Activation));\nexports.Selu = Selu;\ntfjs_core_1.serialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\nvar Relu = /** @class */ (function (_super) {\n    __extends(Relu, _super);\n    function Relu() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Relu.prototype.apply = function (x) {\n        return tfc.relu(x);\n    };\n    /** @nocollapse */\n    Relu.className = 'relu';\n    return Relu;\n}(Activation));\nexports.Relu = Relu;\ntfjs_core_1.serialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nvar Relu6 = /** @class */ (function (_super) {\n    __extends(Relu6, _super);\n    function Relu6() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Relu6.prototype.apply = function (x) {\n        return tfjs_core_1.tidy(function () { return tfc.minimum(6.0, tfc.relu(x)); });\n    };\n    /** @nocollapse */\n    Relu6.className = 'relu6';\n    return Relu6;\n}(Activation));\nexports.Relu6 = Relu6;\ntfjs_core_1.serialization.registerClass(Relu6);\n//* Linear activation (no-op) */\nvar Linear = /** @class */ (function (_super) {\n    __extends(Linear, _super);\n    function Linear() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Linear.prototype.apply = function (x) {\n        return x;\n    };\n    /** @nocollapse */\n    Linear.className = 'linear';\n    return Linear;\n}(Activation));\nexports.Linear = Linear;\ntfjs_core_1.serialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\nvar Sigmoid = /** @class */ (function (_super) {\n    __extends(Sigmoid, _super);\n    function Sigmoid() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Sigmoid.prototype.apply = function (x) {\n        return tfc.sigmoid(x);\n    };\n    /** @nocollapse */\n    Sigmoid.className = 'sigmoid';\n    return Sigmoid;\n}(Activation));\nexports.Sigmoid = Sigmoid;\ntfjs_core_1.serialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nvar HardSigmoid = /** @class */ (function (_super) {\n    __extends(HardSigmoid, _super);\n    function HardSigmoid() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    HardSigmoid.prototype.apply = function (x) {\n        return K.hardSigmoid(x);\n    };\n    /** @nocollapse */\n    HardSigmoid.className = 'hardSigmoid';\n    return HardSigmoid;\n}(Activation));\nexports.HardSigmoid = HardSigmoid;\ntfjs_core_1.serialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\nvar Softplus = /** @class */ (function (_super) {\n    __extends(Softplus, _super);\n    function Softplus() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Softplus.prototype.apply = function (x) {\n        return tfc.softplus(x);\n    };\n    /** @nocollapse */\n    Softplus.className = 'softplus';\n    return Softplus;\n}(Activation));\nexports.Softplus = Softplus;\ntfjs_core_1.serialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\nvar Softsign = /** @class */ (function (_super) {\n    __extends(Softsign, _super);\n    function Softsign() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Softsign.prototype.apply = function (x) {\n        return K.softsign(x);\n    };\n    /** @nocollapse */\n    Softsign.className = 'softsign';\n    return Softsign;\n}(Activation));\nexports.Softsign = Softsign;\ntfjs_core_1.serialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\nvar Tanh = /** @class */ (function (_super) {\n    __extends(Tanh, _super);\n    function Tanh() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    Tanh.prototype.apply = function (x) {\n        return tfc.tanh(x);\n    };\n    /** @nocollapse */\n    Tanh.className = 'tanh';\n    return Tanh;\n}(Activation));\nexports.Tanh = Tanh;\ntfjs_core_1.serialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\nvar Softmax = /** @class */ (function (_super) {\n    __extends(Softmax, _super);\n    function Softmax() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    Softmax.prototype.apply = function (x, axis) {\n        if (axis === void 0) { axis = (-1); }\n        return tfc.softmax(x, axis);\n    };\n    /** @nocollapse */\n    Softmax.className = 'softmax';\n    return Softmax;\n}(Activation));\nexports.Softmax = Softmax;\ntfjs_core_1.serialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\nvar LogSoftmax = /** @class */ (function (_super) {\n    __extends(LogSoftmax, _super);\n    function LogSoftmax() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    /**\n     * Calculate the activation function of log softmax:\n     * log( exp(x_i) / sum(exp(x)) )\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    LogSoftmax.prototype.apply = function (x, axis) {\n        if (axis === void 0) { axis = (-1); }\n        return tfc.logSoftmax(x, axis);\n    };\n    /** @nocollapse */\n    LogSoftmax.className = 'logSoftmax';\n    return LogSoftmax;\n}(Activation));\nexports.LogSoftmax = LogSoftmax;\ntfjs_core_1.serialization.registerClass(LogSoftmax);\nfunction serializeActivation(activation) {\n    return activation.getClassName();\n}\nexports.serializeActivation = serializeActivation;\nfunction deserializeActivation(config, customObjects) {\n    if (customObjects === void 0) { customObjects = {}; }\n    return generic_utils_1.deserializeKerasObject(config, tfjs_core_1.serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexports.deserializeActivation = deserializeActivation;\nfunction getActivation(identifier) {\n    if (identifier == null) {\n        var config = {};\n        config['className'] = 'linear';\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    if (typeof identifier === 'string') {\n        var config = {};\n        config['className'] = identifier;\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    else if (identifier instanceof Activation) {\n        return identifier;\n    }\n    else {\n        return deserializeActivation(identifier);\n    }\n}\nexports.getActivation = getActivation;\n"},"sourceMaps":{"js":{"version":3,"file":"activations.js","sourceRoot":"","sources":["../src/activations.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH,6BAA6B;AAC7B,2CAA6C;AAC7C,mDAAkE;AAClE,0CAA4C;AAE5C,uDAA6D;AAE7D;;;;;;GAMG;AACH;IAAyC,8BAA0B;IAAnE;;IAKA,CAAC;IAHC,8BAAS,GAAT;QACE,OAAO,EAAE,CAAC;IACZ,CAAC;IACH,iBAAC;AAAD,CAAC,AALD,CAAyC,yBAAa,CAAC,YAAY,GAKlE;AALqB,gCAAU;AAOhC;;;GAGG;AACH;IAAyB,uBAAU;IAAnC;;IAaA,CAAC;IAVC;;;;;;OAMG;IACH,mBAAK,GAAL,UAAM,CAAS,EAAE,KAAS;QAAT,sBAAA,EAAA,SAAS;QACxB,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;IACzB,CAAC;IAXD,kBAAkB;IACF,aAAS,GAAG,KAAK,CAAC;IAWpC,UAAC;CAAA,AAbD,CAAyB,UAAU,GAalC;AAbY,kBAAG;AAchB,yBAAa,CAAC,aAAa,CAAC,GAAG,CAAC,CAAC;AAEjC;;;;;;GAMG;AACH;IAA0B,wBAAU;IAApC;;IAMA,CAAC;IAHC,oBAAK,GAAL,UAAM,CAAS;QACb,OAAO,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;IACrB,CAAC;IAJD,kBAAkB;IACF,cAAS,GAAG,MAAM,CAAC;IAIrC,WAAC;CAAA,AAND,CAA0B,UAAU,GAMnC;AANY,oBAAI;AAOjB,yBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG;AACH;IAA0B,wBAAU;IAApC;;IAMA,CAAC;IAHC,oBAAK,GAAL,UAAM,CAAS;QACb,OAAO,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;IACrB,CAAC;IAJD,kBAAkB;IACF,cAAS,GAAG,MAAM,CAAC;IAIrC,WAAC;CAAA,AAND,CAA0B,UAAU,GAMnC;AANY,oBAAI;AAOjB,yBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG;AACH;IAA2B,yBAAU;IAArC;;IAMA,CAAC;IAHC,qBAAK,GAAL,UAAM,CAAS;QACb,OAAO,gBAAI,CAAC,cAAM,OAAA,GAAG,CAAC,OAAO,CAAC,GAAG,EAAE,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAA7B,CAA6B,CAAC,CAAC;IACnD,CAAC;IAJD,kBAAkB;IACF,eAAS,GAAG,OAAO,CAAC;IAItC,YAAC;CAAA,AAND,CAA2B,UAAU,GAMpC;AANY,sBAAK;AAOlB,yBAAa,CAAC,aAAa,CAAC,KAAK,CAAC,CAAC;AAEnC,gCAAgC;AAChC;IAA4B,0BAAU;IAAtC;;IAMA,CAAC;IAHC,sBAAK,GAAL,UAAM,CAAS;QACb,OAAO,CAAC,CAAC;IACX,CAAC;IAJD,kBAAkB;IACF,gBAAS,GAAG,QAAQ,CAAC;IAIvC,aAAC;CAAA,AAND,CAA4B,UAAU,GAMrC;AANY,wBAAM;AAOnB,yBAAa,CAAC,aAAa,CAAC,MAAM,CAAC,CAAC;AAEpC;;GAEG;AACH;IAA6B,2BAAU;IAAvC;;IAMA,CAAC;IAHC,uBAAK,GAAL,UAAM,CAAS;QACb,OAAO,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACxB,CAAC;IAJD,kBAAkB;IACF,iBAAS,GAAG,SAAS,CAAC;IAIxC,cAAC;CAAA,AAND,CAA6B,UAAU,GAMtC;AANY,0BAAO;AAOpB,yBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;GAEG;AACH;IAAiC,+BAAU;IAA3C;;IAMA,CAAC;IAHC,2BAAK,GAAL,UAAM,CAAS;QACb,OAAO,CAAC,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;IAC1B,CAAC;IAJD,kBAAkB;IACF,qBAAS,GAAG,aAAa,CAAC;IAI5C,kBAAC;CAAA,AAND,CAAiC,UAAU,GAM1C;AANY,kCAAW;AAOxB,yBAAa,CAAC,aAAa,CAAC,WAAW,CAAC,CAAC;AAEzC;;GAEG;AACH;IAA8B,4BAAU;IAAxC;;IAMA,CAAC;IAHC,wBAAK,GAAL,UAAM,CAAS;QACb,OAAO,GAAG,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC;IACzB,CAAC;IAJD,kBAAkB;IACF,kBAAS,GAAG,UAAU,CAAC;IAIzC,eAAC;CAAA,AAND,CAA8B,UAAU,GAMvC;AANY,4BAAQ;AAOrB,yBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC;;GAEG;AACH;IAA8B,4BAAU;IAAxC;;IAMA,CAAC;IAHC,wBAAK,GAAL,UAAM,CAAS;QACb,OAAO,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC;IACvB,CAAC;IAJD,kBAAkB;IACF,kBAAS,GAAG,UAAU,CAAC;IAIzC,eAAC;CAAA,AAND,CAA8B,UAAU,GAMvC;AANY,4BAAQ;AAOrB,yBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC;;GAEG;AACH;IAA0B,wBAAU;IAApC;;IAMA,CAAC;IAHC,oBAAK,GAAL,UAAM,CAAS;QACb,OAAO,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;IACrB,CAAC;IAJD,kBAAkB;IACF,cAAS,GAAG,MAAM,CAAC;IAIrC,WAAC;CAAA,AAND,CAA0B,UAAU,GAMnC;AANY,oBAAI;AAOjB,yBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG;AACH;IAA6B,2BAAU;IAAvC;;IAkBA,CAAC;IAfC;;;;;;;;;;;OAWG;IACH,uBAAK,GAAL,UAAM,CAAS,EAAE,IAAmB;QAAnB,qBAAA,EAAA,QAAgB,CAAC,CAAC,CAAC;QAClC,OAAO,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IAC9B,CAAC;IAhBD,kBAAkB;IACF,iBAAS,GAAG,SAAS,CAAC;IAgBxC,cAAC;CAAA,AAlBD,CAA6B,UAAU,GAkBtC;AAlBY,0BAAO;AAmBpB,yBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;GAEG;AACH;IAAgC,8BAAU;IAA1C;;IAmBA,CAAC;IAhBC;;;;;;;;;;;;OAYG;IACH,0BAAK,GAAL,UAAM,CAAS,EAAE,IAAmB;QAAnB,qBAAA,EAAA,QAAgB,CAAC,CAAC,CAAC;QAClC,OAAO,GAAG,CAAC,UAAU,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IACjC,CAAC;IAjBD,kBAAkB;IACF,oBAAS,GAAG,YAAY,CAAC;IAiB3C,iBAAC;CAAA,AAnBD,CAAgC,UAAU,GAmBzC;AAnBY,gCAAU;AAoBvB,yBAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;AAExC,SAAgB,mBAAmB,CAAC,UAAsB;IACxD,OAAO,UAAU,CAAC,YAAY,EAAE,CAAC;AACnC,CAAC;AAFD,kDAEC;AAED,SAAgB,qBAAqB,CAClC,MAAgC,EAChC,aAA4C;IAA5C,8BAAA,EAAA,kBAA4C;IAC7C,OAAO,sCAAsB,CACzB,MAAM,EAAE,yBAAa,CAAC,gBAAgB,CAAC,MAAM,EAAE,CAAC,YAAY,EAC5D,aAAa,EAAE,YAAY,CAAC,CAAC;AACnC,CAAC;AAND,sDAMC;AAED,SAAgB,aAAa,CAAC,UACmC;IAC/D,IAAI,UAAU,IAAI,IAAI,EAAE;QACtB,IAAM,MAAM,GAA6B,EAAE,CAAC;QAC5C,MAAM,CAAC,WAAW,CAAC,GAAG,QAAQ,CAAC;QAC/B,MAAM,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC;QACtB,OAAO,qBAAqB,CAAC,MAAM,CAAC,CAAC;KACtC;IACD,IAAI,OAAO,UAAU,KAAK,QAAQ,EAAE;QAClC,IAAM,MAAM,GAA6B,EAAE,CAAC;QAC5C,MAAM,CAAC,WAAW,CAAC,GAAG,UAAU,CAAC;QACjC,MAAM,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC;QACtB,OAAO,qBAAqB,CAAC,MAAM,CAAC,CAAC;KACtC;SAAM,IAAI,UAAU,YAAY,UAAU,EAAE;QAC3C,OAAO,UAAU,CAAC;KACnB;SAAM;QACL,OAAO,qBAAqB,CAAC,UAAU,CAAC,CAAC;KAC1C;AACH,CAAC;AAlBD,sCAkBC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport {ActivationIdentifier} from './keras_format/activation_config';\nimport {deserializeKerasObject} from './utils/generic_utils';\n\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport abstract class Activation extends serialization.Serializable {\n  abstract apply(tensor: Tensor, axis?: number): Tensor;\n  getConfig(): serialization.ConfigDict {\n    return {};\n  }\n}\n\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'elu';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return K.elu(x, alpha);\n  }\n}\nserialization.registerClass(Elu);\n\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'selu';\n  apply(x: Tensor): Tensor {\n    return tfc.selu(x);\n  }\n}\nserialization.registerClass(Selu);\n\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu';\n  apply(x: Tensor): Tensor {\n    return tfc.relu(x);\n  }\n}\nserialization.registerClass(Relu);\n\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu6';\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n}\nserialization.registerClass(Relu6);\n\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n  /** @nocollapse */\n  static readonly className = 'linear';\n  apply(x: Tensor): Tensor {\n    return x;\n  }\n}\nserialization.registerClass(Linear);\n\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'sigmoid';\n  apply(x: Tensor): Tensor {\n    return tfc.sigmoid(x);\n  }\n}\nserialization.registerClass(Sigmoid);\n\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'hardSigmoid';\n  apply(x: Tensor): Tensor {\n    return K.hardSigmoid(x);\n  }\n}\nserialization.registerClass(HardSigmoid);\n\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softplus';\n  apply(x: Tensor): Tensor {\n    return tfc.softplus(x);\n  }\n}\nserialization.registerClass(Softplus);\n\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softsign';\n  apply(x: Tensor): Tensor {\n    return K.softsign(x);\n  }\n}\nserialization.registerClass(Softsign);\n\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n  /** @nocollapse */\n  static readonly className = 'tanh';\n  apply(x: Tensor): Tensor {\n    return tfc.tanh(x);\n  }\n}\nserialization.registerClass(Tanh);\n\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softmax';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.softmax(x, axis);\n  }\n}\nserialization.registerClass(Softmax);\n\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'logSoftmax';\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.logSoftmax(x, axis);\n  }\n}\nserialization.registerClass(LogSoftmax);\n\nexport function serializeActivation(activation: Activation): string {\n  return activation.getClassName();\n}\n\nexport function deserializeActivation(\n   config: serialization.ConfigDict,\n   customObjects: serialization.ConfigDict = {}): Activation {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'activation');\n}\n\nexport function getActivation(identifier: ActivationIdentifier|\n                              serialization.ConfigDict|Activation): Activation {\n  if (identifier == null) {\n    const config: serialization.ConfigDict = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n  if (typeof identifier === 'string') {\n    const config: serialization.ConfigDict = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}\n"]}},"error":null,"hash":"5eec02e2cff83e816b686181e761a768","cacheData":{"env":{}}}