{"id":"node_modules/@tensorflow/tfjs-layers/dist/layers/noise.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\noise.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\noise.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581030063848},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":28,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\noise.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../backend/tfjs_backend","loc":{"line":29,"column":16},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\noise.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"../engine/topology","loc":{"line":30,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\noise.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"../utils/types_utils","loc":{"line":31,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\noise.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"../backend/tfjs_backend\");\nvar topology_1 = require(\"../engine/topology\");\nvar types_utils_1 = require(\"../utils/types_utils\");\nvar GaussianNoise = /** @class */ (function (_super) {\n    __extends(GaussianNoise, _super);\n    function GaussianNoise(args) {\n        var _this = _super.call(this, args) || this;\n        _this.supportsMasking = true;\n        _this.stddev = args.stddev;\n        return _this;\n    }\n    GaussianNoise.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    GaussianNoise.prototype.getConfig = function () {\n        var baseConfig = _super.prototype.getConfig.call(this);\n        var config = { stddev: this.stddev };\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    GaussianNoise.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            _this.invokeCallHook(inputs, kwargs);\n            var input = types_utils_1.getExactlyOneTensor(inputs);\n            var noised = function () {\n                return K.randomNormal(input.shape, 0, _this.stddev).add(input);\n            };\n            var output = K.inTrainPhase(noised, function () { return input; }, kwargs['training'] || false);\n            return output;\n        });\n    };\n    /** @nocollapse */\n    GaussianNoise.className = 'GaussianNoise';\n    return GaussianNoise;\n}(topology_1.Layer));\nexports.GaussianNoise = GaussianNoise;\ntfjs_core_1.serialization.registerClass(GaussianNoise);\nvar GaussianDropout = /** @class */ (function (_super) {\n    __extends(GaussianDropout, _super);\n    function GaussianDropout(args) {\n        var _this = _super.call(this, args) || this;\n        _this.supportsMasking = true;\n        _this.rate = args.rate;\n        return _this;\n    }\n    GaussianDropout.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    GaussianDropout.prototype.getConfig = function () {\n        var baseConfig = _super.prototype.getConfig.call(this);\n        var config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    GaussianDropout.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            _this.invokeCallHook(inputs, kwargs);\n            var input = types_utils_1.getExactlyOneTensor(inputs);\n            if (_this.rate > 0 && _this.rate < 1) {\n                var noised = function () {\n                    var stddev = Math.sqrt(_this.rate / (1 - _this.rate));\n                    return input.mul(K.randomNormal(input.shape, 1, stddev));\n                };\n                return K.inTrainPhase(noised, function () { return input; }, kwargs['training'] || false);\n            }\n            return input;\n        });\n    };\n    /** @nocollapse */\n    GaussianDropout.className = 'GaussianDropout';\n    return GaussianDropout;\n}(topology_1.Layer));\nexports.GaussianDropout = GaussianDropout;\ntfjs_core_1.serialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nvar AlphaDropout = /** @class */ (function (_super) {\n    __extends(AlphaDropout, _super);\n    function AlphaDropout(args) {\n        var _this = _super.call(this, args) || this;\n        _this.supportsMasking = true;\n        _this.rate = args.rate;\n        _this.noiseShape = args.noiseShape;\n        return _this;\n    }\n    AlphaDropout.prototype._getNoiseShape = function (inputs) {\n        return this.noiseShape || types_utils_1.getExactlyOneTensor(inputs).shape;\n    };\n    AlphaDropout.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    AlphaDropout.prototype.getConfig = function () {\n        var baseConfig = _super.prototype.getConfig.call(this);\n        var config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    AlphaDropout.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            if (_this.rate < 1 && _this.rate > 0) {\n                var noiseShape_1 = _this._getNoiseShape(inputs);\n                var droppedInputs = function () {\n                    var input = types_utils_1.getExactlyOneTensor(inputs);\n                    var alpha = 1.6732632423543772848170429916717;\n                    var scale = 1.0507009873554804934193349852946;\n                    var alphaP = -alpha * scale;\n                    var keptIdx = tfjs_core_1.greaterEqual(tfjs_core_1.randomUniform(noiseShape_1), _this.rate);\n                    keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n                    // Get affine transformation params.\n                    var a = Math.pow(((1 - _this.rate) * (1 + _this.rate * Math.pow(alphaP, 2))), -0.5);\n                    var b = -a * alphaP * _this.rate;\n                    // Apply mask.\n                    var x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\n                    return x.mul(a).add(b);\n                };\n                return K.inTrainPhase(droppedInputs, function () { return types_utils_1.getExactlyOneTensor(inputs); }, kwargs['training'] || false);\n            }\n            return inputs;\n        });\n    };\n    /** @nocollapse */\n    AlphaDropout.className = 'AlphaDropout';\n    return AlphaDropout;\n}(topology_1.Layer));\nexports.AlphaDropout = AlphaDropout;\ntfjs_core_1.serialization.registerClass(AlphaDropout);\n"},"sourceMaps":{"js":{"version":3,"file":"noise.js","sourceRoot":"","sources":["../../src/layers/noise.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH;;GAEG;AAEH,mDAA+F;AAE/F,2CAA6C;AAC7C,+CAAoD;AAGpD,oDAAyD;AAOzD;IAAmC,iCAAK;IAKtC,uBAAY,IAAuB;QAAnC,YACE,kBAAM,IAAI,CAAC,SAGZ;QAFC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC;;IAC5B,CAAC;IAED,0CAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,iCAAS,GAAT;QACE,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,IAAM,MAAM,GAAG,EAAC,MAAM,EAAE,IAAI,CAAC,MAAM,EAAC,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,4BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAUC;QATC,OAAO,gBAAI,CAAC;YACV,KAAI,CAAC,cAAc,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;YACpC,IAAM,KAAK,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;YAC1C,IAAM,MAAM,GAAG;gBACX,OAAA,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,EAAE,KAAI,CAAC,MAAM,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC;YAAtD,CAAsD,CAAC;YAC3D,IAAM,MAAM,GACR,CAAC,CAAC,YAAY,CAAC,MAAM,EAAE,cAAM,OAAA,KAAK,EAAL,CAAK,EAAE,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC,CAAC;YACrE,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IA/BD,kBAAkB;IACX,uBAAS,GAAG,eAAe,CAAC;IA+BrC,oBAAC;CAAA,AAjCD,CAAmC,gBAAK,GAiCvC;AAjCY,sCAAa;AAkC1B,yBAAa,CAAC,aAAa,CAAC,aAAa,CAAC,CAAC;AAO3C;IAAqC,mCAAK;IAKxC,yBAAY,IAAyB;QAArC,YACE,kBAAM,IAAI,CAAC,SAGZ;QAFC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;;IACxB,CAAC;IAED,4CAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,mCAAS,GAAT;QACE,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,IAAM,MAAM,GAAG,EAAC,IAAI,EAAE,IAAI,CAAC,IAAI,EAAC,CAAC;QACjC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,8BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAaC;QAZC,OAAO,gBAAI,CAAC;YACV,KAAI,CAAC,cAAc,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;YACpC,IAAM,KAAK,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;YAC1C,IAAI,KAAI,CAAC,IAAI,GAAG,CAAC,IAAI,KAAI,CAAC,IAAI,GAAG,CAAC,EAAE;gBAClC,IAAM,MAAM,GAAG;oBACb,IAAM,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,KAAI,CAAC,IAAI,GAAG,CAAC,CAAC,GAAG,KAAI,CAAC,IAAI,CAAC,CAAC,CAAC;oBACtD,OAAO,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,KAAK,EAAE,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC;gBAC3D,CAAC,CAAC;gBACF,OAAO,CAAC,CAAC,YAAY,CAAC,MAAM,EAAE,cAAM,OAAA,KAAK,EAAL,CAAK,EAAE,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC,CAAC;aACzE;YACD,OAAO,KAAK,CAAC;QACf,CAAC,CAAC,CAAC;IACL,CAAC;IAlCD,kBAAkB;IACX,yBAAS,GAAG,iBAAiB,CAAC;IAkCvC,sBAAC;CAAA,AApCD,CAAqC,gBAAK,GAoCzC;AApCY,0CAAe;AAqC5B,yBAAa,CAAC,aAAa,CAAC,eAAe,CAAC,CAAC;AAY7C;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA4BG;AACH;IAAkC,gCAAK;IAMrC,sBAAY,IAAsB;QAAlC,YACE,kBAAM,IAAI,CAAC,SAIZ;QAHC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;QACtB,KAAI,CAAC,UAAU,GAAG,IAAI,CAAC,UAAU,CAAC;;IACpC,CAAC;IAED,qCAAc,GAAd,UAAe,MAAuB;QACpC,OAAO,IAAI,CAAC,UAAU,IAAI,iCAAmB,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC;IAC9D,CAAC;IAED,yCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,gCAAS,GAAT;QACE,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,IAAM,MAAM,GAAG,EAAC,IAAI,EAAE,IAAI,CAAC,IAAI,EAAC,CAAC;QACjC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,2BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAgCC;QA/BC,OAAO,gBAAI,CAAC;YACV,IAAI,KAAI,CAAC,IAAI,GAAG,CAAC,IAAI,KAAI,CAAC,IAAI,GAAG,CAAC,EAAE;gBAClC,IAAM,YAAU,GAAG,KAAI,CAAC,cAAc,CAAC,MAAM,CAAC,CAAC;gBAE/C,IAAM,aAAa,GAAG;oBACpB,IAAM,KAAK,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;oBAE1C,IAAM,KAAK,GAAG,iCAAiC,CAAC;oBAChD,IAAM,KAAK,GAAG,iCAAiC,CAAC;oBAEhD,IAAM,MAAM,GAAG,CAAC,KAAK,GAAG,KAAK,CAAC;oBAE9B,IAAI,OAAO,GAAG,wBAAY,CAAC,yBAAa,CAAC,YAAU,CAAC,EAAE,KAAI,CAAC,IAAI,CAAC,CAAC;oBAEjE,OAAO,GAAG,CAAC,CAAC,IAAI,CAAC,OAAO,EAAE,SAAS,CAAC,CAAC,CAAE,qBAAqB;oBAE5D,oCAAoC;oBACpC,IAAM,CAAC,GAAG,SAAA,CAAC,CAAC,CAAC,GAAG,KAAI,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,GAAG,KAAI,CAAC,IAAI,GAAG,SAAA,MAAM,EAAI,CAAC,CAAA,CAAC,CAAC,EAAI,CAAC,GAAG,CAAA,CAAC;oBACpE,IAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM,GAAG,KAAI,CAAC,IAAI,CAAC;oBAElC,cAAc;oBACd,IAAM,CAAC,GAAG,KAAK,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC;oBAE9D,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;gBACzB,CAAC,CAAC;gBACF,OAAO,CAAC,CAAC,YAAY,CACjB,aAAa,EAAE,cAAM,OAAA,iCAAmB,CAAC,MAAM,CAAC,EAA3B,CAA2B,EAChD,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC,CAAC;aAClC;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IA3DD,kBAAkB;IACX,sBAAS,GAAG,cAAc,CAAC;IA2DpC,mBAAC;CAAA,AA7DD,CAAkC,gBAAK,GA6DtC;AA7DY,oCAAY;AA8DzB,yBAAa,CAAC,aAAa,CAAC,YAAY,CAAC,CAAC","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\r\n\r\n/**\r\n * TensorFlow.js Layers: Noise Layers.\r\n */\r\n\r\nimport {greaterEqual, randomUniform, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\r\n\r\nimport * as K from '../backend/tfjs_backend';\r\nimport {Layer, LayerArgs} from '../engine/topology';\r\nimport {Shape} from '../keras_format/common';\r\nimport {Kwargs} from '../types';\r\nimport {getExactlyOneTensor} from '../utils/types_utils';\r\n\r\nexport declare interface GaussianNoiseArgs extends LayerArgs {\r\n  /** Standard Deviation.  */\r\n  stddev: number;\r\n}\r\n\r\nexport class GaussianNoise extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'GaussianNoise';\r\n  readonly stddev: number;\r\n\r\n  constructor(args: GaussianNoiseArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.stddev = args.stddev;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {stddev: this.stddev};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      this.invokeCallHook(inputs, kwargs);\r\n      const input = getExactlyOneTensor(inputs);\r\n      const noised = () =>\r\n          K.randomNormal(input.shape, 0, this.stddev).add(input);\r\n      const output =\r\n          K.inTrainPhase(noised, () => input, kwargs['training'] || false);\r\n      return output;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(GaussianNoise);\r\n\r\nexport declare interface GaussianDropoutArgs extends LayerArgs {\r\n  /** drop probability.  */\r\n  rate: number;\r\n}\r\n\r\nexport class GaussianDropout extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'GaussianDropout';\r\n  readonly rate: number;\r\n\r\n  constructor(args: GaussianDropoutArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.rate = args.rate;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {rate: this.rate};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      this.invokeCallHook(inputs, kwargs);\r\n      const input = getExactlyOneTensor(inputs);\r\n      if (this.rate > 0 && this.rate < 1) {\r\n        const noised = () => {\r\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\r\n          return input.mul(K.randomNormal(input.shape, 1, stddev));\r\n        };\r\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\r\n      }\r\n      return input;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(GaussianDropout);\r\n\r\nexport declare interface AlphaDropoutArgs extends LayerArgs {\r\n  /** drop probability.  */\r\n  rate: number;\r\n  /**\r\n   * A 1-D `Tensor` of type `int32`, representing the\r\n   * shape for randomly generated keep/drop flags.\r\n   */\r\n  noiseShape?: Shape;\r\n}\r\n\r\n/**\r\n * Applies Alpha Dropout to the input.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\r\n * to their original values, in order to ensure the self-normalizing property\r\n * even after this dropout.\r\n * Alpha Dropout fits well to Scaled Exponential Linear Units\r\n * by randomly setting activations to the negative saturation value.\r\n *\r\n * Arguments:\r\n *   - `rate`: float, drop probability (as with `Dropout`).\r\n *     The multiplicative noise will have\r\n *     standard deviation `sqrt(rate / (1 - rate))`.\r\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\r\n *     shape for randomly generated keep/drop flags.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n */\r\nexport class AlphaDropout extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'AlphaDropout';\r\n  readonly rate: number;\r\n  readonly noiseShape: Shape;\r\n\r\n  constructor(args: AlphaDropoutArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.rate = args.rate;\r\n    this.noiseShape = args.noiseShape;\r\n  }\r\n\r\n  _getNoiseShape(inputs: Tensor|Tensor[]) {\r\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {rate: this.rate};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      if (this.rate < 1 && this.rate > 0) {\r\n        const noiseShape = this._getNoiseShape(inputs);\r\n\r\n        const droppedInputs = () => {\r\n          const input = getExactlyOneTensor(inputs);\r\n\r\n          const alpha = 1.6732632423543772848170429916717;\r\n          const scale = 1.0507009873554804934193349852946;\r\n\r\n          const alphaP = -alpha * scale;\r\n\r\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\r\n\r\n          keptIdx = K.cast(keptIdx, 'float32');  // get default dtype.\r\n\r\n          // Get affine transformation params.\r\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\r\n          const b = -a * alphaP * this.rate;\r\n\r\n          // Apply mask.\r\n          const x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\r\n\r\n          return x.mul(a).add(b);\r\n        };\r\n        return K.inTrainPhase(\r\n            droppedInputs, () => getExactlyOneTensor(inputs),\r\n            kwargs['training'] || false);\r\n      }\r\n      return inputs;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(AlphaDropout);\r\n"]}},"error":null,"hash":"fe8d4513744248614c0a8dcb4b32dfd6","cacheData":{"env":{}}}