{"id":"node_modules/@tensorflow/tfjs-layers/dist/layers/normalization.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\normalization.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1577649187475},{"name":"@tensorflow/tfjs-core","loc":{"line":29,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../constraints","loc":{"line":30,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\constraints.js"},{"name":"../engine/topology","loc":{"line":31,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"../errors","loc":{"line":32,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../initializers","loc":{"line":33,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\initializers.js"},{"name":"../regularizers","loc":{"line":34,"column":29},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\regularizers.js"},{"name":"../utils/generic_utils","loc":{"line":35,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"},{"name":"../utils/math_utils","loc":{"line":36,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\math_utils.js"},{"name":"../utils/types_utils","loc":{"line":37,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\normalization.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Normalization layers.\n */\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar constraints_1 = require(\"../constraints\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar initializers_1 = require(\"../initializers\");\nvar regularizers_1 = require(\"../regularizers\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar math_utils = require(\"../utils/math_utils\");\nvar types_utils_1 = require(\"../utils/types_utils\");\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nfunction batchNormalization(x, mean, variance, beta, gamma, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    var out;\n    if (x.rank === 2) {\n        out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 3) {\n        // TODO(cais): Check rank; give proper error message.\n        out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 4) {\n        out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else {\n        throw new errors_1.NotImplementedError(\"batchNormalization is not implemented for array of rank \" + x.rank + \" \" +\n            \"yet\");\n    }\n    return out;\n}\nexports.batchNormalization = batchNormalization;\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    return tfjs_core_1.tidy(function () {\n        var meanAndVariance = tfc.moments(x, reductionAxes);\n        var mean = meanAndVariance.mean;\n        var variance = meanAndVariance.variance;\n        var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    return tfjs_core_1.tidy(function () {\n        var meanAndVariance = tfc.moments(x, reductionAxes);\n        var mean = meanAndVariance.mean;\n        var variance = meanAndVariance.variance;\n        var targetShape = [];\n        for (var _i = 0, _a = math_utils.range(0, x.rank); _i < _a.length; _i++) {\n            var axis = _a[_i];\n            if (reductionAxes.indexOf(axis) !== -1) {\n                targetShape.push(1);\n            }\n            else {\n                targetShape.push(x.shape[axis]);\n            }\n        }\n        var broadcastMean = mean.reshape(targetShape);\n        var broadcastVariance = variance.reshape(targetShape);\n        var broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n        var broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n        var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    if (tfjs_core_1.util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n        return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n    else {\n        return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n}\nexports.normalizeBatchInTraining = normalizeBatchInTraining;\nvar BatchNormalization = /** @class */ (function (_super) {\n    __extends(BatchNormalization, _super);\n    function BatchNormalization(args) {\n        var _this = this;\n        if (args == null) {\n            args = {};\n        }\n        _this = _super.call(this, args) || this;\n        _this.supportsMasking = true;\n        _this.axis = args.axis == null ? -1 : args.axis;\n        _this.momentum = args.momentum == null ? 0.99 : args.momentum;\n        _this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        _this.center = args.center == null ? true : args.center;\n        _this.scale = args.scale == null ? true : args.scale;\n        _this.betaInitializer = initializers_1.getInitializer(args.betaInitializer || 'zeros');\n        _this.gammaInitializer = initializers_1.getInitializer(args.gammaInitializer || 'ones');\n        _this.movingMeanInitializer =\n            initializers_1.getInitializer(args.movingMeanInitializer || 'zeros');\n        _this.movingVarianceInitializer =\n            initializers_1.getInitializer(args.movingVarianceInitializer || 'ones');\n        _this.betaConstraint = constraints_1.getConstraint(args.betaConstraint);\n        _this.gammaConstraint = constraints_1.getConstraint(args.gammaConstraint);\n        _this.betaRegularizer = regularizers_1.getRegularizer(args.betaRegularizer);\n        _this.gammaRegularizer = regularizers_1.getRegularizer(args.gammaRegularizer);\n        return _this;\n    }\n    BatchNormalization.prototype.build = function (inputShape) {\n        var _a;\n        inputShape = types_utils_1.getExactlyOneShape(inputShape);\n        var axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n        var dim = inputShape[axis];\n        if (dim == null) {\n            throw new errors_1.ValueError(\"Axis \" + axis + \" of input tensor should have a defined dimension but \" +\n                \"the layer received an input with shape \" +\n                (JSON.stringify(inputShape) + \".\"));\n        }\n        this.inputSpec =\n            [new topology_1.InputSpec({ ndim: inputShape.length, axes: (_a = {}, _a[axis] = dim, _a) })];\n        var shape = [dim];\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n        }\n        this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n        this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n        this.built = true;\n    };\n    BatchNormalization.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            var training = kwargs['training'] == null ? false : kwargs['training'];\n            var input = types_utils_1.getExactlyOneTensor(inputs);\n            var inputShape = input.shape;\n            var ndim = inputShape.length;\n            var reductionAxes = math_utils.range(0, ndim);\n            var axis = _this.axis >= 0 ? _this.axis : (_this.axis + ndim);\n            reductionAxes.splice(axis, 1);\n            var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n            broadcastShape[axis] = inputShape[axis];\n            var sortedReductionAxes = reductionAxes.slice();\n            sortedReductionAxes.sort();\n            var needsBroadcasting = !tfjs_core_1.util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n            var normalizeInference = function () {\n                if (needsBroadcasting) {\n                    var broadcastMovingMean = _this.movingMean.read().reshape(broadcastShape);\n                    var broadcastMovingVariance = _this.movingVariance.read().reshape(broadcastShape);\n                    var broadcastBeta = _this.center ? _this.beta.read().reshape(broadcastShape) : null;\n                    var broadcastGamma = _this.scale ? _this.gamma.read().reshape(broadcastShape) : null;\n                    return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this.epsilon);\n                }\n                else {\n                    return batchNormalization(input, _this.movingMean.read(), _this.movingVariance.read(), _this.beta == null ? null : _this.beta.read(), _this.gamma == null ? null : _this.gamma.read(), _this.epsilon);\n                }\n            };\n            if (!training) {\n                return normalizeInference();\n            }\n            var _a = normalizeBatchInTraining(input, _this.gamma.read(), _this.beta.read(), reductionAxes, _this.epsilon), normedTraining = _a[0], mean = _a[1], variance = _a[2];\n            var doMovingAverage = function (variable, value, momentum) {\n                tfc.tidy(function () {\n                    var decay = 1 - momentum;\n                    var origValue = variable.read();\n                    var updateDelta = origValue.sub(value).mul(decay);\n                    variable.write(origValue.sub(updateDelta));\n                });\n            };\n            // Perform updates to moving mean and moving variance for training.\n            // Porting Note: In PyKeras, these updates to `movingMean` and\n            //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n            //   `update`s using the `add_update()` method. Here we do it imperatively\n            //   and encapsulate the updates in a function that is invoked\n            //   immediately.\n            var updateMovingMeanAndVariance = function () {\n                doMovingAverage(_this.movingMean, mean, _this.momentum);\n                doMovingAverage(_this.movingVariance, variance, _this.momentum);\n            };\n            updateMovingMeanAndVariance();\n            return normedTraining;\n        });\n    };\n    BatchNormalization.prototype.getConfig = function () {\n        var config = {\n            axis: this.axis,\n            momentum: this.momentum,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: initializers_1.serializeInitializer(this.betaInitializer),\n            gammaInitializer: initializers_1.serializeInitializer(this.gammaInitializer),\n            movingMeanInitializer: initializers_1.serializeInitializer(this.movingMeanInitializer),\n            movingVarianceInitializer: initializers_1.serializeInitializer(this.movingVarianceInitializer),\n            betaRegularizer: regularizers_1.serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: regularizers_1.serializeRegularizer(this.gammaRegularizer),\n            betaConstraint: constraints_1.serializeConstraint(this.betaConstraint),\n            gammaConstraint: constraints_1.serializeConstraint(this.gammaConstraint)\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    BatchNormalization.className = 'BatchNormalization';\n    return BatchNormalization;\n}(topology_1.Layer));\nexports.BatchNormalization = BatchNormalization;\ntfjs_core_1.serialization.registerClass(BatchNormalization);\nvar LayerNormalization = /** @class */ (function (_super) {\n    __extends(LayerNormalization, _super);\n    function LayerNormalization(args) {\n        var _this = this;\n        if (args == null) {\n            args = {};\n        }\n        _this = _super.call(this, args) || this;\n        _this.axis = args.axis == null ? -1 : args.axis;\n        if (typeof _this.axis === 'number') {\n            if (!Number.isInteger(_this.axis)) {\n                throw new Error(\"Expected axis to be an integer, but received \" + _this.axis);\n            }\n        }\n        else if (Array.isArray(_this.axis)) {\n            for (var _i = 0, _a = _this.axis; _i < _a.length; _i++) {\n                var axis = _a[_i];\n                if (!Number.isInteger(axis)) {\n                    throw new Error(\"Expected axis to be an array of integers, \" +\n                        (\"but received \" + JSON.stringify(_this.axis)));\n                }\n            }\n        }\n        else {\n            throw new Error(\"Expected axis to be an integer or an array of integers, \" +\n                (\"but received \" + JSON.stringify(_this.axis)));\n        }\n        _this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        _this.center = args.center == null ? true : args.center;\n        _this.scale = args.scale == null ? true : args.scale;\n        _this.betaInitializer = initializers_1.getInitializer(args.betaInitializer || 'zeros');\n        _this.gammaInitializer = initializers_1.getInitializer(args.gammaInitializer || 'ones');\n        _this.betaRegularizer = regularizers_1.getRegularizer(args.betaRegularizer);\n        _this.gammaRegularizer = regularizers_1.getRegularizer(args.gammaRegularizer);\n        _this.supportsMasking = true;\n        return _this;\n    }\n    LayerNormalization.prototype.build = function (inputShape) {\n        inputShape = types_utils_1.getExactlyOneShape(inputShape);\n        var nDims = inputShape.length;\n        // Convert axis to array and resolve negatives.\n        if (typeof this.axis === 'number') {\n            this.axis = [this.axis];\n        }\n        for (var i = 0; i < this.axis.length; ++i) {\n            if (this.axis[i] < 0) {\n                this.axis[i] += nDims;\n            }\n        }\n        // Further validate axes.\n        for (var _i = 0, _a = this.axis; _i < _a.length; _i++) {\n            var axis = _a[_i];\n            if (axis < 0 || axis >= nDims) {\n                throw new Error(\"Invalid axis: \" + axis);\n            }\n        }\n        if (this.axis.length !== generic_utils.unique(this.axis).length) {\n            throw new Error(\"Found duplicate axes in: \" + this.axis);\n        }\n        var paramShape = this.axis.map(function (axis) { return inputShape[axis]; });\n        var trainable = true;\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n        }\n        else {\n            this.gamma = null;\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n        }\n        else {\n            this.beta = null;\n        }\n        this.built = true;\n    };\n    LayerNormalization.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        var input = types_utils_1.getExactlyOneTensor(inputs);\n        var inputShape = input.shape;\n        var nDims = inputShape.length;\n        return tfjs_core_1.tidy(function () {\n            var keepDims = true;\n            var _a = tfjs_core_1.moments(input, _this.axis, keepDims), mean = _a.mean, variance = _a.variance;\n            var broadcastShape = generic_utils.pyListRepeat(1, nDims);\n            for (var _i = 0, _b = _this.axis; _i < _b.length; _i++) {\n                var dim = _b[_i];\n                broadcastShape[dim] = inputShape[dim];\n            }\n            var broadcast = function (v) {\n                if (v != null && v.shape.length !== nDims &&\n                    _this.axis !== [nDims - 1]) {\n                    return v.reshape(broadcastShape);\n                }\n                else {\n                    return v;\n                }\n            };\n            var scale = broadcast(_this.gamma.read());\n            var offset = broadcast(_this.beta.read());\n            // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n            // is a workaround for the limitation of core's batchNormalization?d don't\n            // support broadcasting in their gradients. In addition, the tiling is\n            // necessary to ensure correctness on the browser CPU backend regardless\n            // of forward or backward computation. Remove this workaround once the\n            // limitation is addressed. See .\n            var momentsTiling = [];\n            var scaleOffsetTiling = [];\n            for (var i = 0; i < nDims; ++i) {\n                if (_this.axis.indexOf(i) !== -1) {\n                    momentsTiling.push(inputShape[i]);\n                    scaleOffsetTiling.push(1);\n                }\n                else {\n                    momentsTiling.push(1);\n                    scaleOffsetTiling.push(inputShape[i]);\n                }\n            }\n            mean = mean.tile(momentsTiling);\n            variance = variance.tile(momentsTiling);\n            scale = scale.tile(scaleOffsetTiling);\n            offset = offset.tile(scaleOffsetTiling);\n            return batchNormalization(input, mean, variance, offset, scale, _this.epsilon);\n        });\n    };\n    LayerNormalization.prototype.getConfig = function () {\n        var config = {\n            axis: this.axis,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: initializers_1.serializeInitializer(this.betaInitializer),\n            gammaInitializer: initializers_1.serializeInitializer(this.gammaInitializer),\n            betaRegularizer: regularizers_1.serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: regularizers_1.serializeRegularizer(this.gammaRegularizer)\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    LayerNormalization.className = 'LayerNormalization';\n    return LayerNormalization;\n}(topology_1.Layer));\nexports.LayerNormalization = LayerNormalization;\ntfjs_core_1.serialization.registerClass(LayerNormalization);\n"},"sourceMaps":{"js":{"version":3,"file":"normalization.js","sourceRoot":"","sources":["../../src/layers/normalization.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH;;GAEG;AAEH,2CAA6C;AAC7C,mDAAyH;AAEzH,8CAAoG;AACpG,+CAA+D;AAC/D,oCAA0D;AAC1D,gDAAyG;AAEzG,gDAAyG;AAEzG,sDAAwD;AACxD,gDAAkD;AAClD,oDAA6E;AAG7E;;;;;;;;;;;;;GAaG;AACH,SAAgB,kBAAkB,CAC9B,CAAS,EAAE,IAAY,EAAE,QAAgB,EAAE,IAAa,EAAE,KAAc,EACxE,OAAc;IAAd,wBAAA,EAAA,cAAc;IAChB,IAAI,GAAW,CAAC;IAChB,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAAE;QAChB,GAAG,GAAG,GAAG,CAAC,WAAW,CACjB,CAAa,EAAE,IAA2B,EAC1C,QAA+B,EAAE,IAA2B,EAC5D,KAA4B,EAAE,OAAO,CAAC,CAAC;KAC5C;SAAM,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAAE;QACvB,qDAAqD;QACrD,GAAG,GAAG,GAAG,CAAC,WAAW,CACjB,CAAa,EAAE,IAA2B,EAC1C,QAA+B,EAAE,IAA2B,EAC5D,KAA4B,EAAE,OAAO,CAAC,CAAC;KAC5C;SAAM,IAAI,CAAC,CAAC,IAAI,KAAK,CAAC,EAAE;QACvB,GAAG,GAAG,GAAG,CAAC,WAAW,CACjB,CAAa,EAAE,IAA2B,EAC1C,QAA+B,EAAE,IAA2B,EAC5D,KAA4B,EAAE,OAAO,CAAC,CAAC;KAC5C;SAAM;QACL,MAAM,IAAI,4BAAmB,CACzB,6DAA2D,CAAC,CAAC,IAAI,MAAG;YACpE,KAAK,CAAC,CAAC;KACZ;IACD,OAAO,GAAG,CAAC;AACb,CAAC;AA1BD,gDA0BC;AAED;;;;;;;;;;;;;;;;GAgBG;AACH,SAAS,+BAA+B,CACpC,CAAS,EAAE,KAAa,EAAE,IAAY,EAAE,aAAuB,EAC/D,OAAc;IAAd,wBAAA,EAAA,cAAc;IAChB,OAAO,gBAAI,CAAC;QACH,IAAM,eAAe,GAAG,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC;QACtD,IAAM,IAAI,GAAG,eAAe,CAAC,IAAI,CAAC;QAClC,IAAM,QAAQ,GAAG,eAAe,CAAC,QAAQ,CAAC;QAC1C,IAAM,MAAM,GACR,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,OAAO,CAAC,CAAC;QAChE,OAAO,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC,CAAC;IAClC,CAAC,CAA6B,CAAC;AACxC,CAAC;AAED;;;;;;;;;;;;;;;;GAgBG;AACH,SAAS,iCAAiC,CACtC,CAAS,EAAE,KAAa,EAAE,IAAY,EAAE,aAAuB,EAC/D,OAAc;IAAd,wBAAA,EAAA,cAAc;IAChB,OAAO,gBAAI,CAAC;QACH,IAAM,eAAe,GAAG,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC;QACtD,IAAM,IAAI,GAAG,eAAe,CAAC,IAAI,CAAC;QAClC,IAAM,QAAQ,GAAG,eAAe,CAAC,QAAQ,CAAC;QAC1C,IAAM,WAAW,GAAa,EAAE,CAAC;QACjC,KAAmB,UAA2B,EAA3B,KAAA,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,EAA3B,cAA2B,EAA3B,IAA2B,EAAE;YAA3C,IAAM,IAAI,SAAA;YACb,IAAI,aAAa,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACtC,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACrB;iBAAM;gBACL,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC;aACjC;SACF;QACD,IAAM,aAAa,GAAG,IAAI,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QAChD,IAAM,iBAAiB,GAAG,QAAQ,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QACxD,IAAM,cAAc,GAChB,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QACtD,IAAM,aAAa,GACf,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;QACpD,IAAM,MAAM,GAAG,kBAAkB,CAC7B,CAAC,EAAE,aAAa,EAAE,iBAAiB,EAAE,aAAa,EAClD,cAAc,EAAE,OAAO,CAAC,CAAC;QAC7B,OAAO,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC,CAAC;IAClC,CAAC,CAA6B,CAAC;AACxC,CAAC;AAED;;;;;;;;;;GAUG;AACH,SAAgB,wBAAwB,CACpC,CAAS,EAAE,KAAa,EAAE,IAAY,EAAE,aAAuB,EAC/D,OAAc;IAAd,wBAAA,EAAA,cAAc;IAChB,IAAI,gBAAI,CAAC,WAAW,CACZ,aAAa,CAAC,KAAK,EAAE,CAAC,IAAI,EAAE,EAAE,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,EAAE;QACtE,OAAO,+BAA+B,CAClC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,EAAE,OAAO,CAAC,CAAC;KAC7C;SAAM;QACL,OAAO,iCAAiC,CACpC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,EAAE,OAAO,CAAC,CAAC;KAC7C;AACH,CAAC;AAXD,4DAWC;AAoFD;IAAwC,sCAAK;IAqB3C,4BAAY,IAAkC;QAA9C,iBAsBC;QArBC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QACD,QAAA,kBAAM,IAAI,CAAC,SAAC;QAEZ,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;QAC/C,KAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC;QAC7D,KAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC;QAC1D,KAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC;QACvD,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;QACpD,KAAI,CAAC,eAAe,GAAG,6BAAc,CAAC,IAAI,CAAC,eAAe,IAAI,OAAO,CAAC,CAAC;QACvE,KAAI,CAAC,gBAAgB,GAAG,6BAAc,CAAC,IAAI,CAAC,gBAAgB,IAAI,MAAM,CAAC,CAAC;QACxE,KAAI,CAAC,qBAAqB;YACtB,6BAAc,CAAC,IAAI,CAAC,qBAAqB,IAAI,OAAO,CAAC,CAAC;QAC1D,KAAI,CAAC,yBAAyB;YAC1B,6BAAc,CAAC,IAAI,CAAC,yBAAyB,IAAI,MAAM,CAAC,CAAC;QAC7D,KAAI,CAAC,cAAc,GAAG,2BAAa,CAAC,IAAI,CAAC,cAAc,CAAC,CAAC;QACzD,KAAI,CAAC,eAAe,GAAG,2BAAa,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;QAC3D,KAAI,CAAC,eAAe,GAAG,6BAAc,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;QAC5D,KAAI,CAAC,gBAAgB,GAAG,6BAAc,CAAC,IAAI,CAAC,gBAAgB,CAAC,CAAC;;IAChE,CAAC;IAEM,kCAAK,GAAZ,UAAa,UAAyB;;QACpC,UAAU,GAAG,gCAAkB,CAAC,UAAU,CAAC,CAAC;QAC5C,IAAM,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,GAAG,UAAU,CAAC,MAAM,CAAC,CAAC;QAC1E,IAAM,GAAG,GAAG,UAAU,CAAC,IAAI,CAAC,CAAC;QAC7B,IAAI,GAAG,IAAI,IAAI,EAAE;YACf,MAAM,IAAI,mBAAU,CAChB,UAAQ,IAAI,0DAAuD;gBACnE,yCAAyC;iBACtC,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,MAAG,CAAA,CAAC,CAAC;SACvC;QACD,IAAI,CAAC,SAAS;YACV,CAAC,IAAI,oBAAS,CAAC,EAAC,IAAI,EAAE,UAAU,CAAC,MAAM,EAAE,IAAI,YAAG,GAAC,IAAI,IAAG,GAAG,KAAC,EAAC,CAAC,CAAC,CAAC;QACpE,IAAM,KAAK,GAAG,CAAC,GAAG,CAAC,CAAC;QACpB,IAAI,IAAI,CAAC,KAAK,EAAE;YACd,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,SAAS,CACvB,OAAO,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,gBAAgB,EAAE,IAAI,CAAC,gBAAgB,EAClE,IAAI,EAAE,IAAI,CAAC,eAAe,CAAC,CAAC;SACjC;QACD,IAAI,IAAI,CAAC,MAAM,EAAE;YACf,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,eAAe,EAAE,IAAI,CAAC,eAAe,EAAE,IAAI,EACrE,IAAI,CAAC,cAAc,CAAC,CAAC;SAC1B;QACD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,SAAS,CAC5B,aAAa,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,qBAAqB,EAAE,IAAI,EAAE,KAAK,CAAC,CAAC;QACzE,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,SAAS,CAChC,iBAAiB,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,yBAAyB,EAAE,IAAI,EACpE,KAAK,CAAC,CAAC;QACX,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAED,iCAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAsEC;QArEC,OAAO,gBAAI,CAAC;YACV,IAAM,QAAQ,GAAG,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;YACzE,IAAM,KAAK,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;YAC1C,IAAM,UAAU,GAAG,KAAK,CAAC,KAAK,CAAC;YAC/B,IAAM,IAAI,GAAG,UAAU,CAAC,MAAM,CAAC;YAC/B,IAAM,aAAa,GAAG,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;YAChD,IAAM,IAAI,GAAG,KAAI,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,KAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,CAAC;YAC7D,aAAa,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC;YAC9B,IAAM,cAAc,GAAG,aAAa,CAAC,YAAY,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;YAC3D,cAAc,CAAC,IAAI,CAAC,GAAG,UAAU,CAAC,IAAI,CAAC,CAAC;YAExC,IAAM,mBAAmB,GAAG,aAAa,CAAC,KAAK,EAAE,CAAC;YAClD,mBAAmB,CAAC,IAAI,EAAE,CAAC;YAC3B,IAAM,iBAAiB,GAAG,CAAC,gBAAI,CAAC,WAAW,CACvC,mBAAmB,EAAE,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC;YAEvE,IAAM,kBAAkB,GAAiB;gBACvC,IAAI,iBAAiB,EAAE;oBACrB,IAAM,mBAAmB,GACrB,KAAI,CAAC,UAAU,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC,CAAC;oBACnD,IAAM,uBAAuB,GACzB,KAAI,CAAC,cAAc,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC,CAAC;oBACvD,IAAM,aAAa,GACf,KAAI,CAAC,MAAM,CAAC,CAAC,CAAC,KAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC;oBAClE,IAAM,cAAc,GAChB,KAAI,CAAC,KAAK,CAAC,CAAC,CAAC,KAAI,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,OAAO,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC;oBAClE,OAAO,kBAAkB,CACrB,KAAK,EAAE,mBAAmB,EAAE,uBAAuB,EACnD,aAAa,EAAE,cAAc,EAAE,KAAI,CAAC,OAAO,CAAC,CAAC;iBAClD;qBAAM;oBACL,OAAO,kBAAkB,CACrB,KAAK,EAAE,KAAI,CAAC,UAAU,CAAC,IAAI,EAAE,EAAE,KAAI,CAAC,cAAc,CAAC,IAAI,EAAE,EACzD,KAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,IAAI,CAAC,IAAI,EAAE,EAC3C,KAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,KAAI,CAAC,OAAO,CAAC,CAAC;iBAClE;YACH,CAAC,CAAC;YAEF,IAAI,CAAC,QAAQ,EAAE;gBACb,OAAO,kBAAkB,EAAE,CAAC;aAC7B;YAEK,IAAA,yGAEW,EAFV,sBAAc,EAAE,YAAI,EAAE,gBAEZ,CAAC;YAElB,IAAM,eAAe,GACjB,UAAC,QAAuB,EAAE,KAAa,EAAE,QAAgB;gBACvD,GAAG,CAAC,IAAI,CAAC;oBACP,IAAM,KAAK,GAAG,CAAC,GAAG,QAAQ,CAAC;oBAC3B,IAAM,SAAS,GAAG,QAAQ,CAAC,IAAI,EAAE,CAAC;oBAClC,IAAM,WAAW,GAAG,SAAS,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;oBACpD,QAAQ,CAAC,KAAK,CAAC,SAAS,CAAC,GAAG,CAAC,WAAW,CAAC,CAAC,CAAC;gBAC7C,CAAC,CAAC,CAAC;YACL,CAAC,CAAC;YAEN,mEAAmE;YACnE,8DAA8D;YAC9D,yEAAyE;YACzE,0EAA0E;YAC1E,8DAA8D;YAC9D,iBAAiB;YACjB,IAAM,2BAA2B,GAAG;gBAClC,eAAe,CAAC,KAAI,CAAC,UAAU,EAAE,IAAI,EAAE,KAAI,CAAC,QAAQ,CAAC,CAAC;gBACtD,eAAe,CAAC,KAAI,CAAC,cAAc,EAAE,QAAQ,EAAE,KAAI,CAAC,QAAQ,CAAC,CAAC;YAChE,CAAC,CAAC;YACF,2BAA2B,EAAE,CAAC;YAE9B,OAAO,cAAc,CAAC;QACxB,CAAC,CAAC,CAAC;IACL,CAAC;IAED,sCAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,IAAI,EAAE,IAAI,CAAC,IAAI;YACf,QAAQ,EAAE,IAAI,CAAC,QAAQ;YACvB,OAAO,EAAE,IAAI,CAAC,OAAO;YACrB,MAAM,EAAE,IAAI,CAAC,MAAM;YACnB,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,eAAe,EAAE,mCAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;YAC3D,gBAAgB,EAAE,mCAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;YAC7D,qBAAqB,EAAE,mCAAoB,CAAC,IAAI,CAAC,qBAAqB,CAAC;YACvE,yBAAyB,EACrB,mCAAoB,CAAC,IAAI,CAAC,yBAAyB,CAAC;YACxD,eAAe,EAAE,mCAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;YAC3D,gBAAgB,EAAE,mCAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;YAC7D,cAAc,EAAE,iCAAmB,CAAC,IAAI,CAAC,cAAc,CAAC;YACxD,eAAe,EAAE,iCAAmB,CAAC,IAAI,CAAC,eAAe,CAAC;SAC3D,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAvKD,kBAAkB;IACX,4BAAS,GAAG,oBAAoB,CAAC;IAuK1C,yBAAC;CAAA,AAzKD,CAAwC,gBAAK,GAyK5C;AAzKY,gDAAkB;AA0K/B,yBAAa,CAAC,aAAa,CAAC,kBAAkB,CAAC,CAAC;AAkDhD;IAAwC,sCAAK;IAgB3C,4BAAY,IAAkC;QAA9C,iBAmCC;QAlCC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QACD,QAAA,kBAAM,IAAI,CAAC,SAAC;QAEZ,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;QAC/C,IAAI,OAAO,KAAI,CAAC,IAAI,KAAK,QAAQ,EAAE;YACjC,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,KAAI,CAAC,IAAI,CAAC,EAAE;gBAChC,MAAM,IAAI,KAAK,CACX,kDAAgD,KAAI,CAAC,IAAM,CAAC,CAAC;aAClE;SACF;aAAM,IAAI,KAAK,CAAC,OAAO,CAAC,KAAI,CAAC,IAAI,CAAC,EAAE;YACnC,KAAmB,UAAS,EAAT,KAAA,KAAI,CAAC,IAAI,EAAT,cAAS,EAAT,IAAS,EAAE;gBAAzB,IAAM,IAAI,SAAA;gBACb,IAAI,CAAC,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,EAAE;oBAC3B,MAAM,IAAI,KAAK,CACX,4CAA4C;yBAC5C,kBAAgB,IAAI,CAAC,SAAS,CAAC,KAAI,CAAC,IAAI,CAAG,CAAA,CAAC,CAAC;iBAClD;aACF;SACF;aAAM;YACL,MAAM,IAAI,KAAK,CACX,0DAA0D;iBAC1D,kBAAgB,IAAI,CAAC,SAAS,CAAC,KAAI,CAAC,IAAI,CAAG,CAAA,CAAC,CAAC;SAClD;QAED,KAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC;QAC1D,KAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC;QACvD,KAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;QACpD,KAAI,CAAC,eAAe,GAAG,6BAAc,CAAC,IAAI,CAAC,eAAe,IAAI,OAAO,CAAC,CAAC;QACvE,KAAI,CAAC,gBAAgB,GAAG,6BAAc,CAAC,IAAI,CAAC,gBAAgB,IAAI,MAAM,CAAC,CAAC;QACxE,KAAI,CAAC,eAAe,GAAG,6BAAc,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;QAC5D,KAAI,CAAC,gBAAgB,GAAG,6BAAc,CAAC,IAAI,CAAC,gBAAgB,CAAC,CAAC;QAE9D,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;;IAC9B,CAAC;IAEM,kCAAK,GAAZ,UAAa,UAAyB;QACpC,UAAU,GAAG,gCAAkB,CAAC,UAAU,CAAC,CAAC;QAC5C,IAAM,KAAK,GAAG,UAAU,CAAC,MAAM,CAAC;QAEhC,+CAA+C;QAC/C,IAAI,OAAO,IAAI,CAAC,IAAI,KAAK,QAAQ,EAAE;YACjC,IAAI,CAAC,IAAI,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACzB;QACD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;YACzC,IAAI,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE;gBACpB,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC;aACvB;SACF;QAED,yBAAyB;QACzB,KAAmB,UAAS,EAAT,KAAA,IAAI,CAAC,IAAI,EAAT,cAAS,EAAT,IAAS,EAAE;YAAzB,IAAM,IAAI,SAAA;YACb,IAAI,IAAI,GAAG,CAAC,IAAI,IAAI,IAAI,KAAK,EAAE;gBAC7B,MAAM,IAAI,KAAK,CAAC,mBAAiB,IAAM,CAAC,CAAC;aAC1C;SACF;QACD,IAAI,IAAI,CAAC,IAAI,CAAC,MAAM,KAAK,aAAa,CAAC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,MAAM,EAAE;YAC/D,MAAM,IAAI,KAAK,CAAC,8BAA4B,IAAI,CAAC,IAAM,CAAC,CAAC;SAC1D;QAED,IAAM,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,UAAA,IAAI,IAAI,OAAA,UAAU,CAAC,IAAI,CAAC,EAAhB,CAAgB,CAAa,CAAC;QAEvE,IAAM,SAAS,GAAG,IAAI,CAAC;QACvB,IAAI,IAAI,CAAC,KAAK,EAAE;YACd,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,SAAS,CACvB,OAAO,EAAE,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC,gBAAgB,EACrD,IAAI,CAAC,gBAAgB,EAAE,SAAS,CAAC,CAAC;SACvC;aAAM;YACL,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;SACnB;QACD,IAAI,IAAI,CAAC,MAAM,EAAE;YACf,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,SAAS,CACtB,MAAM,EAAE,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC,eAAe,EACnD,IAAI,CAAC,eAAe,EAAE,SAAS,CAAC,CAAC;SACtC;aAAM;YACL,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC;SAClB;QAED,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAED,iCAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAkDC;QAjDC,IAAM,KAAK,GAAG,iCAAmB,CAAC,MAAM,CAAC,CAAC;QAC1C,IAAM,UAAU,GAAG,KAAK,CAAC,KAAK,CAAC;QAC/B,IAAM,KAAK,GAAG,UAAU,CAAC,MAAM,CAAC;QAEhC,OAAO,gBAAI,CAAC;YACV,IAAM,QAAQ,GAAG,IAAI,CAAC;YAClB,IAAA,qDAAsD,EAArD,cAAI,EAAE,sBAA+C,CAAC;YAC3D,IAAM,cAAc,GAAG,aAAa,CAAC,YAAY,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;YAC5D,KAAkB,UAAqB,EAArB,KAAA,KAAI,CAAC,IAAgB,EAArB,cAAqB,EAArB,IAAqB,EAAE;gBAApC,IAAM,GAAG,SAAA;gBACZ,cAAc,CAAC,GAAG,CAAC,GAAG,UAAU,CAAC,GAAG,CAAC,CAAC;aACvC;YAED,IAAM,SAAS,GAAG,UAAC,CAAS;gBAC1B,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,KAAK,KAAK;oBACrC,KAAI,CAAC,IAAI,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE;oBAC7B,OAAO,CAAC,CAAC,OAAO,CAAC,cAAc,CAAC,CAAC;iBAClC;qBAAM;oBACL,OAAO,CAAC,CAAC;iBACV;YACH,CAAC,CAAC;YAEF,IAAI,KAAK,GAAG,SAAS,CAAC,KAAI,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YACzC,IAAI,MAAM,GAAG,SAAS,CAAC,KAAI,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC,CAAC;YAEzC,yEAAyE;YACzE,0EAA0E;YAC1E,sEAAsE;YACtE,wEAAwE;YACxE,sEAAsE;YACtE,iCAAiC;YACjC,IAAM,aAAa,GAAa,EAAE,CAAC;YACnC,IAAM,iBAAiB,GAAa,EAAE,CAAC;YACvC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,EAAE,EAAE,CAAC,EAAE;gBAC9B,IAAK,KAAI,CAAC,IAAiB,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;oBAC7C,aAAa,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;oBAClC,iBAAiB,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;iBAC3B;qBAAM;oBACL,aAAa,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;oBACtB,iBAAiB,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;iBACvC;aACF;YACD,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC,aAAa,CAAC,CAAC;YAChC,QAAQ,GAAG,QAAQ,CAAC,IAAI,CAAC,aAAa,CAAC,CAAC;YACxC,KAAK,GAAG,KAAK,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;YACtC,MAAM,GAAG,MAAM,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC;YAExC,OAAO,kBAAkB,CACrB,KAAK,EAAE,IAAI,EAAE,QAAQ,EAAE,MAAM,EAAE,KAAK,EAAE,KAAI,CAAC,OAAO,CAAC,CAAC;QAC1D,CAAC,CAAC,CAAC;IACL,CAAC;IAED,sCAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,IAAI,EAAE,IAAI,CAAC,IAAI;YACf,OAAO,EAAE,IAAI,CAAC,OAAO;YACrB,MAAM,EAAE,IAAI,CAAC,MAAM;YACnB,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,eAAe,EAAE,mCAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;YAC3D,gBAAgB,EAAE,mCAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;YAC7D,eAAe,EAAE,mCAAoB,CAAC,IAAI,CAAC,eAAe,CAAC;YAC3D,gBAAgB,EAAE,mCAAoB,CAAC,IAAI,CAAC,gBAAgB,CAAC;SAC9D,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAnKD,kBAAkB;IACX,4BAAS,GAAG,oBAAoB,CAAC;IAmK1C,yBAAC;CAAA,AArKD,CAAwC,gBAAK,GAqK5C;AArKY,gDAAkB;AAsK/B,yBAAa,CAAC,aAAa,CAAC,kBAAkB,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {moments, serialization, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(\n    x: Tensor, mean: Tensor, variance: Tensor, beta?: Tensor, gamma?: Tensor,\n    epsilon = 1e-3): Tensor {\n  let out: Tensor;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(\n        x as Tensor2D, mean as Tensor2D | Tensor1D,\n        variance as Tensor2D | Tensor1D, beta as Tensor2D | Tensor1D,\n        gamma as Tensor2D | Tensor1D, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(\n        x as Tensor3D, mean as Tensor3D | Tensor1D,\n        variance as Tensor3D | Tensor1D, beta as Tensor3D | Tensor1D,\n        gamma as Tensor3D | Tensor1D, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(\n        x as Tensor4D, mean as Tensor4D | Tensor1D,\n        variance as Tensor4D | Tensor1D, beta as Tensor4D | Tensor1D,\n        gamma as Tensor4D | Tensor1D, epsilon);\n  } else {\n    throw new NotImplementedError(\n        `batchNormalization is not implemented for array of rank ${x.rank} ` +\n        `yet`);\n  }\n  return out;\n}\n\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const normed =\n               batchNormalization(x, mean, variance, beta, gamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const targetShape: number[] = [];\n           for (const axis of math_utils.range(0, x.rank)) {\n             if (reductionAxes.indexOf(axis) !== -1) {\n               targetShape.push(1);\n             } else {\n               targetShape.push(x.shape[axis]);\n             }\n           }\n           const broadcastMean = mean.reshape(targetShape);\n           const broadcastVariance = variance.reshape(targetShape);\n           const broadcastGamma =\n               gamma == null ? null : gamma.reshape(targetShape);\n           const broadcastBeta =\n               beta == null ? null : beta.reshape(targetShape);\n           const normed = batchNormalization(\n               x, broadcastMean, broadcastVariance, broadcastBeta,\n               broadcastGamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  if (util.arraysEqual(\n          reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  }\n}\n\nexport declare interface BatchNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The integer axis that should be normalized (typically the features axis).\n   * Defaults to -1.\n   *\n   * For instance, after a `Conv2D` layer with `data_format=\"channels_first\"`,\n   * set `axis=1` in `batchNormalization`.\n   */\n  axis?: number;\n\n  /**\n   * Momentum of the moving average. Defaults to 0.99.\n   */\n  momentum?: number;\n\n  /**\n   * Small float added to the variance to avoid dividing by zero. Defaults to\n   * 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Defaults to `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear (also e.g. `nn.relu`),\n   * this can be disabled since the scaling will be done by the next layer.\n   * Defaults to `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   *  Defaults to 'zeros'.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   *  Defaults to `ones`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving mean.\n   * Defaults to `zeros`\n   */\n  movingMeanInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving variance.\n   *  Defaults to 'Ones'.\n   */\n  movingVarianceInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Constraint for the beta weight.\n   */\n  betaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint for gamma weight.\n   */\n  gammaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Regularizer for the beta weight.\n   */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer for the gamma weight.\n   */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class BatchNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'BatchNormalization';\n  private readonly axis: number;\n  private readonly momentum: number;\n  private readonly epsilon: number;\n  private readonly center: boolean;\n  private readonly scale: boolean;\n  private readonly betaInitializer: Initializer;\n  private readonly gammaInitializer: Initializer;\n  private readonly movingMeanInitializer: Initializer;\n  private readonly movingVarianceInitializer: Initializer;\n  private readonly betaConstraint: Constraint;\n  private readonly gammaConstraint: Constraint;\n  private readonly betaRegularizer: Regularizer;\n  private readonly gammaRegularizer: Regularizer;\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n  private movingMean: LayerVariable;\n  private movingVariance: LayerVariable;\n\n  constructor(args?: BatchNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer =\n        getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer =\n        getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(\n          `Axis ${axis} of input tensor should have a defined dimension but ` +\n          `the layer received an input with shape ` +\n          `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec =\n        [new InputSpec({ndim: inputShape.length, axes: {[axis]: dim}})];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', shape, null, this.gammaInitializer, this.gammaRegularizer,\n          true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', shape, null, this.betaInitializer, this.betaRegularizer, true,\n          this.betaConstraint);\n    }\n    this.movingMean = this.addWeight(\n        'moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight(\n        'moving_variance', shape, null, this.movingVarianceInitializer, null,\n        false);\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(\n          sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference: () => Tensor = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean =\n              this.movingMean.read().reshape(broadcastShape);\n          const broadcastMovingVariance =\n              this.movingVariance.read().reshape(broadcastShape);\n          const broadcastBeta =\n              this.center ? this.beta.read().reshape(broadcastShape) : null;\n          const broadcastGamma =\n              this.scale ? this.gamma.read().reshape(broadcastShape) : null;\n          return batchNormalization(\n              input, broadcastMovingMean, broadcastMovingVariance,\n              broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(\n              input, this.movingMean.read(), this.movingVariance.read(),\n              this.beta == null ? null : this.beta.read(),\n              this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(\n          input, this.gamma.read(), this.beta.read(), reductionAxes,\n          this.epsilon);\n\n      const doMovingAverage =\n          (variable: LayerVariable, value: Tensor, momentum: number): void => {\n            tfc.tidy(() => {\n              const decay = 1 - momentum;\n              const origValue = variable.read();\n              const updateDelta = origValue.sub(value).mul(decay);\n              variable.write(origValue.sub(updateDelta));\n            });\n          };\n\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n\n      return normedTraining;\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer:\n          serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(BatchNormalization);\n\nexport interface LayerNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The axis or axes that should be normalized (typically, the feature axis.)\n   * Defaults to -1 (the last axis.)\n   */\n  axis?: number|number[];\n\n  /**\n   * A small positive float added to variance to avoid divison by zero.\n   * Defaults to 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Default: `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply output by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear, this can be disabled since scaling will\n   * be done by the next layer.\n   * Default: `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   * Default: `'zeros'`.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   * Default: `'ones'`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /** Regularizer for the beta weight. */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /** Regularizer for the gamma weight. */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class LayerNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'LayerNormalization';\n\n  private axis: number|number[];\n  readonly epsilon: number;\n  readonly center: boolean;\n  readonly scale: boolean;\n  readonly betaInitializer: Initializer;\n  readonly gammaInitializer: Initializer;\n  readonly betaRegularizer: Regularizer;\n  readonly gammaRegularizer: Regularizer;\n\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n\n  constructor(args?: LayerNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(\n            `Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(\n              `Expected axis to be an array of integers, ` +\n              `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(\n          `Expected axis to be an integer or an array of integers, ` +\n          `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n\n    this.supportsMasking = true;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]) as number[];\n\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', paramShape, 'float32', this.gammaInitializer,\n          this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', paramShape, 'float32', this.betaInitializer,\n          this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n\n    return tidy(() => {\n      const keepDims = true;\n      let {mean, variance} = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis as number[]) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = (v: Tensor) => {\n        if (v != null && v.shape.length !== nDims &&\n            this.axis !== [nDims - 1]) {\n          return v.reshape(broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = broadcast(this.gamma.read());\n      let offset = broadcast(this.beta.read());\n\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling: number[] = [];\n      const scaleOffsetTiling: number[] = [];\n      for (let i = 0; i < nDims; ++i) {\n        if ((this.axis as number[]).indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = mean.tile(momentsTiling);\n      variance = variance.tile(momentsTiling);\n      scale = scale.tile(scaleOffsetTiling);\n      offset = offset.tile(scaleOffsetTiling);\n\n      return batchNormalization(\n          input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LayerNormalization);\n"]}},"error":null,"hash":"c1507605d4721ac6644e789dc298af2c","cacheData":{"env":{}}}