{"id":"node_modules/@tensorflow/tfjs-layers/dist/losses.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\losses.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\losses.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581896610560},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":14,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\losses.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"./backend/common","loc":{"line":15,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\losses.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\common.js"},{"name":"./backend/tfjs_backend","loc":{"line":16,"column":16},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\losses.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"./errors","loc":{"line":17,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\losses.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/* Original Source: losses.py */\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar common_1 = require(\"./backend/common\");\nvar K = require(\"./backend/tfjs_backend\");\nvar errors_1 = require(\"./errors\");\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\nfunction l2Normalize(x, axis) {\n    return tfjs_core_1.tidy(function () {\n        if (x.dtype !== 'float32') {\n            x = x.asType('float32');\n        }\n        var squareSum = tfc.sum(K.square(x), axis, true);\n        var epsilonTensor = tfc.fill(squareSum.shape, common_1.epsilon());\n        var norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n        return tfc.div(x, norm);\n    });\n}\nexports.l2Normalize = l2Normalize;\nfunction meanSquaredError(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () { return tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1); });\n}\nexports.meanSquaredError = meanSquaredError;\nfunction meanAbsoluteError(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () { return tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1); });\n}\nexports.meanAbsoluteError = meanAbsoluteError;\nfunction meanAbsolutePercentageError(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var diff = tfc.sub(yTrue, yPred);\n        var clippedTrue = tfc.clipByValue(tfc.abs(yTrue), common_1.epsilon(), Number.MAX_VALUE);\n        var absResult = tfc.abs(tfc.div(diff, clippedTrue));\n        return tfc.mul(100, tfc.mean(absResult, -1));\n    });\n}\nexports.meanAbsolutePercentageError = meanAbsolutePercentageError;\nfunction meanSquaredLogarithmicError(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var clippedPred = tfc.clipByValue(yPred, common_1.epsilon(), Number.MAX_VALUE);\n        var firstLog = tfc.log(tfc.add(1, clippedPred));\n        var clippedTrue = tfc.clipByValue(yTrue, common_1.epsilon(), Number.MAX_VALUE);\n        var secondLog = tfc.log(tfc.add(1, clippedTrue));\n        return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n    });\n}\nexports.meanSquaredLogarithmicError = meanSquaredLogarithmicError;\nfunction squaredHinge(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n        return tfc.mean(K.square(maxResult), -1);\n    });\n}\nexports.squaredHinge = squaredHinge;\nfunction hinge(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n        return tfc.mean(maxResult, -1);\n    });\n}\nexports.hinge = hinge;\nfunction categoricalHinge(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n        var neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n        return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n    });\n}\nexports.categoricalHinge = categoricalHinge;\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\nfunction logcosh(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var log2 = Math.log(2);\n        var predictionDiff = tfc.sub(yPred, yTrue);\n        var logcoshResult = tfc.sub(tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))), log2);\n        return tfc.mean(logcoshResult, -1);\n    });\n}\nexports.logcosh = logcosh;\nfunction categoricalCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    return tfjs_core_1.tidy(function () {\n        if (fromLogits) {\n            output = tfc.softmax(output);\n        }\n        else {\n            // scale preds so that the class probabilities of each sample sum to 1.\n            var outputSum = tfc.sum(output, output.shape.length - 1, true);\n            output = tfc.div(output, outputSum);\n        }\n        output = tfc.clipByValue(output, common_1.epsilon(), 1 - common_1.epsilon());\n        return tfc.neg(tfc.sum(tfc.mul(target.toFloat(), tfc.log(output)), output.shape.length - 1));\n    });\n}\nexports.categoricalCrossentropy = categoricalCrossentropy;\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\nfunction sparseCategoricalCrossentropy(target, output) {\n    return tfjs_core_1.tidy(function () {\n        var flatTarget = tfc.floor(K.flatten(target)).toInt();\n        output = tfc.clipByValue(output, common_1.epsilon(), 1 - common_1.epsilon());\n        var outputShape = output.shape;\n        var oneHotTarget = tfc.oneHot(flatTarget, outputShape[outputShape.length - 1])\n            .reshape(outputShape);\n        var fromLogits = false;\n        return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n    });\n}\nexports.sparseCategoricalCrossentropy = sparseCategoricalCrossentropy;\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\nfunction sigmoidCrossEntropyWithLogits(labels, logits) {\n    if (!tfjs_core_1.util.arraysEqual(labels.shape, logits.shape)) {\n        throw new errors_1.ValueError(\"logits and labels must have the same shape, but got shapes \" +\n            (JSON.stringify(labels.shape) + \" and \" + JSON.stringify(logits.shape)));\n    }\n    return tfjs_core_1.tidy(function () {\n        // The logistic loss formula from above is\n        //   x - x * z + log(1 + exp(-x))\n        // For x < 0, a more numerically stable formula is\n        //   -x * z + log(1 + exp(x))\n        // Note that these two expressions can be combined into the following:\n        //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n        var reluLogits = logits.relu();\n        var negAbsLogits = logits.abs().neg();\n        return reluLogits.sub(logits.mul(labels)).add(negAbsLogits.exp().log1p());\n    });\n}\nexports.sigmoidCrossEntropyWithLogits = sigmoidCrossEntropyWithLogits;\nfunction binaryCrossentropy(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var y;\n        y = tfc.clipByValue(yPred, common_1.epsilon(), 1 - common_1.epsilon());\n        y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n        return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n    });\n}\nexports.binaryCrossentropy = binaryCrossentropy;\nfunction kullbackLeiblerDivergence(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var clippedTrue = tfc.clipByValue(yTrue, common_1.epsilon(), 1);\n        var clippedPred = tfc.clipByValue(yPred, common_1.epsilon(), 1);\n        return tfc.sum(tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n    });\n}\nexports.kullbackLeiblerDivergence = kullbackLeiblerDivergence;\nfunction poisson(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var logPred = tfc.log(tfc.add(common_1.epsilon(), yPred));\n        return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n    });\n}\nexports.poisson = poisson;\nfunction cosineProximity(yTrue, yPred) {\n    return tfjs_core_1.tidy(function () {\n        var trueNormalized = l2Normalize(yTrue, -1);\n        var predNormalized = l2Normalize(yPred, -1);\n        var trueXPred = tfc.mul(trueNormalized, predNormalized);\n        return tfc.neg(tfc.sum(trueXPred, -1));\n    });\n}\nexports.cosineProximity = cosineProximity;\nexports.mse = meanSquaredError;\nexports.MSE = meanSquaredError;\nexports.mae = meanAbsoluteError;\nexports.MAE = meanAbsoluteError;\nexports.mape = meanAbsolutePercentageError;\nexports.MAPE = meanAbsolutePercentageError;\nexports.msle = meanSquaredLogarithmicError;\nexports.MSLE = meanSquaredLogarithmicError;\nexports.kld = kullbackLeiblerDivergence;\nexports.KLD = kullbackLeiblerDivergence;\nexports.cosine = cosineProximity;\n// TODO(michaelterry): Add deserialize() function.\nexports.lossesMap = {\n    meanSquaredError: meanSquaredError,\n    meanAbsoluteError: meanAbsoluteError,\n    meanAbsolutePercentageError: meanAbsolutePercentageError,\n    meanSquaredLogarithmicError: meanSquaredLogarithmicError,\n    squaredHinge: squaredHinge,\n    hinge: hinge,\n    categoricalHinge: categoricalHinge,\n    logcosh: logcosh,\n    categoricalCrossentropy: categoricalCrossentropy,\n    sparseCategoricalCrossentropy: sparseCategoricalCrossentropy,\n    binaryCrossentropy: binaryCrossentropy,\n    kullbackLeiblerDivergence: kullbackLeiblerDivergence,\n    poisson: poisson,\n    cosineProximity: cosineProximity\n};\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nfunction get(identifierOrFn) {\n    if (typeof identifierOrFn === 'string') {\n        if (identifierOrFn in exports.lossesMap) {\n            return exports.lossesMap[identifierOrFn];\n        }\n        var errMsg = \"Unknown loss \" + identifierOrFn;\n        if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n            errMsg = \"Unknown loss \" + identifierOrFn + \". \" +\n                'Use \"categoricalCrossentropy\" as the string name for ' +\n                'tf.losses.softmaxCrossEntropy';\n        }\n        throw new errors_1.ValueError(errMsg);\n    }\n    else {\n        return identifierOrFn;\n    }\n}\nexports.get = get;\n"},"sourceMaps":{"js":{"version":3,"file":"losses.js","sourceRoot":"","sources":["../src/losses.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;AAEH,gCAAgC;AAChC,2CAA6C;AAC7C,mDAAmE;AAEnE,2CAAyC;AACzC,0CAA4C;AAC5C,mCAAoC;AAGpC;;;;GAIG;AACH,SAAgB,WAAW,CAAC,CAAS,EAAE,IAAa;IAClD,OAAO,gBAAI,CAAC;QACV,IAAI,CAAC,CAAC,KAAK,KAAK,SAAS,EAAE;YACzB,CAAC,GAAG,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;SACzB;QACD,IAAM,SAAS,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;QACnD,IAAM,aAAa,GAAG,GAAG,CAAC,IAAI,CAAC,SAAS,CAAC,KAAK,EAAE,gBAAO,EAAE,CAAC,CAAC;QAC3D,IAAM,IAAI,GAAG,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,OAAO,CAAC,SAAS,EAAE,aAAa,CAAC,CAAC,CAAC;QAC7D,OAAO,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IAC1B,CAAC,CAAC,CAAC;AACL,CAAC;AAVD,kCAUC;AAED,SAAgB,gBAAgB,CAAC,KAAa,EAAE,KAAa;IAC3D,OAAO,gBAAI,CAAC,cAAM,OAAA,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAA7C,CAA6C,CAAC,CAAC;AACnE,CAAC;AAFD,4CAEC;AAED,SAAgB,iBAAiB,CAAC,KAAa,EAAE,KAAa;IAC5D,OAAO,gBAAI,CAAC,cAAM,OAAA,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAA5C,CAA4C,CAAC,CAAC;AAClE,CAAC;AAFD,8CAEC;AAED,SAAgB,2BAA2B,CACvC,KAAa,EAAE,KAAa;IAC9B,OAAO,gBAAI,CAAC;QACV,IAAM,IAAI,GAAG,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QACnC,IAAM,WAAW,GACb,GAAG,CAAC,WAAW,CAAC,GAAG,CAAC,GAAG,CAAC,KAAK,CAAC,EAAE,gBAAO,EAAE,EAAE,MAAM,CAAC,SAAS,CAAC,CAAC;QACjE,IAAM,SAAS,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,EAAE,WAAW,CAAC,CAAC,CAAC;QACtD,OAAO,GAAG,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,CAAC,IAAI,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C,CAAC,CAAC,CAAC;AACL,CAAC;AATD,kEASC;AAED,SAAgB,2BAA2B,CACvC,KAAa,EAAE,KAAa;IAC9B,OAAO,gBAAI,CAAC;QACV,IAAM,WAAW,GAAG,GAAG,CAAC,WAAW,CAAC,KAAK,EAAE,gBAAO,EAAE,EAAE,MAAM,CAAC,SAAS,CAAC,CAAC;QACxE,IAAM,QAAQ,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC,CAAC;QAElD,IAAM,WAAW,GAAG,GAAG,CAAC,WAAW,CAAC,KAAK,EAAE,gBAAO,EAAE,EAAE,MAAM,CAAC,SAAS,CAAC,CAAC;QACxE,IAAM,SAAS,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC,CAAC;QAEnD,OAAO,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC9D,CAAC,CAAC,CAAC;AACL,CAAC;AAXD,kEAWC;AAED,SAAgB,YAAY,CAAC,KAAa,EAAE,KAAa;IACvD,OAAO,gBAAI,CAAC;QACV,IAAM,SAAS,GAAG,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC;QACpE,OAAO,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;AACL,CAAC;AALD,oCAKC;AAED,SAAgB,KAAK,CAAC,KAAa,EAAE,KAAa;IAChD,OAAO,gBAAI,CAAC;QACV,IAAM,SAAS,GAAG,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC;QACpE,OAAO,GAAG,CAAC,IAAI,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC;IACjC,CAAC,CAAC,CAAC;AACL,CAAC;AALD,sBAKC;AAED,SAAgB,gBAAgB,CAAC,KAAa,EAAE,KAAa;IAC3D,OAAO,gBAAI,CAAC;QACV,IAAM,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/C,IAAM,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,KAAK,CAAC,EAAE,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC3D,OAAO,GAAG,CAAC,OAAO,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC,CAAC;IACvD,CAAC,CAAC,CAAC;AACL,CAAC;AAND,4CAMC;AAED;;;;;;;GAOG;AACH,SAAgB,OAAO,CAAC,KAAa,EAAE,KAAa;IAClD,OAAO,gBAAI,CAAC;QACV,IAAM,IAAI,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QACzB,IAAM,cAAc,GAAG,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QAC7C,IAAM,aAAa,GAAG,GAAG,CAAC,GAAG,CACzB,GAAG,CAAC,GAAG,CAAC,cAAc,EAAE,GAAG,CAAC,QAAQ,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,cAAc,CAAC,CAAC,CAAC,EAClE,IAAI,CAAC,CAAC;QACV,OAAO,GAAG,CAAC,IAAI,CAAC,aAAa,EAAE,CAAC,CAAC,CAAC,CAAC;IACrC,CAAC,CAAC,CAAC;AACL,CAAC;AATD,0BASC;AAED,SAAgB,uBAAuB,CACnC,MAAc,EAAE,MAAc,EAAE,UAAkB;IAAlB,2BAAA,EAAA,kBAAkB;IACpD,OAAO,gBAAI,CAAC;QACV,IAAI,UAAU,EAAE;YACd,MAAM,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC;SAC9B;aAAM;YACL,uEAAuE;YACvE,IAAM,SAAS,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE,IAAI,CAAC,CAAC;YACjE,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,SAAS,CAAC,CAAC;SACrC;QACD,MAAM,GAAG,GAAG,CAAC,WAAW,CAAC,MAAM,EAAE,gBAAO,EAAE,EAAE,CAAC,GAAG,gBAAO,EAAE,CAAC,CAAC;QAC3D,OAAO,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAClB,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,EAAE,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,EAAE,MAAM,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;IAC5E,CAAC,CAAC,CAAC;AACL,CAAC;AAdD,0DAcC;AAED;;;;;;;;GAQG;AACH,SAAgB,6BAA6B,CACzC,MAAc,EAAE,MAAc;IAChC,OAAO,gBAAI,CAAC;QACV,IAAM,UAAU,GAAG,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,KAAK,EAAc,CAAC;QACpE,MAAM,GAAG,GAAG,CAAC,WAAW,CAAC,MAAM,EAAE,gBAAO,EAAE,EAAE,CAAC,GAAG,gBAAO,EAAE,CAAC,CAAC;QAC3D,IAAM,WAAW,GAAG,MAAM,CAAC,KAAK,CAAC;QACjC,IAAM,YAAY,GACd,GAAG,CAAC,MAAM,CAAC,UAAU,EAAE,WAAW,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;aACtD,OAAO,CAAC,WAAW,CAAC,CAAC;QAC9B,IAAM,UAAU,GAAG,KAAK,CAAC;QACzB,OAAO,uBAAuB,CAAC,YAAY,EAAE,MAAM,EAAE,UAAU,CAAC,CAAC;IACnE,CAAC,CAAC,CAAC;AACL,CAAC;AAZD,sEAYC;AAED;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,SAAgB,6BAA6B,CACzC,MAAc,EAAE,MAAc;IAChC,IAAI,CAAC,gBAAI,CAAC,WAAW,CAAC,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,KAAK,CAAC,EAAE;QACjD,MAAM,IAAI,mBAAU,CAChB,6DAA6D;aAC1D,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,aAAQ,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAG,CAAA,CAAC,CAAC;KAC5E;IACD,OAAO,gBAAI,CAAC;QACV,0CAA0C;QAC1C,iCAAiC;QACjC,kDAAkD;QAClD,6BAA6B;QAC7B,sEAAsE;QACtE,8CAA8C;QAC9C,IAAM,UAAU,GAAG,MAAM,CAAC,IAAI,EAAE,CAAC;QACjC,IAAM,YAAY,GAAG,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,EAAE,CAAC;QACxC,OAAO,UAAU,CAAC,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,YAAY,CAAC,GAAG,EAAE,CAAC,KAAK,EAAE,CAAC,CAAC;IAC5E,CAAC,CAAC,CAAC;AACL,CAAC;AAlBD,sEAkBC;AAED,SAAgB,kBAAkB,CAAC,KAAa,EAAE,KAAa;IAC7D,OAAO,gBAAI,CAAC;QACV,IAAI,CAAS,CAAC;QACd,CAAC,GAAG,GAAG,CAAC,WAAW,CAAC,KAAK,EAAE,gBAAO,EAAE,EAAE,CAAC,GAAG,gBAAO,EAAE,CAAC,CAAC;QACrD,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvC,OAAO,GAAG,CAAC,IAAI,CAAC,6BAA6B,CAAC,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC/D,CAAC,CAAC,CAAC;AACL,CAAC;AAPD,gDAOC;AAED,SAAgB,yBAAyB,CACrC,KAAa,EAAE,KAAa;IAC9B,OAAO,gBAAI,CAAC;QACV,IAAM,WAAW,GAAG,GAAG,CAAC,WAAW,CAAC,KAAK,EAAE,gBAAO,EAAE,EAAE,CAAC,CAAC,CAAC;QACzD,IAAM,WAAW,GAAG,GAAG,CAAC,WAAW,CAAC,KAAK,EAAE,gBAAO,EAAE,EAAE,CAAC,CAAC,CAAC;QACzD,OAAO,GAAG,CAAC,GAAG,CACV,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACtE,CAAC,CAAC,CAAC;AACL,CAAC;AARD,8DAQC;AAED,SAAgB,OAAO,CAAC,KAAa,EAAE,KAAa;IAClD,OAAO,gBAAI,CAAC;QACV,IAAM,OAAO,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,gBAAO,EAAE,EAAE,KAAK,CAAC,CAAC,CAAC;QACnD,OAAO,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,GAAG,CAAC,GAAG,CAAC,KAAK,EAAE,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC/D,CAAC,CAAC,CAAC;AACL,CAAC;AALD,0BAKC;AAED,SAAgB,eAAe,CAAC,KAAa,EAAE,KAAa;IAC1D,OAAO,gBAAI,CAAC;QACV,IAAM,cAAc,GAAG,WAAW,CAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9C,IAAM,cAAc,GAAG,WAAW,CAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9C,IAAM,SAAS,GAAG,GAAG,CAAC,GAAG,CAAC,cAAc,EAAE,cAAc,CAAC,CAAC;QAC1D,OAAO,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACzC,CAAC,CAAC,CAAC;AACL,CAAC;AAPD,0CAOC;AAEY,QAAA,GAAG,GAAG,gBAAgB,CAAC;AACvB,QAAA,GAAG,GAAG,gBAAgB,CAAC;AACvB,QAAA,GAAG,GAAG,iBAAiB,CAAC;AACxB,QAAA,GAAG,GAAG,iBAAiB,CAAC;AACxB,QAAA,IAAI,GAAG,2BAA2B,CAAC;AACnC,QAAA,IAAI,GAAG,2BAA2B,CAAC;AACnC,QAAA,IAAI,GAAG,2BAA2B,CAAC;AACnC,QAAA,IAAI,GAAG,2BAA2B,CAAC;AACnC,QAAA,GAAG,GAAG,yBAAyB,CAAC;AAChC,QAAA,GAAG,GAAG,yBAAyB,CAAC;AAChC,QAAA,MAAM,GAAG,eAAe,CAAC;AAEtC,kDAAkD;AAErC,QAAA,SAAS,GAA6C;IACjE,gBAAgB,kBAAA;IAChB,iBAAiB,mBAAA;IACjB,2BAA2B,6BAAA;IAC3B,2BAA2B,6BAAA;IAC3B,YAAY,cAAA;IACZ,KAAK,OAAA;IACL,gBAAgB,kBAAA;IAChB,OAAO,SAAA;IACP,uBAAuB,yBAAA;IACvB,6BAA6B,+BAAA;IAC7B,kBAAkB,oBAAA;IAClB,yBAAyB,2BAAA;IACzB,OAAO,SAAA;IACP,eAAe,iBAAA;CAChB,CAAC;AAEF,8EAA8E;AAC9E,kDAAkD;AAClD,SAAgB,GAAG,CAAC,cAAqC;IACvD,IAAI,OAAO,cAAc,KAAK,QAAQ,EAAE;QACtC,IAAI,cAAc,IAAI,iBAAS,EAAE;YAC/B,OAAO,iBAAS,CAAC,cAAc,CAAC,CAAC;SAClC;QACD,IAAI,MAAM,GAAG,kBAAgB,cAAgB,CAAC;QAC9C,IAAI,cAAc,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,qBAAqB,CAAC,EAAE;YAChE,MAAM,GAAG,kBAAgB,cAAc,OAAI;gBACvC,uDAAuD;gBACvD,+BAA+B,CAAC;SACrC;QACD,MAAM,IAAI,mBAAU,CAAC,MAAM,CAAC,CAAC;KAC9B;SAAM;QACL,OAAO,cAAc,CAAC;KACvB;AACH,CAAC;AAfD,kBAeC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {Tensor, Tensor1D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {epsilon} from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport {ValueError} from './errors';\nimport {LossOrMetricFn} from './types';\n\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\nexport function l2Normalize(x: Tensor, axis?: number): Tensor {\n  return tidy(() => {\n    if (x.dtype !== 'float32') {\n      x = x.asType('float32');\n    }\n    const squareSum = tfc.sum(K.square(x), axis, true);\n    const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n    const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n    return tfc.div(x, norm);\n  });\n}\n\nexport function meanSquaredError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\n\nexport function meanAbsoluteError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\n\nexport function meanAbsolutePercentageError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const diff = tfc.sub(yTrue, yPred);\n    const clippedTrue =\n        tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n    const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n    return tfc.mul(100, tfc.mean(absResult, -1));\n  });\n}\n\nexport function meanSquaredLogarithmicError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n    const firstLog = tfc.log(tfc.add(1, clippedPred));\n\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n    const secondLog = tfc.log(tfc.add(1, clippedTrue));\n\n    return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n  });\n}\n\nexport function squaredHinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(K.square(maxResult), -1);\n  });\n}\n\nexport function hinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(maxResult, -1);\n  });\n}\n\nexport function categoricalHinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n    const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n    return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n  });\n}\n\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\nexport function logcosh(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const log2 = Math.log(2);\n    const predictionDiff = tfc.sub(yPred, yTrue);\n    const logcoshResult = tfc.sub(\n        tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))),\n        log2);\n    return tfc.mean(logcoshResult, -1);\n  });\n}\n\nexport function categoricalCrossentropy(\n    target: Tensor, output: Tensor, fromLogits = false): Tensor {\n  return tidy(() => {\n    if (fromLogits) {\n      output = tfc.softmax(output);\n    } else {\n      // scale preds so that the class probabilities of each sample sum to 1.\n      const outputSum = tfc.sum(output, output.shape.length - 1, true);\n      output = tfc.div(output, outputSum);\n    }\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    return tfc.neg(tfc.sum(\n        tfc.mul(target.toFloat(), tfc.log(output)), output.shape.length - 1));\n  });\n}\n\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\nexport function sparseCategoricalCrossentropy(\n    target: Tensor, output: Tensor): Tensor {\n  return tidy(() => {\n    const flatTarget = tfc.floor(K.flatten(target)).toInt() as Tensor1D;\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    const outputShape = output.shape;\n    const oneHotTarget =\n        tfc.oneHot(flatTarget, outputShape[outputShape.length - 1])\n            .reshape(outputShape);\n    const fromLogits = false;\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n  });\n}\n\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\nexport function sigmoidCrossEntropyWithLogits(\n    labels: Tensor, logits: Tensor): Tensor {\n  if (!util.arraysEqual(labels.shape, logits.shape)) {\n    throw new ValueError(\n        `logits and labels must have the same shape, but got shapes ` +\n        `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n  }\n  return tidy(() => {\n    // The logistic loss formula from above is\n    //   x - x * z + log(1 + exp(-x))\n    // For x < 0, a more numerically stable formula is\n    //   -x * z + log(1 + exp(x))\n    // Note that these two expressions can be combined into the following:\n    //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    const reluLogits = logits.relu();\n    const negAbsLogits = logits.abs().neg();\n    return reluLogits.sub(logits.mul(labels)).add(negAbsLogits.exp().log1p());\n  });\n}\n\nexport function binaryCrossentropy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    let y: Tensor;\n    y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n    y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n    return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n  });\n}\n\nexport function kullbackLeiblerDivergence(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n    return tfc.sum(\n        tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n  });\n}\n\nexport function poisson(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const logPred = tfc.log(tfc.add(epsilon(), yPred));\n    return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n  });\n}\n\nexport function cosineProximity(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const trueNormalized = l2Normalize(yTrue, -1);\n    const predNormalized = l2Normalize(yPred, -1);\n    const trueXPred = tfc.mul(trueNormalized, predNormalized);\n    return tfc.neg(tfc.sum(trueXPred, -1));\n  });\n}\n\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity;\n\n// TODO(michaelterry): Add deserialize() function.\n\nexport const lossesMap: {[functionName: string]: LossOrMetricFn} = {\n  meanSquaredError,\n  meanAbsoluteError,\n  meanAbsolutePercentageError,\n  meanSquaredLogarithmicError,\n  squaredHinge,\n  hinge,\n  categoricalHinge,\n  logcosh,\n  categoricalCrossentropy,\n  sparseCategoricalCrossentropy,\n  binaryCrossentropy,\n  kullbackLeiblerDivergence,\n  poisson,\n  cosineProximity\n};\n\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nexport function get(identifierOrFn: string|LossOrMetricFn): LossOrMetricFn {\n  if (typeof identifierOrFn === 'string') {\n    if (identifierOrFn in lossesMap) {\n      return lossesMap[identifierOrFn];\n    }\n    let errMsg = `Unknown loss ${identifierOrFn}`;\n    if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n      errMsg = `Unknown loss ${identifierOrFn}. ` +\n          'Use \"categoricalCrossentropy\" as the string name for ' +\n          'tf.losses.softmaxCrossEntropy';\n    }\n    throw new ValueError(errMsg);\n  } else {\n    return identifierOrFn;\n  }\n}\n"]}},"error":null,"hash":"970f8e9c05d3f984772379832e3b68fe","cacheData":{"env":{}}}