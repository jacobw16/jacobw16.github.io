{"id":"node_modules/@tensorflow/tfjs-layers/dist/layers/merge.js","dependencies":[{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js.map","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\merge.ts","includedInParent":true,"mtime":499162500000},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\package.json","includedInParent":true,"mtime":1581030063848},{"name":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1581030261368},{"name":"@tensorflow/tfjs-core","loc":{"line":29,"column":26},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-core\\dist\\tf-core.esm.js"},{"name":"../backend/tfjs_backend","loc":{"line":30,"column":16},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\backend\\tfjs_backend.js"},{"name":"../engine/topology","loc":{"line":31,"column":25},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\engine\\topology.js"},{"name":"../errors","loc":{"line":32,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\errors.js"},{"name":"../losses","loc":{"line":33,"column":23},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\losses.js"},{"name":"../utils/generic_utils","loc":{"line":34,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\generic_utils.js"},{"name":"../utils/math_utils","loc":{"line":35,"column":24},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\math_utils.js"},{"name":"../utils/types_utils","loc":{"line":36,"column":28},"parent":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\layers\\merge.js","resolved":"C:\\Users\\Jacob\\Documents\\jump(3)\\scripts\\node_modules\\@tensorflow\\tfjs-layers\\dist\\utils\\types_utils.js"}],"generated":{"js":"\"use strict\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = function (d, b) {\n        extendStatics = Object.setPrototypeOf ||\n            ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n            function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n        return extendStatics(d, b);\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * TensorFlow.js Layers: Merge Layers.\n */\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"../backend/tfjs_backend\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar losses_1 = require(\"../losses\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar mathUtils = require(\"../utils/math_utils\");\nvar types_utils_1 = require(\"../utils/types_utils\");\n/**\n * Generic Merge layer for element-wise merge functions.\n *\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\n */\nvar Merge = /** @class */ (function (_super) {\n    __extends(Merge, _super);\n    function Merge(args) {\n        var _this = _super.call(this, args || {}) || this;\n        _this.supportsMasking = true;\n        return _this;\n    }\n    /**\n     * Logic for merging multiple tensors, to be overridden by subclasses.\n     * @param inputs\n     */\n    Merge.prototype.mergeFunction = function (inputs) {\n        throw new errors_1.NotImplementedError();\n    };\n    /**\n     * Computes the shape of the result of an elementwise operation.\n     *\n     * @param shape1: Shape of the first tensor.\n     * @param shape2: Shape of the second tensor.\n     * @returns Expected output shape when an elementwise operation is carried\n     *   out on 2 tensors with shapes `shape1` and `shape2`.\n     * @throws ValueError: If `shape1` and `shape2` are not compatible for\n     *   element-wise operations.\n     */\n    Merge.prototype.computeElementwiseOpOutputShape = function (shape1, shape2) {\n        if (shape1 == null || shape2 == null) {\n            return null;\n        }\n        else if (shape1.length < shape2.length) {\n            return this.computeElementwiseOpOutputShape(shape2, shape1);\n        }\n        else if (shape2.length === 0) {\n            return shape1;\n        }\n        var outputShape = shape1.slice(0, shape1.length - shape2.length);\n        for (var k = 0; k < shape2.length; ++k) {\n            var i = shape1[shape1.length - shape2.length + k];\n            var j = shape2[k];\n            if (i == null || j == null || i < 0 || j < 0) {\n                outputShape.push(null);\n            }\n            else if (i === 1) {\n                outputShape.push(j);\n            }\n            else if (j === 1) {\n                outputShape.push(i);\n            }\n            else {\n                if (i !== j) {\n                    throw new errors_1.ValueError('Operands could not be broadcast together with shapes ' +\n                        JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n                }\n                outputShape.push(i);\n            }\n        }\n        return outputShape;\n    };\n    Merge.prototype.build = function (inputShape) {\n        // Used purely for shape validation.\n        if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n            // Make sure that inputShape is an Array of shape.\n            inputShape = [types_utils_1.getExactlyOneShape(inputShape)];\n        }\n        inputShape = inputShape;\n        if (inputShape.length < 2) {\n            throw new errors_1.ValueError('A merge layer should be called on an Array of at least 2 inputs.' +\n                (\" Got \" + inputShape.length + \" input(s).\"));\n        }\n        // Make sure that there is at most one unique batch size among the input\n        // shapes.\n        var batchSizes = [];\n        for (var _i = 0, inputShape_1 = inputShape; _i < inputShape_1.length; _i++) {\n            var shape = inputShape_1[_i];\n            if (shape != null && shape[0] !== null) {\n                batchSizes.push(shape[0]);\n            }\n        }\n        batchSizes = generic_utils.unique(batchSizes);\n        if (batchSizes.length > 1) {\n            throw new errors_1.ValueError(\"Can not merge tensors with different batch sizes. \" +\n                (\"Got tensors with shapes: \" + JSON.stringify(inputShape) + \".\"));\n        }\n        var outputShape = inputShape[0] == null ? null : inputShape[0].slice(1);\n        for (var i = 1; i < inputShape.length; ++i) {\n            var shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n            outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n        }\n        // If the inputs have different ranks, we have to reshape them to make them\n        // broadcastable.\n        var allRanks = inputShape.map(function (shape) { return shape.length; });\n        if (inputShape.indexOf(null) === -1 &&\n            generic_utils.unique(allRanks).length === 1) {\n            this.reshapeRequired = false;\n        }\n        else {\n            this.reshapeRequired = true;\n        }\n    };\n    Merge.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            inputs = inputs;\n            if (_this.reshapeRequired) {\n                var reshapedInputs = [];\n                var inputDims = inputs.map(function (input) { return input.rank; });\n                if (inputDims.indexOf(null) === -1) {\n                    // If ranks of all inputs are available, we simply expand each of them\n                    // at axis=1 until all of them have the same rank.\n                    var maxNDim = mathUtils.max(inputDims);\n                    for (var _i = 0, inputs_1 = inputs; _i < inputs_1.length; _i++) {\n                        var x = inputs_1[_i];\n                        var xNDim = x.rank;\n                        for (var k = 0; k < maxNDim - xNDim; ++k) {\n                            x = K.expandDims(x, 1);\n                        }\n                        reshapedInputs.push(x);\n                    }\n                    return _this.mergeFunction(reshapedInputs);\n                }\n                else {\n                    // Transpose all inputs so that batch size is the last dimension.\n                    // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n                    var transposed = false;\n                    for (var _a = 0, inputs_2 = inputs; _a < inputs_2.length; _a++) {\n                        var x = inputs_2[_a];\n                        var xNDim = x.rank;\n                        if (xNDim == null) {\n                            var xShape = x.shape;\n                            var batchSize = xShape[0];\n                            var newShape = xShape.slice(1).concat([batchSize]);\n                            var xTransposed = x.reshape([batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n                            xTransposed = tfc.transpose(xTransposed, [1, 0]);\n                            xTransposed = xTransposed.reshape(newShape);\n                            reshapedInputs.push(xTransposed);\n                            transposed = true;\n                        }\n                        else if (xNDim > 1) {\n                            var dims = mathUtils.range(1, xNDim).concat([0]);\n                            reshapedInputs.push(tfc.transpose(x, dims));\n                            transposed = true;\n                        }\n                        else {\n                            // We don't transpose inputs if they are 1D vectors or scalars.\n                            reshapedInputs.push(x);\n                        }\n                    }\n                    var y = _this.mergeFunction(reshapedInputs);\n                    var yNDim = y.rank;\n                    if (transposed) {\n                        // If inputs have been transposed, we have to transpose the output\n                        // too.\n                        if (yNDim == null) {\n                            var yShape = y.shape;\n                            var yNDim_1 = yShape.length;\n                            var batchSize = yShape[yNDim_1 - 1];\n                            var newShape = [batchSize].concat(yShape.slice(0, yShape.length - 1));\n                            y = tfc.transpose(y.reshape([-1, batchSize]), [1, 0])\n                                .reshape(newShape);\n                        }\n                        else if (yNDim > 1) {\n                            var dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n                            y = tfc.transpose(y, dims);\n                        }\n                    }\n                    return y;\n                }\n            }\n            else {\n                return _this.mergeFunction(inputs);\n            }\n        });\n    };\n    Merge.prototype.computeOutputShape = function (inputShape) {\n        inputShape = inputShape;\n        var outputShape;\n        if (inputShape[0] == null) {\n            outputShape = null;\n        }\n        else {\n            outputShape = inputShape[0].slice(1);\n        }\n        for (var i = 1; i < inputShape.length; ++i) {\n            var shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n            outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n        }\n        var batchSizes = [];\n        for (var _i = 0, inputShape_2 = inputShape; _i < inputShape_2.length; _i++) {\n            var shape = inputShape_2[_i];\n            if (shape != null && shape[0] !== null) {\n                batchSizes.push(shape[0]);\n            }\n        }\n        batchSizes = generic_utils.unique(batchSizes);\n        if (batchSizes.length === 1) {\n            outputShape = batchSizes.concat(outputShape);\n        }\n        else {\n            outputShape = [null].concat(outputShape);\n        }\n        return outputShape;\n    };\n    Merge.prototype.computeMask = function (inputs, mask) {\n        return tfc.tidy(function () {\n            if (mask == null) {\n                return null;\n            }\n            if (!Array.isArray(mask)) {\n                throw new errors_1.ValueError('`mask` should be an Array');\n            }\n            if (!Array.isArray(inputs)) {\n                throw new errors_1.ValueError('`inputs` should be an Array');\n            }\n            if (mask.length !== inputs.length) {\n                throw new errors_1.ValueError(\"The Array 'inputs' and 'mask' are expected to have the same \" +\n                    \"length, but have different lengths \" +\n                    (\"(\" + inputs.length + \" vs \" + mask.length + \")\"));\n            }\n            if (mask.every(function (m) { return m == null; })) {\n                return null;\n            }\n            mask = mask.map(function (m) { return m == null ? m : tfc.expandDims(m, 0); });\n            var output = mask[0];\n            for (var i = 1; i < mask.length - 1; ++i) {\n                output = tfc.logicalAnd(output, mask[i]);\n            }\n            return output;\n        });\n    };\n    return Merge;\n}(topology_1.Layer));\nexports.Merge = Merge;\nvar Add = /** @class */ (function (_super) {\n    __extends(Add, _super);\n    function Add(args) {\n        return _super.call(this, args) || this;\n    }\n    Add.prototype.mergeFunction = function (inputs) {\n        return tfjs_core_1.tidy(function () {\n            var output = inputs[0].clone();\n            for (var i = 1; i < inputs.length; ++i) {\n                output = tfc.add(output, inputs[i]);\n            }\n            return output;\n        });\n    };\n    /** @nocollapse */\n    Add.className = 'Add';\n    return Add;\n}(Merge));\nexports.Add = Add;\ntfjs_core_1.serialization.registerClass(Add);\n/**\n * Calculate the element-wise sum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Add` layer, by using no input argument\n *    or a single configuration argument. The resultant `Add` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const addLayer = tf.layers.add();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = addLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.add([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.add([input1, input2]).print();\n * // Gives [[11, 22], [33, 44]].\n *\n */\nfunction add(config) {\n    if (Array.isArray(config)) {\n        var layer = new Add({});\n        return layer.apply(config);\n    }\n    else {\n        return new Add(config);\n    }\n}\nexports.add = add;\nvar Multiply = /** @class */ (function (_super) {\n    __extends(Multiply, _super);\n    function Multiply(args) {\n        return _super.call(this, args) || this;\n    }\n    Multiply.prototype.mergeFunction = function (inputs) {\n        return tfjs_core_1.tidy(function () {\n            var output = inputs[0].clone();\n            for (var i = 1; i < inputs.length; ++i) {\n                output = tfc.mul(output, inputs[i]);\n            }\n            return output;\n        });\n    };\n    /** @nocollapse */\n    Multiply.className = 'Multiply';\n    return Multiply;\n}(Merge));\nexports.Multiply = Multiply;\ntfjs_core_1.serialization.registerClass(Multiply);\n/**\n * Calculate the element-wise product of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Multiply` layer, by using no input argument\n *    or a single configuration argument. The resultant `Multiply` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const multiplyLayer = tf.layers.multiply();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = multiplyLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.multiply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.multiply([input1, input2]).print();\n * // Gives [[10, 40], [90, 160]].\n *\n */\nfunction multiply(config) {\n    if (Array.isArray(config)) {\n        var layer = new Multiply({});\n        return layer.apply(config);\n    }\n    else {\n        return new Multiply(config);\n    }\n}\nexports.multiply = multiply;\nvar Average = /** @class */ (function (_super) {\n    __extends(Average, _super);\n    function Average(args) {\n        return _super.call(this, args) || this;\n    }\n    Average.prototype.mergeFunction = function (inputs) {\n        return tfjs_core_1.tidy(function () {\n            var output = inputs[0].clone();\n            for (var i = 1; i < inputs.length; ++i) {\n                output = tfc.add(output, inputs[i]);\n            }\n            return tfc.mul(1 / inputs.length, output);\n        });\n    };\n    /** @nocollapse */\n    Average.className = 'Average';\n    return Average;\n}(Merge));\nexports.Average = Average;\ntfjs_core_1.serialization.registerClass(Average);\n/**\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\n * shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Average` layer, by using no input argument\n *    or a single configuration argument. The resultant `Average` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const averageLayer = tf.layers.average();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = averageLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.average([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.average([input1, input2]).print();\n * // Gives [[5.5, 11], [16.5, 22]].\n *\n */\nfunction average(config) {\n    if (Array.isArray(config)) {\n        var layer = new Average({});\n        return layer.apply(config);\n    }\n    else {\n        return new Average(config);\n    }\n}\nexports.average = average;\nvar Maximum = /** @class */ (function (_super) {\n    __extends(Maximum, _super);\n    function Maximum(args) {\n        return _super.call(this, args) || this;\n    }\n    Maximum.prototype.mergeFunction = function (inputs) {\n        return tfjs_core_1.tidy(function () {\n            var output = inputs[0];\n            for (var i = 1; i < inputs.length; ++i) {\n                output = tfc.maximum(output, inputs[i]);\n            }\n            return output;\n        });\n    };\n    /** @nocollapse */\n    Maximum.className = 'Maximum';\n    return Maximum;\n}(Merge));\nexports.Maximum = Maximum;\ntfjs_core_1.serialization.registerClass(Maximum);\n/**\n * Calculate the element-wise maximum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Maximum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Maximum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const maximumLayer = tf.layers.maximum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = maximumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.maximum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.maximum([input1, input2]).print();\n * // Gives [[10, 20], [30, 40]].\n *\n */\nfunction maximum(config) {\n    if (Array.isArray(config)) {\n        var layer = new Maximum({});\n        return layer.apply(config);\n    }\n    else {\n        return new Maximum(config);\n    }\n}\nexports.maximum = maximum;\nvar Minimum = /** @class */ (function (_super) {\n    __extends(Minimum, _super);\n    function Minimum(args) {\n        return _super.call(this, args) || this;\n    }\n    Minimum.prototype.mergeFunction = function (inputs) {\n        return tfjs_core_1.tidy(function () {\n            var output = inputs[0];\n            for (var i = 1; i < inputs.length; ++i) {\n                output = tfc.minimum(output, inputs[i]);\n            }\n            return output;\n        });\n    };\n    /** @nocollapse */\n    Minimum.className = 'Minimum';\n    return Minimum;\n}(Merge));\nexports.Minimum = Minimum;\ntfjs_core_1.serialization.registerClass(Minimum);\n/**\n * Calculate the element-wise minimum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Minimum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Minimum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const minimumLayer = tf.layers.minimum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = minimumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.minimum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.minimum([input1, input2]).print();\n * // Gives [[1, 2], [3, 4]].\n *\n */\nfunction minimum(config) {\n    if (Array.isArray(config)) {\n        var layer = new Minimum({});\n        return layer.apply(config);\n    }\n    else {\n        return new Minimum(config);\n    }\n}\nexports.minimum = minimum;\nvar Concatenate = /** @class */ (function (_super) {\n    __extends(Concatenate, _super);\n    function Concatenate(args) {\n        var _this = _super.call(this, args) || this;\n        _this.DEFAULT_AXIS = -1;\n        if (args == null) {\n            args = {};\n        }\n        _this.axis = args.axis == null ? _this.DEFAULT_AXIS : args.axis;\n        _this.supportsMasking = true;\n        _this.reshapeRequired = false;\n        return _this;\n    }\n    Concatenate.prototype.build = function (inputShape) {\n        // Used purely for shape validation.]\n        if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) ||\n            inputShape.length === 1) {\n            throw new errors_1.ValueError('A `Concatenate` layer should be called on a list of at least 2 ' +\n                'inputs');\n        }\n        inputShape = inputShape;\n        var allNoneShape = true;\n        for (var _i = 0, inputShape_3 = inputShape; _i < inputShape_3.length; _i++) {\n            var shape = inputShape_3[_i];\n            if (shape != null) {\n                allNoneShape = false;\n                break;\n            }\n        }\n        if (allNoneShape) {\n            return;\n        }\n        var shapeSet = [];\n        for (var i = 0; i < inputShape.length; ++i) {\n            var shapeWithoutConcatAxis = inputShape[i].slice();\n            shapeWithoutConcatAxis.splice(this.axis, 1);\n            var exists = false;\n            for (var _a = 0, shapeSet_1 = shapeSet; _a < shapeSet_1.length; _a++) {\n                var shape = shapeSet_1[_a];\n                if (tfjs_core_1.util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n                    exists = true;\n                    break;\n                }\n            }\n            if (!exists) {\n                shapeSet.push(shapeWithoutConcatAxis);\n            }\n        }\n        if (shapeSet.length > 1) {\n            throw new errors_1.ValueError('A `Concatenate` layer requires inputs with matching shapes ' +\n                'except for the concat axis. Got input shapes: ' +\n                JSON.stringify(inputShape));\n        }\n    };\n    Concatenate.prototype.mergeFunction = function (inputs) {\n        var _this = this;\n        return tfjs_core_1.tidy(function () {\n            return K.concatenate(inputs, _this.axis);\n        });\n    };\n    Concatenate.prototype.computeOutputShape = function (inputShape) {\n        if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n            throw new errors_1.ValueError('A `Concatenate` layer should be called on a list of inputs.');\n        }\n        var inputShapes = inputShape;\n        var outputShape = inputShapes[0].slice();\n        var axis = this.axis < 0 ? outputShape.length + this.axis : this.axis;\n        // Porting Note: the line above is because TypeScript doesn't support\n        //   negative indices.\n        for (var _i = 0, _a = inputShapes.slice(1); _i < _a.length; _i++) {\n            var shape = _a[_i];\n            if (outputShape[axis] == null || shape[axis] == null) {\n                outputShape[axis] = null;\n                break;\n            }\n            outputShape[axis] += shape[axis];\n        }\n        return outputShape;\n    };\n    Concatenate.prototype.computeMask = function (inputs, mask) {\n        var _this = this;\n        if (mask == null) {\n            return null;\n        }\n        if (!Array.isArray(mask)) {\n            throw new errors_1.ValueError('`mask` should be an array for Concatenate');\n        }\n        if (!Array.isArray(inputs)) {\n            throw new errors_1.ValueError('`inputs` should be an array for Concatenate');\n        }\n        if (mask.length !== inputs.length) {\n            throw new errors_1.ValueError(\"Mismatch in the length of mask (\" + mask.length + \") \" +\n                (\"and the legnth of inputs (\" + inputs.length + \")\"));\n        }\n        return tfc.tidy(function () {\n            var allNullMasks = true;\n            mask.forEach(function (m) {\n                if (m != null) {\n                    allNullMasks = false;\n                    return;\n                }\n            });\n            if (allNullMasks) {\n                return null;\n            }\n            var outputMasks = [];\n            for (var i = 0; i < inputs.length; ++i) {\n                if (mask[i] == null) {\n                    // Input is unmasked. Append all 1's to masks.\n                    outputMasks.push(tfc.onesLike(inputs[i]).asType('bool'));\n                }\n                else if (mask[i].rank < inputs[i].rank) {\n                    // Mask is smaller than the input, expand it.\n                    outputMasks.push(tfc.expandDims(mask[i], -1));\n                }\n                else {\n                    outputMasks.push(mask[i]);\n                }\n            }\n            var concatenatedMasks = tfc.concat(outputMasks, _this.axis);\n            return tfc.all(concatenatedMasks, -1, false);\n        });\n    };\n    Concatenate.prototype.getConfig = function () {\n        var config = {\n            'axis': this.axis,\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    Concatenate.className = 'Concatenate';\n    return Concatenate;\n}(Merge));\nexports.Concatenate = Concatenate;\ntfjs_core_1.serialization.registerClass(Concatenate);\n/**\n * Concatenate an `Array` of inputs.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\n *    or a single configuration argument. The resultant `Concatenate` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const concatLayer = tf.layers.concatenate();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = concatLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = tf.layers.concatenate([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\n * tf.layers.concatenate([input1, input2]).print();\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\n *\n */\nfunction concatenate(config) {\n    if (Array.isArray(config)) {\n        var layer = new Concatenate({});\n        return layer.apply(config);\n    }\n    else {\n        return new Concatenate(config);\n    }\n}\nexports.concatenate = concatenate;\n/**\n * Interpretable potentially negative axis index.\n *\n * For example, given axis = -1, and dim = 3, this function will return 2.\n *\n * @param axis The axis index, may be a positive, zero or negative integer.\n * @param dim Total number of dimensions, a positive integer.\n * @returns A non-negative axis index equivalent to the input `axis`.\n */\nfunction interpretAxis(axis, dim) {\n    while (axis < 0) {\n        axis += dim;\n    }\n    return axis;\n}\nfunction batchDot(x, y, axes) {\n    if (x.shape.length > 3 || y.shape.length > 3) {\n        throw new errors_1.NotImplementedError('batchDot is not implemented for tensors of 4D or higher rank yet');\n    }\n    tfc.util.assert(x.shape.length >= 2, function () { return \"batchDot requires the rank of x to be >= 2, \" +\n        (\"but got \" + x.shape.length); });\n    tfc.util.assert(x.shape.length >= 2, function () { return \"batchDot requires the rank of y to be >= 2, \" +\n        (\"but got \" + y.shape.length); });\n    if (typeof axes === 'number') {\n        axes = [axes, axes];\n    }\n    if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n        throw new errors_1.NotImplementedError('batchDot is not implemented for complex64-type Tensors yet.');\n    }\n    var xNDim = x.shape.length;\n    var yNDim = y.shape.length;\n    if (axes == null) {\n        // Behave like batchMatmul by default.\n        axes = [xNDim - 1, yNDim - 2];\n    }\n    var axesArray = axes;\n    return tfc.tidy(function () {\n        var diff;\n        if (xNDim > yNDim) {\n            diff = xNDim - yNDim;\n            var diffShape = [];\n            for (var i = 0; i < diff; ++i) {\n                diffShape.push(1);\n            }\n            y = y.reshape(y.shape.concat(diffShape));\n        }\n        else if (yNDim > xNDim) {\n            diff = yNDim - xNDim;\n            var diffShape = [];\n            for (var i = 0; i < diff; ++i) {\n                diffShape.push(1);\n            }\n            x = x.reshape(x.shape.concat(diffShape));\n        }\n        else {\n            diff = 0;\n        }\n        var out;\n        if (x.shape.length === 2 && y.shape.length === 2) {\n            if (axesArray[0] === axesArray[1]) {\n                out = x.mulStrict(y).sum(axesArray[0]);\n            }\n            else {\n                out = x.transpose([1, 0]).mulStrict(y).sum(axesArray[1]);\n            }\n        }\n        else {\n            var adjX = axesArray[0] !== x.shape.length - 1;\n            var adjY = axesArray[1] === y.shape.length - 1;\n            out = x.matMul(y, adjX, adjY);\n        }\n        if (diff > 0) {\n            var idx = void 0;\n            if (xNDim > yNDim) {\n                idx = xNDim + yNDim - 3;\n            }\n            else {\n                idx = xNDim - 1;\n            }\n            var squeezeAxes = [];\n            for (var i = idx; i < idx + diff; ++i) {\n                squeezeAxes.push(i);\n            }\n            out = out.squeeze(squeezeAxes);\n        }\n        if (out.shape.length === 1) {\n            out = out.expandDims(1);\n        }\n        return out;\n    });\n}\nvar Dot = /** @class */ (function (_super) {\n    __extends(Dot, _super);\n    function Dot(args) {\n        var _this = _super.call(this, args) || this;\n        _this.axes = args.axes;\n        _this.normalize = args.normalize == null ? false : args.normalize;\n        _this.supportsMasking = true;\n        _this.reshapeRequired = false;\n        return _this;\n    }\n    Dot.prototype.build = function (inputShape) {\n        tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), function () { return 'A `Dot` layer should be called on a list of exactly 2 inputs.'; });\n        var shape1 = inputShape[0];\n        var shape2 = inputShape[1];\n        if (shape1.length > 3 || shape2.length > 3) {\n            throw new errors_1.NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n        }\n        var axes = this.interpretAxes(shape1, shape2);\n        if (shape1[axes[0]] !== shape2[axes[1]]) {\n            throw new errors_1.ValueError(\"Dimension incompatibility: \" +\n                (shape1[axes[0]] + \" !== \" + shape2[axes[1]]));\n        }\n    };\n    Dot.prototype.mergeFunction = function (inputs) {\n        if (inputs.length !== 2) {\n            throw new errors_1.ValueError('A `Dot` layer must be called on exactly 2 inputs, ' +\n                (\"but received \" + inputs.length + \" input(s).\"));\n        }\n        var x1 = inputs[0];\n        var x2 = inputs[1];\n        var axes;\n        if (!Array.isArray(this.axes)) {\n            axes = [\n                interpretAxis(this.axes, x1.shape.length),\n                interpretAxis(this.axes, x2.shape.length)\n            ];\n        }\n        else {\n            axes = this.axes.map(function (axis, i) { return interpretAxis(axis, inputs[i].shape.length); });\n        }\n        if (this.normalize) {\n            x1 = losses_1.l2Normalize(x1, axes[0]);\n            x2 = losses_1.l2Normalize(x2, axes[1]);\n        }\n        return batchDot(x1, x2, axes);\n    };\n    Dot.prototype.interpretAxes = function (shape1, shape2) {\n        var axes;\n        if (!Array.isArray(this.axes)) {\n            // `this.axes` is a single integer.\n            axes = [\n                interpretAxis(this.axes, shape1.length),\n                interpretAxis(this.axes, shape2.length)\n            ];\n        }\n        else {\n            // `this.axes` is an Array of integers.\n            axes = this.axes;\n        }\n        return axes;\n    };\n    Dot.prototype.computeOutputShape = function (inputShape) {\n        tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), function () { return 'A `Dot` layer should be called on a list of exactly 2 inputs.'; });\n        var shape1 = inputShape[0].slice();\n        var shape2 = inputShape[1].slice();\n        if (shape1.length > 3 || shape2.length > 3) {\n            throw new errors_1.NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n        }\n        var axes = this.interpretAxes(shape1, shape2);\n        shape1.splice(axes[0], 1);\n        shape2.splice(axes[1], 1);\n        shape2.splice(0, 1);\n        var outputShape = shape1.concat(shape2);\n        if (outputShape.length === 1) {\n            outputShape.push(1);\n        }\n        return outputShape;\n    };\n    Dot.prototype.computeMask = function (inputs, mask) {\n        return null;\n    };\n    Dot.prototype.getConfig = function () {\n        var config = {\n            'axes': this.axes,\n            'normalize': this.normalize\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    /** @nocollapse */\n    Dot.className = 'Dot';\n    return Dot;\n}(Merge));\nexports.Dot = Dot;\ntfjs_core_1.serialization.registerClass(Dot);\n// TODO(cais): Add functional interfaces for the merge layers.\n"},"sourceMaps":{"js":{"version":3,"file":"merge.js","sourceRoot":"","sources":["../../src/layers/merge.ts"],"names":[],"mappings":";AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;AAEH;;GAEG;AAEH,2CAA6C;AAC7C,mDAAwE;AACxE,2CAA6C;AAC7C,+CAAoE;AACpE,oCAA0D;AAE1D,oCAAsC;AAEtC,sDAAwD;AACxD,+CAAiD;AACjD,oDAAwD;AAExD;;;;GAIG;AACH;IAAoC,yBAAK;IAGvC,eAAY,IAAgB;QAA5B,YACE,kBAAM,IAAI,IAAI,EAAE,CAAC,SAElB;QADC,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;;IAC9B,CAAC;IAED;;;OAGG;IACO,6BAAa,GAAvB,UAAwB,MAAgB;QACtC,MAAM,IAAI,4BAAmB,EAAE,CAAC;IAClC,CAAC;IAED;;;;;;;;;OASG;IACK,+CAA+B,GAAvC,UAAwC,MAAa,EAAE,MAAa;QAClE,IAAI,MAAM,IAAI,IAAI,IAAI,MAAM,IAAI,IAAI,EAAE;YACpC,OAAO,IAAI,CAAC;SACb;aAAM,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;YACxC,OAAO,IAAI,CAAC,+BAA+B,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;SAC7D;aAAM,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;YAC9B,OAAO,MAAM,CAAC;SACf;QACD,IAAM,WAAW,GAAU,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;QAC1E,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;YACtC,IAAM,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACpD,IAAM,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACpB,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,EAAE;gBAC5C,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aACxB;iBAAM,IAAI,CAAC,KAAK,CAAC,EAAE;gBAClB,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACrB;iBAAM,IAAI,CAAC,KAAK,CAAC,EAAE;gBAClB,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACrB;iBAAM;gBACL,IAAI,CAAC,KAAK,CAAC,EAAE;oBACX,MAAM,IAAI,mBAAU,CAChB,uDAAuD;wBACvD,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,GAAG,GAAG,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC,CAAC;iBAC5D;gBACD,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACrB;SACF;QACD,OAAO,WAAW,CAAC;IACrB,CAAC;IAED,qBAAK,GAAL,UAAM,UAAyB;QAC7B,oCAAoC;QACpC,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,EAAE;YAC9D,kDAAkD;YAClD,UAAU,GAAG,CAAC,gCAAkB,CAAC,UAAU,CAAC,CAAC,CAAC;SAC/C;QACD,UAAU,GAAG,UAAqB,CAAC;QACnC,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;YACzB,MAAM,IAAI,mBAAU,CAChB,kEAAkE;iBAClE,UAAQ,UAAU,CAAC,MAAM,eAAY,CAAA,CAAC,CAAC;SAC5C;QAED,wEAAwE;QACxE,UAAU;QACV,IAAI,UAAU,GAAa,EAAE,CAAC;QAC9B,KAAoB,UAAU,EAAV,yBAAU,EAAV,wBAAU,EAAV,IAAU,EAAE;YAA3B,IAAM,KAAK,mBAAA;YACd,IAAI,KAAK,IAAI,IAAI,IAAI,KAAK,CAAC,CAAC,CAAC,KAAK,IAAI,EAAE;gBACtC,UAAU,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;aAC3B;SACF;QACD,UAAU,GAAG,aAAa,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;QAC9C,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;YACzB,MAAM,IAAI,mBAAU,CAChB,oDAAoD;iBACpD,8BAA4B,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,MAAG,CAAA,CAAC,CAAC;SAChE;QAED,IAAI,WAAW,GACX,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QAC1D,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;YAC1C,IAAM,KAAK,GAAG,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YACpE,WAAW,GAAG,IAAI,CAAC,+BAA+B,CAAC,WAAW,EAAE,KAAK,CAAC,CAAC;SACxE;QACD,2EAA2E;QAC3E,iBAAiB;QACjB,IAAM,QAAQ,GAAG,UAAU,CAAC,GAAG,CAAC,UAAA,KAAK,IAAI,OAAA,KAAK,CAAC,MAAM,EAAZ,CAAY,CAAC,CAAC;QACvD,IAAI,UAAU,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAC/B,aAAa,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC,MAAM,KAAK,CAAC,EAAE;YAC/C,IAAI,CAAC,eAAe,GAAG,KAAK,CAAC;SAC9B;aAAM;YACL,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC;SAC7B;IACH,CAAC;IAED,oBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAA5C,iBAmEC;QAlEC,OAAO,gBAAI,CAAC;YACV,MAAM,GAAG,MAAkB,CAAC;YAC5B,IAAI,KAAI,CAAC,eAAe,EAAE;gBACxB,IAAM,cAAc,GAAa,EAAE,CAAC;gBACpC,IAAM,SAAS,GAAG,MAAM,CAAC,GAAG,CAAC,UAAA,KAAK,IAAI,OAAA,KAAK,CAAC,IAAI,EAAV,CAAU,CAAC,CAAC;gBAClD,IAAI,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;oBAClC,sEAAsE;oBACtE,kDAAkD;oBAClD,IAAM,OAAO,GAAG,SAAS,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC;oBACzC,KAAc,UAAM,EAAN,iBAAM,EAAN,oBAAM,EAAN,IAAM,EAAE;wBAAjB,IAAI,CAAC,eAAA;wBACR,IAAM,KAAK,GAAG,CAAC,CAAC,IAAI,CAAC;wBACrB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,GAAG,KAAK,EAAE,EAAE,CAAC,EAAE;4BACxC,CAAC,GAAG,CAAC,CAAC,UAAU,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;yBACxB;wBACD,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;qBACxB;oBACD,OAAO,KAAI,CAAC,aAAa,CAAC,cAAc,CAAC,CAAC;iBAC3C;qBAAM;oBACL,iEAAiE;oBACjE,+DAA+D;oBAC/D,IAAI,UAAU,GAAG,KAAK,CAAC;oBACvB,KAAgB,UAAM,EAAN,iBAAM,EAAN,oBAAM,EAAN,IAAM,EAAE;wBAAnB,IAAM,CAAC,eAAA;wBACV,IAAM,KAAK,GAAG,CAAC,CAAC,IAAI,CAAC;wBACrB,IAAI,KAAK,IAAI,IAAI,EAAE;4BACjB,IAAM,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC;4BACvB,IAAM,SAAS,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;4BAC5B,IAAM,QAAQ,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC;4BACrD,IAAI,WAAW,GAAG,CAAC,CAAC,OAAO,CACvB,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;4BAC9D,WAAW,GAAG,GAAG,CAAC,SAAS,CAAC,WAAW,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;4BACjD,WAAW,GAAG,WAAW,CAAC,OAAO,CAAC,QAAQ,CAAC,CAAC;4BAC5C,cAAc,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;4BACjC,UAAU,GAAG,IAAI,CAAC;yBACnB;6BAAM,IAAI,KAAK,GAAG,CAAC,EAAE;4BACpB,IAAM,IAAI,GAAG,SAAS,CAAC,KAAK,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;4BACnD,cAAc,CAAC,IAAI,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC;4BAC5C,UAAU,GAAG,IAAI,CAAC;yBACnB;6BAAM;4BACL,+DAA+D;4BAC/D,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;yBACxB;qBACF;oBACD,IAAI,CAAC,GAAG,KAAI,CAAC,aAAa,CAAC,cAAc,CAAC,CAAC;oBAC3C,IAAM,KAAK,GAAG,CAAC,CAAC,IAAI,CAAC;oBACrB,IAAI,UAAU,EAAE;wBACd,kEAAkE;wBAClE,OAAO;wBACP,IAAI,KAAK,IAAI,IAAI,EAAE;4BACjB,IAAM,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC;4BACvB,IAAM,OAAK,GAAG,MAAM,CAAC,MAAM,CAAC;4BAC5B,IAAM,SAAS,GAAG,MAAM,CAAC,OAAK,GAAG,CAAC,CAAC,CAAC;4BACpC,IAAM,QAAQ,GACV,CAAC,SAAS,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;4BAC3D,CAAC,GAAG,GAAG,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;iCAC5C,OAAO,CAAC,QAAQ,CAAC,CAAC;yBAC5B;6BAAM,IAAI,KAAK,GAAG,CAAC,EAAE;4BACpB,IAAM,IAAI,GAAG,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC,SAAS,CAAC,KAAK,CAAC,CAAC,EAAE,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC;4BAC/D,CAAC,GAAG,GAAG,CAAC,SAAS,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;yBAC5B;qBACF;oBACD,OAAO,CAAC,CAAC;iBACV;aACF;iBAAM;gBACL,OAAO,KAAI,CAAC,aAAa,CAAC,MAAM,CAAC,CAAC;aACnC;QACH,CAAC,CAAC,CAAC;IACL,CAAC;IAED,kCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,UAAU,GAAG,UAAqB,CAAC;QACnC,IAAI,WAAkB,CAAC;QACvB,IAAI,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;YACzB,WAAW,GAAG,IAAI,CAAC;SACpB;aAAM;YACL,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;SACtC;QACD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;YAC1C,IAAM,KAAK,GAAG,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YACpE,WAAW,GAAG,IAAI,CAAC,+BAA+B,CAAC,WAAW,EAAE,KAAK,CAAC,CAAC;SACxE;QAED,IAAI,UAAU,GAAa,EAAE,CAAC;QAC9B,KAAoB,UAAU,EAAV,yBAAU,EAAV,wBAAU,EAAV,IAAU,EAAE;YAA3B,IAAM,KAAK,mBAAA;YACd,IAAI,KAAK,IAAI,IAAI,IAAI,KAAK,CAAC,CAAC,CAAC,KAAK,IAAI,EAAE;gBACtC,UAAU,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;aAC3B;SACF;QACD,UAAU,GAAG,aAAa,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;QAC9C,IAAI,UAAU,CAAC,MAAM,KAAK,CAAC,EAAE;YAC3B,WAAW,GAAG,UAAU,CAAC,MAAM,CAAC,WAAW,CAAC,CAAC;SAC9C;aAAM;YACL,WAAW,GAAG,CAAC,IAAI,CAAC,CAAC,MAAM,CAAC,WAAW,CAAC,CAAC;SAC1C;QACD,OAAO,WAAW,CAAC;IACrB,CAAC;IAED,2BAAW,GAAX,UAAY,MAAuB,EAAE,IAAsB;QACzD,OAAO,GAAG,CAAC,IAAI,CAAC;YACd,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,OAAO,IAAI,CAAC;aACb;YACD,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;gBACxB,MAAM,IAAI,mBAAU,CAAC,2BAA2B,CAAC,CAAC;aACnD;YACD,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;gBAC1B,MAAM,IAAI,mBAAU,CAAC,6BAA6B,CAAC,CAAC;aACrD;YACD,IAAI,IAAI,CAAC,MAAM,KAAK,MAAM,CAAC,MAAM,EAAE;gBACjC,MAAM,IAAI,mBAAU,CAChB,8DAA8D;oBAC9D,qCAAqC;qBACrC,MAAI,MAAM,CAAC,MAAM,YAAO,IAAI,CAAC,MAAM,MAAG,CAAA,CAAC,CAAC;aAC7C;YACD,IAAI,IAAI,CAAC,KAAK,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,IAAI,IAAI,EAAT,CAAS,CAAC,EAAE;gBAC9B,OAAO,IAAI,CAAC;aACb;YACD,IAAI,GAAG,IAAI,CAAC,GAAG,CAAC,UAAA,CAAC,IAAI,OAAA,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,EAAE,CAAC,CAAC,EAApC,CAAoC,CAAC,CAAC;YAC3D,IAAI,MAAM,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;YACrB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,GAAG,CAAC,EAAE,EAAE,CAAC,EAAE;gBACxC,MAAM,GAAG,GAAG,CAAC,UAAU,CAAC,MAAM,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;aAC1C;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IACH,YAAC;AAAD,CAAC,AAlOD,CAAoC,gBAAK,GAkOxC;AAlOqB,sBAAK;AAoO3B;IAAyB,uBAAK;IAG5B,aAAY,IAAgB;eAC1B,kBAAM,IAAI,CAAC;IACb,CAAC;IAES,2BAAa,GAAvB,UAAwB,MAAgB;QACtC,OAAO,gBAAI,CAAC;YACV,IAAI,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC;YAC/B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACrC;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAdD,kBAAkB;IACX,aAAS,GAAG,KAAK,CAAC;IAc3B,UAAC;CAAA,AAhBD,CAAyB,KAAK,GAgB7B;AAhBY,kBAAG;AAiBhB,yBAAa,CAAC,aAAa,CAAC,GAAG,CAAC,CAAC;AAEjC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6CG;AACH,SAAgB,GAAG,CAAC,MAA4C;IAE9D,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QACzB,IAAM,KAAK,GAAG,IAAI,GAAG,CAAC,EAAE,CAAC,CAAC;QAC1B,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,CAA4B,CAAC;KACvD;SAAM;QACL,OAAO,IAAI,GAAG,CAAC,MAAM,CAAC,CAAC;KACxB;AACH,CAAC;AARD,kBAQC;AAED;IAA8B,4BAAK;IAGjC,kBAAY,IAAgB;eAC1B,kBAAM,IAAI,CAAC;IACb,CAAC;IAES,gCAAa,GAAvB,UAAwB,MAAgB;QACtC,OAAO,gBAAI,CAAC;YACV,IAAI,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC;YAC/B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACrC;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAdD,kBAAkB;IACX,kBAAS,GAAG,UAAU,CAAC;IAchC,eAAC;CAAA,AAhBD,CAA8B,KAAK,GAgBlC;AAhBY,4BAAQ;AAiBrB,yBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6CG;AACH,SAAgB,QAAQ,CAAC,MAA4C;IAEnE,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QACzB,IAAM,KAAK,GAAG,IAAI,QAAQ,CAAC,EAAE,CAAC,CAAC;QAC/B,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,CAA4B,CAAC;KACvD;SAAM;QACL,OAAO,IAAI,QAAQ,CAAC,MAAM,CAAC,CAAC;KAC7B;AACH,CAAC;AARD,4BAQC;AAED;IAA6B,2BAAK;IAGhC,iBAAY,IAAgB;eAC1B,kBAAM,IAAI,CAAC;IACb,CAAC;IAES,+BAAa,GAAvB,UAAwB,MAAgB;QACtC,OAAO,gBAAI,CAAC;YACV,IAAI,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC;YAC/B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,MAAM,GAAG,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACrC;YACD,OAAO,GAAG,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;QAC5C,CAAC,CAAC,CAAC;IACL,CAAC;IAdD,kBAAkB;IACX,iBAAS,GAAG,SAAS,CAAC;IAc/B,cAAC;CAAA,AAhBD,CAA6B,KAAK,GAgBjC;AAhBY,0BAAO;AAiBpB,yBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA8CG;AACH,SAAgB,OAAO,CAAC,MAA4C;IAElE,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QACzB,IAAM,KAAK,GAAG,IAAI,OAAO,CAAC,EAAE,CAAC,CAAC;QAC9B,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,CAA4B,CAAC;KACvD;SAAM;QACL,OAAO,IAAI,OAAO,CAAC,MAAM,CAAC,CAAC;KAC5B;AACH,CAAC;AARD,0BAQC;AAED;IAA6B,2BAAK;IAGhC,iBAAY,IAAgB;eAC1B,kBAAM,IAAI,CAAC;IACb,CAAC;IAES,+BAAa,GAAvB,UAAwB,MAAgB;QACtC,OAAO,gBAAI,CAAC;YACV,IAAI,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACvB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,MAAM,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACzC;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAdD,kBAAkB;IACX,iBAAS,GAAG,SAAS,CAAC;IAc/B,cAAC;CAAA,AAhBD,CAA6B,KAAK,GAgBjC;AAhBY,0BAAO;AAiBpB,yBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6CG;AACH,SAAgB,OAAO,CAAC,MAA4C;IAElE,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QACzB,IAAM,KAAK,GAAG,IAAI,OAAO,CAAC,EAAE,CAAC,CAAC;QAC9B,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,CAA4B,CAAC;KACvD;SAAM;QACL,OAAO,IAAI,OAAO,CAAC,MAAM,CAAC,CAAC;KAC5B;AACH,CAAC;AARD,0BAQC;AAED;IAA6B,2BAAK;IAGhC,iBAAY,IAAgB;eAC1B,kBAAM,IAAI,CAAC;IACb,CAAC;IAES,+BAAa,GAAvB,UAAwB,MAAgB;QACtC,OAAO,gBAAI,CAAC;YACV,IAAI,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACvB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,MAAM,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACzC;YACD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAdD,kBAAkB;IACX,iBAAS,GAAG,SAAS,CAAC;IAc/B,cAAC;CAAA,AAhBD,CAA6B,KAAK,GAgBjC;AAhBY,0BAAO;AAiBpB,yBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6CG;AACH,SAAgB,OAAO,CAAC,MAA4C;IAElE,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QACzB,IAAM,KAAK,GAAG,IAAI,OAAO,CAAC,EAAE,CAAC,CAAC;QAC9B,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,CAA4B,CAAC;KACvD;SAAM;QACL,OAAO,IAAI,OAAO,CAAC,MAAM,CAAC,CAAC;KAC5B;AACH,CAAC;AARD,0BAQC;AASD;IAAiC,+BAAK;IAMpC,qBAAY,IAA2B;QAAvC,YACE,kBAAM,IAAI,CAAC,SAOZ;QAXQ,kBAAY,GAAG,CAAC,CAAC,CAAC;QAKzB,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,EAAE,CAAC;SACX;QACD,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,YAAY,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;QAC9D,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,eAAe,GAAG,KAAK,CAAC;;IAC/B,CAAC;IAED,2BAAK,GAAL,UAAM,UAAyB;QAC7B,qCAAqC;QACrC,IAAI,CAAC,CAAC,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YAC5D,UAAU,CAAC,MAAM,KAAK,CAAC,EAAE;YAC3B,MAAM,IAAI,mBAAU,CAChB,iEAAiE;gBACjE,QAAQ,CAAC,CAAC;SACf;QACD,UAAU,GAAG,UAAqB,CAAC;QAEnC,IAAI,YAAY,GAAG,IAAI,CAAC;QACxB,KAAoB,UAAU,EAAV,yBAAU,EAAV,wBAAU,EAAV,IAAU,EAAE;YAA3B,IAAM,KAAK,mBAAA;YACd,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,YAAY,GAAG,KAAK,CAAC;gBACrB,MAAM;aACP;SACF;QACD,IAAI,YAAY,EAAE;YAChB,OAAO;SACR;QAED,IAAM,QAAQ,GAAY,EAAE,CAAC;QAC7B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;YAC1C,IAAM,sBAAsB,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC;YACrD,sBAAsB,CAAC,MAAM,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC;YAC5C,IAAI,MAAM,GAAG,KAAK,CAAC;YACnB,KAAoB,UAAQ,EAAR,qBAAQ,EAAR,sBAAQ,EAAR,IAAQ,EAAE;gBAAzB,IAAM,KAAK,iBAAA;gBACd,IAAI,gBAAI,CAAC,WAAW,CAAC,KAAK,EAAE,sBAAsB,CAAC,EAAE;oBACnD,MAAM,GAAG,IAAI,CAAC;oBACd,MAAM;iBACP;aACF;YACD,IAAI,CAAC,MAAM,EAAE;gBACX,QAAQ,CAAC,IAAI,CAAC,sBAAsB,CAAC,CAAC;aACvC;SACF;QACD,IAAI,QAAQ,CAAC,MAAM,GAAG,CAAC,EAAE;YACvB,MAAM,IAAI,mBAAU,CAChB,6DAA6D;gBAC7D,gDAAgD;gBAChD,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,CAAC,CAAC;SACjC;IACH,CAAC;IAES,mCAAa,GAAvB,UAAwB,MAAgB;QAAxC,iBAIC;QAHC,OAAO,gBAAI,CAAC;YACV,OAAO,CAAC,CAAC,WAAW,CAAC,MAAM,EAAE,KAAI,CAAC,IAAI,CAAC,CAAC;QAC1C,CAAC,CAAC,CAAC;IACL,CAAC;IAED,wCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,IAAI,CAAC,CAAC,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE;YAChE,MAAM,IAAI,mBAAU,CAChB,6DAA6D,CAAC,CAAC;SACpE;QACD,IAAM,WAAW,GAAG,UAAqB,CAAC;QAC1C,IAAM,WAAW,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC;QAC3C,IAAM,IAAI,GAAG,IAAI,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;QACxE,qEAAqE;QACrE,sBAAsB;QACtB,KAAoB,UAAoB,EAApB,KAAA,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,EAApB,cAAoB,EAApB,IAAoB,EAAE;YAArC,IAAM,KAAK,SAAA;YACd,IAAI,WAAW,CAAC,IAAI,CAAC,IAAI,IAAI,IAAI,KAAK,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;gBACpD,WAAW,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC;gBACzB,MAAM;aACP;YACD,WAAW,CAAC,IAAI,CAAC,IAAI,KAAK,CAAC,IAAI,CAAC,CAAC;SAClC;QACD,OAAO,WAAW,CAAC;IACrB,CAAC;IAED,iCAAW,GAAX,UAAY,MAAuB,EAAE,IAAsB;QAA3D,iBAyCC;QAxCC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,OAAO,IAAI,CAAC;SACb;QACD,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;YACxB,MAAM,IAAI,mBAAU,CAAC,2CAA2C,CAAC,CAAC;SACnE;QACD,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;YAC1B,MAAM,IAAI,mBAAU,CAAC,6CAA6C,CAAC,CAAC;SACrE;QACD,IAAI,IAAI,CAAC,MAAM,KAAK,MAAM,CAAC,MAAM,EAAE;YACjC,MAAM,IAAI,mBAAU,CAChB,qCAAmC,IAAI,CAAC,MAAM,OAAI;iBAClD,+BAA6B,MAAM,CAAC,MAAM,MAAG,CAAA,CAAC,CAAC;SACpD;QACD,OAAO,GAAG,CAAC,IAAI,CAAC;YACd,IAAI,YAAY,GAAG,IAAI,CAAC;YACxB,IAAI,CAAC,OAAO,CAAC,UAAA,CAAC;gBACZ,IAAI,CAAC,IAAI,IAAI,EAAE;oBACb,YAAY,GAAG,KAAK,CAAC;oBACrB,OAAO;iBACR;YACH,CAAC,CAAC,CAAC;YACH,IAAI,YAAY,EAAE;gBAChB,OAAO,IAAI,CAAC;aACb;YACD,IAAM,WAAW,GAAa,EAAE,CAAC;YACjC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,EAAE;gBACtC,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;oBACnB,8CAA8C;oBAC9C,WAAW,CAAC,IAAI,CAAC,GAAG,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,CAAC;iBAC1D;qBAAM,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,IAAI,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE;oBACxC,6CAA6C;oBAC7C,WAAW,CAAC,IAAI,CAAC,GAAG,CAAC,UAAU,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;iBAC/C;qBAAM;oBACL,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;iBAC3B;aACF;YACD,IAAM,iBAAiB,GAAG,GAAG,CAAC,MAAM,CAAC,WAAW,EAAE,KAAI,CAAC,IAAI,CAAC,CAAC;YAC7D,OAAO,GAAG,CAAC,GAAG,CAAC,iBAAiB,EAAE,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;QAC/C,CAAC,CAAC,CAAC;IACL,CAAC;IAED,+BAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,MAAM,EAAE,IAAI,CAAC,IAAI;SAClB,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAvID,kBAAkB;IACX,qBAAS,GAAG,aAAa,CAAC;IAuInC,kBAAC;CAAA,AAzID,CAAiC,KAAK,GAyIrC;AAzIY,kCAAW;AA0IxB,yBAAa,CAAC,aAAa,CAAC,WAAW,CAAC,CAAC;AAEzC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA+CG;AACH,SAAgB,WAAW,CAAC,MACoB;IAC9C,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QACzB,IAAM,KAAK,GAAG,IAAI,WAAW,CAAC,EAAE,CAAC,CAAC;QAClC,OAAO,KAAK,CAAC,KAAK,CAAC,MAAM,CAA4B,CAAC;KACvD;SAAM;QACL,OAAO,IAAI,WAAW,CAAC,MAAM,CAAC,CAAC;KAChC;AACH,CAAC;AARD,kCAQC;AAoBD;;;;;;;;GAQG;AACH,SAAS,aAAa,CAAC,IAAY,EAAE,GAAW;IAC9C,OAAO,IAAI,GAAG,CAAC,EAAE;QACf,IAAI,IAAI,GAAG,CAAC;KACb;IACD,OAAO,IAAI,CAAC;AACd,CAAC;AAED,SAAS,QAAQ,CAAC,CAAS,EAAE,CAAS,EAAE,IAA6B;IACnE,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE;QAC5C,MAAM,IAAI,4BAAmB,CACzB,kEAAkE,CAAC,CAAC;KACzE;IACD,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,CAAC,CAAC,KAAK,CAAC,MAAM,IAAI,CAAC,EACnB,cAAM,OAAA,8CAA8C;SAChD,aAAW,CAAC,CAAC,KAAK,CAAC,MAAQ,CAAA,EADzB,CACyB,CAAC,CAAC;IACrC,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,CAAC,CAAC,KAAK,CAAC,MAAM,IAAI,CAAC,EACnB,cAAM,OAAA,8CAA8C;SAChD,aAAW,CAAC,CAAC,KAAK,CAAC,MAAQ,CAAA,EADzB,CACyB,CAAC,CAAC;IAErC,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;QAC5B,IAAI,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC;KACrB;IAED,IAAI,CAAC,CAAC,KAAK,KAAK,WAAW,IAAI,CAAC,CAAC,KAAK,KAAK,WAAW,EAAE;QACtD,MAAM,IAAI,4BAAmB,CACzB,6DAA6D,CAAC,CAAC;KACpE;IAED,IAAM,KAAK,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC;IAC7B,IAAM,KAAK,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC;IAC7B,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,sCAAsC;QACtC,IAAI,GAAG,CAAC,KAAK,GAAG,CAAC,EAAE,KAAK,GAAG,CAAC,CAAC,CAAC;KAC/B;IACD,IAAM,SAAS,GAAG,IAAwB,CAAC;IAE3C,OAAO,GAAG,CAAC,IAAI,CAAC;QACd,IAAI,IAAY,CAAC;QACjB,IAAI,KAAK,GAAG,KAAK,EAAE;YACjB,IAAI,GAAG,KAAK,GAAG,KAAK,CAAC;YACrB,IAAM,SAAS,GAAU,EAAE,CAAC;YAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,EAAE,EAAE,CAAC,EAAE;gBAC7B,SAAS,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACnB;YACD,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,CAAC;SAC1C;aAAM,IAAI,KAAK,GAAG,KAAK,EAAE;YACxB,IAAI,GAAG,KAAK,GAAG,KAAK,CAAC;YACrB,IAAM,SAAS,GAAU,EAAE,CAAC;YAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,EAAE,EAAE,CAAC,EAAE;gBAC7B,SAAS,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACnB;YACD,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,CAAC;SAC1C;aAAM;YACL,IAAI,GAAG,CAAC,CAAC;SACV;QAED,IAAI,GAAW,CAAC;QAChB,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,KAAK,CAAC,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;YAChD,IAAI,SAAS,CAAC,CAAC,CAAC,KAAK,SAAS,CAAC,CAAC,CAAC,EAAE;gBACjC,GAAG,GAAG,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;aACxC;iBAAM;gBACL,GAAG,GAAG,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;aAC1D;SACF;aAAM;YACL,IAAM,IAAI,GAAG,SAAS,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;YACjD,IAAM,IAAI,GAAG,SAAS,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;YACjD,GAAG,GAAG,CAAC,CAAC,MAAM,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;SAC/B;QAED,IAAI,IAAI,GAAG,CAAC,EAAE;YACZ,IAAI,GAAG,SAAQ,CAAC;YAChB,IAAI,KAAK,GAAG,KAAK,EAAE;gBACjB,GAAG,GAAG,KAAK,GAAG,KAAK,GAAG,CAAC,CAAC;aACzB;iBAAM;gBACL,GAAG,GAAG,KAAK,GAAG,CAAC,CAAC;aACjB;YACD,IAAM,WAAW,GAAa,EAAE,CAAC;YACjC,KAAK,IAAI,CAAC,GAAG,GAAG,EAAE,CAAC,GAAG,GAAG,GAAG,IAAI,EAAE,EAAE,CAAC,EAAE;gBACrC,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACrB;YACD,GAAG,GAAG,GAAG,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;SAChC;QACD,IAAI,GAAG,CAAC,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;YAC1B,GAAG,GAAG,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC;SACzB;QACD,OAAO,GAAG,CAAC;IACb,CAAC,CAAC,CAAC;AACL,CAAC;AAED;IAAyB,uBAAK;IAO5B,aAAY,IAAkB;QAA9B,YACE,kBAAM,IAAI,CAAC,SAKZ;QAJC,KAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;QACtB,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,SAAS,IAAI,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC;QACjE,KAAI,CAAC,eAAe,GAAG,IAAI,CAAC;QAC5B,KAAI,CAAC,eAAe,GAAG,KAAK,CAAC;;IAC/B,CAAC;IAED,mBAAK,GAAL,UAAM,UAAyB;QAC7B,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,IAAI,UAAU,CAAC,MAAM,KAAK,CAAC;YAChD,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,EAChE,cAAM,OAAA,+DAA+D,EAA/D,CAA+D,CAAC,CAAC;QAC3E,IAAM,MAAM,GAAG,UAAU,CAAC,CAAC,CAAU,CAAC;QACtC,IAAM,MAAM,GAAG,UAAU,CAAC,CAAC,CAAU,CAAC;QACtC,IAAI,MAAM,CAAC,MAAM,GAAG,CAAC,IAAI,MAAM,CAAC,MAAM,GAAG,CAAC,EAAE;YAC1C,MAAM,IAAI,4BAAmB,CACzB,8DAA8D,CAAC,CAAC;SACrE;QAED,IAAM,IAAI,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;QAChD,IAAI,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,KAAK,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAAE;YACvC,MAAM,IAAI,mBAAU,CAChB,6BAA6B;iBAC1B,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,aAAQ,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAG,CAAA,CAAC,CAAC;SAClD;IACH,CAAC;IAES,2BAAa,GAAvB,UAAwB,MAAgB;QACtC,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;YACvB,MAAM,IAAI,mBAAU,CAChB,oDAAoD;iBACpD,kBAAgB,MAAM,CAAC,MAAM,eAAY,CAAA,CAAC,CAAC;SAChD;QAED,IAAI,EAAE,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;QACnB,IAAI,EAAE,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;QACnB,IAAI,IAAsB,CAAC;QAC3B,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;YAC7B,IAAI,GAAG;gBACL,aAAa,CAAC,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,KAAK,CAAC,MAAM,CAAC;gBACzC,aAAa,CAAC,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,KAAK,CAAC,MAAM,CAAC;aAC1C,CAAC;SACH;aAAM;YACL,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC,GAAG,CACT,UAAC,IAAI,EAAE,CAAC,IAAK,OAAA,aAAa,CACtB,IAAI,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC,EADpB,CACoB,CAAqB,CAAC;SACnE;QACD,IAAI,IAAI,CAAC,SAAS,EAAE;YAClB,EAAE,GAAG,oBAAW,CAAC,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;YAC9B,EAAE,GAAG,oBAAW,CAAC,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;SAC/B;QACD,OAAO,QAAQ,CAAC,EAAE,EAAE,EAAE,EAAE,IAAI,CAAC,CAAC;IAChC,CAAC;IAEO,2BAAa,GAArB,UAAsB,MAAa,EAAE,MAAa;QAChD,IAAI,IAAc,CAAC;QACnB,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;YAC7B,mCAAmC;YACnC,IAAI,GAAG;gBACL,aAAa,CAAC,IAAI,CAAC,IAAI,EAAE,MAAM,CAAC,MAAM,CAAC;gBACvC,aAAa,CAAC,IAAI,CAAC,IAAI,EAAE,MAAM,CAAC,MAAM,CAAC;aACxC,CAAC;SACH;aAAM;YACL,uCAAuC;YACvC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;SAClB;QACD,OAAO,IAAI,CAAC;IACd,CAAC;IAED,gCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,GAAG,CAAC,IAAI,CAAC,MAAM,CACX,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,IAAI,UAAU,CAAC,MAAM,KAAK,CAAC;YAChD,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,EAChE,cAAM,OAAA,+DAA+D,EAA/D,CAA+D,CAAC,CAAC;QAC3E,IAAM,MAAM,GAAI,UAAU,CAAC,CAAC,CAAW,CAAC,KAAK,EAAE,CAAC;QAChD,IAAM,MAAM,GAAI,UAAU,CAAC,CAAC,CAAW,CAAC,KAAK,EAAE,CAAC;QAChD,IAAI,MAAM,CAAC,MAAM,GAAG,CAAC,IAAI,MAAM,CAAC,MAAM,GAAG,CAAC,EAAE;YAC1C,MAAM,IAAI,4BAAmB,CACzB,8DAA8D,CAAC,CAAC;SACrE;QAED,IAAM,IAAI,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;QAChD,MAAM,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAC1B,MAAM,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAC1B,MAAM,CAAC,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QACpB,IAAM,WAAW,GAAG,MAAM,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;QAC1C,IAAI,WAAW,CAAC,MAAM,KAAK,CAAC,EAAE;YAC5B,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;SACrB;QACD,OAAO,WAAW,CAAC;IACrB,CAAC;IAED,yBAAW,GAAX,UAAY,MAAuB,EAAE,IAAsB;QACzD,OAAO,IAAI,CAAC;IACd,CAAC;IAED,uBAAS,GAAT;QACE,IAAM,MAAM,GAA6B;YACvC,MAAM,EAAE,IAAI,CAAC,IAAI;YACjB,WAAW,EAAE,IAAI,CAAC,SAAS;SAC5B,CAAC;QACF,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA/GD,kBAAkB;IACX,aAAS,GAAG,KAAK,CAAC;IA+G3B,UAAC;CAAA,AAjHD,CAAyB,KAAK,GAiH7B;AAjHY,kBAAG;AAkHhB,yBAAa,CAAC,aAAa,CAAC,GAAG,CAAC,CAAC;AAEjC,8DAA8D","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Merge Layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy, util} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {l2Normalize} from '../losses';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as mathUtils from '../utils/math_utils';\nimport {getExactlyOneShape} from '../utils/types_utils';\n\n/**\n * Generic Merge layer for element-wise merge functions.\n *\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\n */\nexport abstract class Merge extends Layer {\n  protected reshapeRequired: boolean;\n\n  constructor(args?: LayerArgs) {\n    super(args || {});\n    this.supportsMasking = true;\n  }\n\n  /**\n   * Logic for merging multiple tensors, to be overridden by subclasses.\n   * @param inputs\n   */\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    throw new NotImplementedError();\n  }\n\n  /**\n   * Computes the shape of the result of an elementwise operation.\n   *\n   * @param shape1: Shape of the first tensor.\n   * @param shape2: Shape of the second tensor.\n   * @returns Expected output shape when an elementwise operation is carried\n   *   out on 2 tensors with shapes `shape1` and `shape2`.\n   * @throws ValueError: If `shape1` and `shape2` are not compatible for\n   *   element-wise operations.\n   */\n  private computeElementwiseOpOutputShape(shape1: Shape, shape2: Shape): Shape {\n    if (shape1 == null || shape2 == null) {\n      return null;\n    } else if (shape1.length < shape2.length) {\n      return this.computeElementwiseOpOutputShape(shape2, shape1);\n    } else if (shape2.length === 0) {\n      return shape1;\n    }\n    const outputShape: Shape = shape1.slice(0, shape1.length - shape2.length);\n    for (let k = 0; k < shape2.length; ++k) {\n      const i = shape1[shape1.length - shape2.length + k];\n      const j = shape2[k];\n      if (i == null || j == null || i < 0 || j < 0) {\n        outputShape.push(null);\n      } else if (i === 1) {\n        outputShape.push(j);\n      } else if (j === 1) {\n        outputShape.push(i);\n      } else {\n        if (i !== j) {\n          throw new ValueError(\n              'Operands could not be broadcast together with shapes ' +\n              JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n        }\n        outputShape.push(i);\n      }\n    }\n    return outputShape;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    // Used purely for shape validation.\n    if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n      // Make sure that inputShape is an Array of shape.\n      inputShape = [getExactlyOneShape(inputShape)];\n    }\n    inputShape = inputShape as Shape[];\n    if (inputShape.length < 2) {\n      throw new ValueError(\n          'A merge layer should be called on an Array of at least 2 inputs.' +\n          ` Got ${inputShape.length} input(s).`);\n    }\n\n    // Make sure that there is at most one unique batch size among the input\n    // shapes.\n    let batchSizes: number[] = [];\n    for (const shape of inputShape) {\n      if (shape != null && shape[0] !== null) {\n        batchSizes.push(shape[0]);\n      }\n    }\n    batchSizes = generic_utils.unique(batchSizes);\n    if (batchSizes.length > 1) {\n      throw new ValueError(\n          `Can not merge tensors with different batch sizes. ` +\n          `Got tensors with shapes: ${JSON.stringify(inputShape)}.`);\n    }\n\n    let outputShape: Shape =\n        inputShape[0] == null ? null : inputShape[0].slice(1);\n    for (let i = 1; i < inputShape.length; ++i) {\n      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n    }\n    // If the inputs have different ranks, we have to reshape them to make them\n    // broadcastable.\n    const allRanks = inputShape.map(shape => shape.length);\n    if (inputShape.indexOf(null) === -1 &&\n        generic_utils.unique(allRanks).length === 1) {\n      this.reshapeRequired = false;\n    } else {\n      this.reshapeRequired = true;\n    }\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      if (this.reshapeRequired) {\n        const reshapedInputs: Tensor[] = [];\n        const inputDims = inputs.map(input => input.rank);\n        if (inputDims.indexOf(null) === -1) {\n          // If ranks of all inputs are available, we simply expand each of them\n          // at axis=1 until all of them have the same rank.\n          const maxNDim = mathUtils.max(inputDims);\n          for (let x of inputs) {\n            const xNDim = x.rank;\n            for (let k = 0; k < maxNDim - xNDim; ++k) {\n              x = K.expandDims(x, 1);\n            }\n            reshapedInputs.push(x);\n          }\n          return this.mergeFunction(reshapedInputs);\n        } else {\n          // Transpose all inputs so that batch size is the last dimension.\n          // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n          let transposed = false;\n          for (const x of inputs) {\n            const xNDim = x.rank;\n            if (xNDim == null) {\n              const xShape = x.shape;\n              const batchSize = xShape[0];\n              const newShape = xShape.slice(1).concat([batchSize]);\n              let xTransposed = x.reshape(\n                  [batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n              xTransposed = tfc.transpose(xTransposed, [1, 0]);\n              xTransposed = xTransposed.reshape(newShape);\n              reshapedInputs.push(xTransposed);\n              transposed = true;\n            } else if (xNDim > 1) {\n              const dims = mathUtils.range(1, xNDim).concat([0]);\n              reshapedInputs.push(tfc.transpose(x, dims));\n              transposed = true;\n            } else {\n              // We don't transpose inputs if they are 1D vectors or scalars.\n              reshapedInputs.push(x);\n            }\n          }\n          let y = this.mergeFunction(reshapedInputs);\n          const yNDim = y.rank;\n          if (transposed) {\n            // If inputs have been transposed, we have to transpose the output\n            // too.\n            if (yNDim == null) {\n              const yShape = y.shape;\n              const yNDim = yShape.length;\n              const batchSize = yShape[yNDim - 1];\n              const newShape =\n                  [batchSize].concat(yShape.slice(0, yShape.length - 1));\n              y = tfc.transpose(y.reshape([-1, batchSize]), [1, 0])\n                      .reshape(newShape);\n            } else if (yNDim > 1) {\n              const dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n              y = tfc.transpose(y, dims);\n            }\n          }\n          return y;\n        }\n      } else {\n        return this.mergeFunction(inputs);\n      }\n    });\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = inputShape as Shape[];\n    let outputShape: Shape;\n    if (inputShape[0] == null) {\n      outputShape = null;\n    } else {\n      outputShape = inputShape[0].slice(1);\n    }\n    for (let i = 1; i < inputShape.length; ++i) {\n      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n    }\n\n    let batchSizes: number[] = [];\n    for (const shape of inputShape) {\n      if (shape != null && shape[0] !== null) {\n        batchSizes.push(shape[0]);\n      }\n    }\n    batchSizes = generic_utils.unique(batchSizes);\n    if (batchSizes.length === 1) {\n      outputShape = batchSizes.concat(outputShape);\n    } else {\n      outputShape = [null].concat(outputShape);\n    }\n    return outputShape;\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor {\n    return tfc.tidy(() => {\n      if (mask == null) {\n        return null;\n      }\n      if (!Array.isArray(mask)) {\n        throw new ValueError('`mask` should be an Array');\n      }\n      if (!Array.isArray(inputs)) {\n        throw new ValueError('`inputs` should be an Array');\n      }\n      if (mask.length !== inputs.length) {\n        throw new ValueError(\n            `The Array 'inputs' and 'mask' are expected to have the same ` +\n            `length, but have different lengths ` +\n            `(${inputs.length} vs ${mask.length})`);\n      }\n      if (mask.every(m => m == null)) {\n        return null;\n      }\n      mask = mask.map(m => m == null ? m : tfc.expandDims(m, 0));\n      let output = mask[0];\n      for (let i = 1; i < mask.length - 1; ++i) {\n        output = tfc.logicalAnd(output, mask[i]);\n      }\n      return output;\n    });\n  }\n}\n\nexport class Add extends Merge {\n  /** @nocollapse */\n  static className = 'Add';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0].clone();\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.add(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Add);\n\n/**\n * Calculate the element-wise sum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Add` layer, by using no input argument\n *    or a single configuration argument. The resultant `Add` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const addLayer = tf.layers.add();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = addLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.add([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.add([input1, input2]).print();\n * // Gives [[11, 22], [33, 44]].\n *\n */\nexport function add(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Add({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Add(config);\n  }\n}\n\nexport class Multiply extends Merge {\n  /** @nocollapse */\n  static className = 'Multiply';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0].clone();\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.mul(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Multiply);\n\n/**\n * Calculate the element-wise product of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Multiply` layer, by using no input argument\n *    or a single configuration argument. The resultant `Multiply` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const multiplyLayer = tf.layers.multiply();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = multiplyLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.multiply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.multiply([input1, input2]).print();\n * // Gives [[10, 40], [90, 160]].\n *\n */\nexport function multiply(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Multiply({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Multiply(config);\n  }\n}\n\nexport class Average extends Merge {\n  /** @nocollapse */\n  static className = 'Average';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0].clone();\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.add(output, inputs[i]);\n      }\n      return tfc.mul(1 / inputs.length, output);\n    });\n  }\n}\nserialization.registerClass(Average);\n\n/**\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\n * shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Average` layer, by using no input argument\n *    or a single configuration argument. The resultant `Average` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const averageLayer = tf.layers.average();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = averageLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.average([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.average([input1, input2]).print();\n * // Gives [[5.5, 11], [16.5, 22]].\n *\n */\nexport function average(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Average({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Average(config);\n  }\n}\n\nexport class Maximum extends Merge {\n  /** @nocollapse */\n  static className = 'Maximum';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0];\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.maximum(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Maximum);\n\n/**\n * Calculate the element-wise maximum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Maximum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Maximum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const maximumLayer = tf.layers.maximum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = maximumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.maximum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.maximum([input1, input2]).print();\n * // Gives [[10, 20], [30, 40]].\n *\n */\nexport function maximum(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Maximum({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Maximum(config);\n  }\n}\n\nexport class Minimum extends Merge {\n  /** @nocollapse */\n  static className = 'Minimum';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0];\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.minimum(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Minimum);\n\n/**\n * Calculate the element-wise minimum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Minimum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Minimum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const minimumLayer = tf.layers.minimum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = minimumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.minimum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.minimum([input1, input2]).print();\n * // Gives [[1, 2], [3, 4]].\n *\n */\nexport function minimum(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Minimum({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Minimum(config);\n  }\n}\n\nexport declare interface ConcatenateLayerArgs extends LayerArgs {\n  /**\n   * Axis along which to concatenate.\n   */\n  axis?: number;\n}\n\nexport class Concatenate extends Merge {\n  /** @nocollapse */\n  static className = 'Concatenate';\n  readonly DEFAULT_AXIS = -1;\n  private readonly axis: number;\n\n  constructor(args?: ConcatenateLayerArgs) {\n    super(args);\n    if (args == null) {\n      args = {};\n    }\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    this.supportsMasking = true;\n    this.reshapeRequired = false;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    // Used purely for shape validation.]\n    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) ||\n        inputShape.length === 1) {\n      throw new ValueError(\n          'A `Concatenate` layer should be called on a list of at least 2 ' +\n          'inputs');\n    }\n    inputShape = inputShape as Shape[];\n\n    let allNoneShape = true;\n    for (const shape of inputShape) {\n      if (shape != null) {\n        allNoneShape = false;\n        break;\n      }\n    }\n    if (allNoneShape) {\n      return;\n    }\n\n    const shapeSet: Shape[] = [];\n    for (let i = 0; i < inputShape.length; ++i) {\n      const shapeWithoutConcatAxis = inputShape[i].slice();\n      shapeWithoutConcatAxis.splice(this.axis, 1);\n      let exists = false;\n      for (const shape of shapeSet) {\n        if (util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n          exists = true;\n          break;\n        }\n      }\n      if (!exists) {\n        shapeSet.push(shapeWithoutConcatAxis);\n      }\n    }\n    if (shapeSet.length > 1) {\n      throw new ValueError(\n          'A `Concatenate` layer requires inputs with matching shapes ' +\n          'except for the concat axis. Got input shapes: ' +\n          JSON.stringify(inputShape));\n    }\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      return K.concatenate(inputs, this.axis);\n    });\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n      throw new ValueError(\n          'A `Concatenate` layer should be called on a list of inputs.');\n    }\n    const inputShapes = inputShape as Shape[];\n    const outputShape = inputShapes[0].slice();\n    const axis = this.axis < 0 ? outputShape.length + this.axis : this.axis;\n    // Porting Note: the line above is because TypeScript doesn't support\n    //   negative indices.\n    for (const shape of inputShapes.slice(1)) {\n      if (outputShape[axis] == null || shape[axis] == null) {\n        outputShape[axis] = null;\n        break;\n      }\n      outputShape[axis] += shape[axis];\n    }\n    return outputShape;\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor {\n    if (mask == null) {\n      return null;\n    }\n    if (!Array.isArray(mask)) {\n      throw new ValueError('`mask` should be an array for Concatenate');\n    }\n    if (!Array.isArray(inputs)) {\n      throw new ValueError('`inputs` should be an array for Concatenate');\n    }\n    if (mask.length !== inputs.length) {\n      throw new ValueError(\n          `Mismatch in the length of mask (${mask.length}) ` +\n          `and the legnth of inputs (${inputs.length})`);\n    }\n    return tfc.tidy(() => {\n      let allNullMasks = true;\n      mask.forEach(m => {\n        if (m != null) {\n          allNullMasks = false;\n          return;\n        }\n      });\n      if (allNullMasks) {\n        return null;\n      }\n      const outputMasks: Tensor[] = [];\n      for (let i = 0; i < inputs.length; ++i) {\n        if (mask[i] == null) {\n          // Input is unmasked. Append all 1's to masks.\n          outputMasks.push(tfc.onesLike(inputs[i]).asType('bool'));\n        } else if (mask[i].rank < inputs[i].rank) {\n          // Mask is smaller than the input, expand it.\n          outputMasks.push(tfc.expandDims(mask[i], -1));\n        } else {\n          outputMasks.push(mask[i]);\n        }\n      }\n      const concatenatedMasks = tfc.concat(outputMasks, this.axis);\n      return tfc.all(concatenatedMasks, -1, false);\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'axis': this.axis,\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Concatenate);\n\n/**\n * Concatenate an `Array` of inputs.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\n *    or a single configuration argument. The resultant `Concatenate` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const concatLayer = tf.layers.concatenate();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = concatLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = tf.layers.concatenate([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\n * tf.layers.concatenate([input1, input2]).print();\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\n *\n */\nexport function concatenate(config?: SymbolicTensor[]|Tensor[]|\n                            ConcatenateLayerArgs): Layer|SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Concatenate({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Concatenate(config);\n  }\n}\n\nexport declare interface DotLayerArgs extends LayerArgs {\n  /**\n   * Axis or axes along which the dot product will be taken.\n   *\n   * Integer or an Array of integers.\n   */\n  axes: number|[number, number];\n\n  /**\n   * Whether to L2-normalize samples along the dot product axis\n   * before taking the dot product.\n   *\n   * If set to `true`, the output of the dot product isthe cosine\n   * proximity between the two samples.\n   */\n  normalize?: boolean;\n}\n\n/**\n * Interpretable potentially negative axis index.\n *\n * For example, given axis = -1, and dim = 3, this function will return 2.\n *\n * @param axis The axis index, may be a positive, zero or negative integer.\n * @param dim Total number of dimensions, a positive integer.\n * @returns A non-negative axis index equivalent to the input `axis`.\n */\nfunction interpretAxis(axis: number, dim: number): number {\n  while (axis < 0) {\n    axis += dim;\n  }\n  return axis;\n}\n\nfunction batchDot(x: Tensor, y: Tensor, axes: number|[number, number]): Tensor {\n  if (x.shape.length > 3 || y.shape.length > 3) {\n    throw new NotImplementedError(\n        'batchDot is not implemented for tensors of 4D or higher rank yet');\n  }\n  tfc.util.assert(\n      x.shape.length >= 2,\n      () => `batchDot requires the rank of x to be >= 2, ` +\n          `but got ${x.shape.length}`);\n  tfc.util.assert(\n      x.shape.length >= 2,\n      () => `batchDot requires the rank of y to be >= 2, ` +\n          `but got ${y.shape.length}`);\n\n  if (typeof axes === 'number') {\n    axes = [axes, axes];\n  }\n\n  if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n    throw new NotImplementedError(\n        'batchDot is not implemented for complex64-type Tensors yet.');\n  }\n\n  const xNDim = x.shape.length;\n  const yNDim = y.shape.length;\n  if (axes == null) {\n    // Behave like batchMatmul by default.\n    axes = [xNDim - 1, yNDim - 2];\n  }\n  const axesArray = axes as [number, number];\n\n  return tfc.tidy(() => {\n    let diff: number;\n    if (xNDim > yNDim) {\n      diff = xNDim - yNDim;\n      const diffShape: Shape = [];\n      for (let i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n      y = y.reshape(y.shape.concat(diffShape));\n    } else if (yNDim > xNDim) {\n      diff = yNDim - xNDim;\n      const diffShape: Shape = [];\n      for (let i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n      x = x.reshape(x.shape.concat(diffShape));\n    } else {\n      diff = 0;\n    }\n\n    let out: Tensor;\n    if (x.shape.length === 2 && y.shape.length === 2) {\n      if (axesArray[0] === axesArray[1]) {\n        out = x.mulStrict(y).sum(axesArray[0]);\n      } else {\n        out = x.transpose([1, 0]).mulStrict(y).sum(axesArray[1]);\n      }\n    } else {\n      const adjX = axesArray[0] !== x.shape.length - 1;\n      const adjY = axesArray[1] === y.shape.length - 1;\n      out = x.matMul(y, adjX, adjY);\n    }\n\n    if (diff > 0) {\n      let idx: number;\n      if (xNDim > yNDim) {\n        idx = xNDim + yNDim - 3;\n      } else {\n        idx = xNDim - 1;\n      }\n      const squeezeAxes: number[] = [];\n      for (let i = idx; i < idx + diff; ++i) {\n        squeezeAxes.push(i);\n      }\n      out = out.squeeze(squeezeAxes);\n    }\n    if (out.shape.length === 1) {\n      out = out.expandDims(1);\n    }\n    return out;\n  });\n}\n\nexport class Dot extends Merge {\n  /** @nocollapse */\n  static className = 'Dot';\n\n  private axes: number|[number, number];\n  private normalize: boolean;\n\n  constructor(args: DotLayerArgs) {\n    super(args);\n    this.axes = args.axes;\n    this.normalize = args.normalize == null ? false : args.normalize;\n    this.supportsMasking = true;\n    this.reshapeRequired = false;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    tfc.util.assert(\n        Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]),\n        () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n    const shape1 = inputShape[0] as Shape;\n    const shape2 = inputShape[1] as Shape;\n    if (shape1.length > 3 || shape2.length > 3) {\n      throw new NotImplementedError(\n          'Dot layer does not support tensors of 4D or higher rank yet.');\n    }\n\n    const axes = this.interpretAxes(shape1, shape2);\n    if (shape1[axes[0]] !== shape2[axes[1]]) {\n      throw new ValueError(\n          `Dimension incompatibility: ` +\n          `${shape1[axes[0]]} !== ${shape2[axes[1]]}`);\n    }\n  }\n\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    if (inputs.length !== 2) {\n      throw new ValueError(\n          'A `Dot` layer must be called on exactly 2 inputs, ' +\n          `but received ${inputs.length} input(s).`);\n    }\n\n    let x1 = inputs[0];\n    let x2 = inputs[1];\n    let axes: [number, number];\n    if (!Array.isArray(this.axes)) {\n      axes = [\n        interpretAxis(this.axes, x1.shape.length),\n        interpretAxis(this.axes, x2.shape.length)\n      ];\n    } else {\n      axes = this.axes.map(\n                 (axis, i) => interpretAxis(\n                     axis, inputs[i].shape.length)) as [number, number];\n    }\n    if (this.normalize) {\n      x1 = l2Normalize(x1, axes[0]);\n      x2 = l2Normalize(x2, axes[1]);\n    }\n    return batchDot(x1, x2, axes);\n  }\n\n  private interpretAxes(shape1: Shape, shape2: Shape): number[] {\n    let axes: number[];\n    if (!Array.isArray(this.axes)) {\n      // `this.axes` is a single integer.\n      axes = [\n        interpretAxis(this.axes, shape1.length),\n        interpretAxis(this.axes, shape2.length)\n      ];\n    } else {\n      // `this.axes` is an Array of integers.\n      axes = this.axes;\n    }\n    return axes;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    tfc.util.assert(\n        Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]),\n        () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n    const shape1 = (inputShape[0] as Shape).slice();\n    const shape2 = (inputShape[1] as Shape).slice();\n    if (shape1.length > 3 || shape2.length > 3) {\n      throw new NotImplementedError(\n          'Dot layer does not support tensors of 4D or higher rank yet.');\n    }\n\n    const axes = this.interpretAxes(shape1, shape2);\n    shape1.splice(axes[0], 1);\n    shape2.splice(axes[1], 1);\n    shape2.splice(0, 1);\n    const outputShape = shape1.concat(shape2);\n    if (outputShape.length === 1) {\n      outputShape.push(1);\n    }\n    return outputShape;\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor {\n    return null;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'axes': this.axes,\n      'normalize': this.normalize\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Dot);\n\n// TODO(cais): Add functional interfaces for the merge layers.\n"]}},"error":null,"hash":"0ee55edeeb445ce744d8707de69abaff","cacheData":{"env":{}}}